{
    "docs": [
        {
            "location": "/",
            "text": "Eel Pond\n\u00b6\n\n\nThis is a lightweight protocol for assembling up to a few hundred million mRNAseq reads, annotating the resulting assembly, and doing differential expression analysis. The input is short-insert paired-end Illumina reads. This protocol can be run in a single command because it uses the snakemake automated workflow management system.\n\n\nPrevious versions of this protocol included line-by-line commands that the user could follow along with using a test dataset provided in the instructions. Since the recent development of \nsnakemake\n workflow management tool and \nsnakemake-wrappers\n to manage sofware installation of commonly-used bioinformatics tools, we have re-implemented the Eel Pond Protocol to make it easier for users to install software and run a \nde novo\n transcriptome assembly, annotation, and quick differential expression analysis on a set of short-read Illumina data using a single command.\n\n\nThe software for this protocol can be found \nhere\n. \n\n\nTo run the protocol on your own computer system (requires Ubuntu 16.04):\n\n\nInstall \nminiconda\n (for Ubuntu 16.04 \nJetstream image\n).\n\n\nwget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh -b\necho export PATH=\"$HOME/miniconda3/bin:$PATH\" >> ~/.bash_profile\nsource ~/.bash_profile\n\n\n\n\n\nThen,\n\n\nconda install -c bioconda -c conda-forge -y snakemake\n\n\n\n\n\nAnd:\n\n\ngit clone https://github.com/dib-lab/eelpond.git\ncd eelpond\n\ngit submodule update --init --recursive #download test data submodule\n\n#run eelpond\nsnakemake --use-conda --configfile rna_testdata/nema_config.yaml\n\n\n\n\n\nReferences:\n \n\n \noriginal eel-pond protocol docs, last updated 2015\n\n\n \neel-pond protocol docs, last updated 2016\n\n\n \nDIBSI, nonmodel RNAseq workshop, July 2017\n\n\n \nSIO-BUG, nonmodel RNAseq workshop, October 2017\n\n\nintended workflows:\n\n  - \nRead Quality Trimming and Filtering\n\n  - \nDigital Normalization\n\n  - \nAssembly\n\n  - \nQuality Assessment\n\n  - \nAnnotation\n\n  - \nTranscript Quantification\n\n  - \nDifferential Expression\n\n\nsnakemake style follows \nrna-seq-star example workflow",
            "title": "Index"
        },
        {
            "location": "/#eel-pond",
            "text": "This is a lightweight protocol for assembling up to a few hundred million mRNAseq reads, annotating the resulting assembly, and doing differential expression analysis. The input is short-insert paired-end Illumina reads. This protocol can be run in a single command because it uses the snakemake automated workflow management system.  Previous versions of this protocol included line-by-line commands that the user could follow along with using a test dataset provided in the instructions. Since the recent development of  snakemake  workflow management tool and  snakemake-wrappers  to manage sofware installation of commonly-used bioinformatics tools, we have re-implemented the Eel Pond Protocol to make it easier for users to install software and run a  de novo  transcriptome assembly, annotation, and quick differential expression analysis on a set of short-read Illumina data using a single command.  The software for this protocol can be found  here .   To run the protocol on your own computer system (requires Ubuntu 16.04):  Install  miniconda  (for Ubuntu 16.04  Jetstream image ).  wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh -b\necho export PATH=\"$HOME/miniconda3/bin:$PATH\" >> ~/.bash_profile\nsource ~/.bash_profile  Then,  conda install -c bioconda -c conda-forge -y snakemake  And:  git clone https://github.com/dib-lab/eelpond.git\ncd eelpond\n\ngit submodule update --init --recursive #download test data submodule\n\n#run eelpond\nsnakemake --use-conda --configfile rna_testdata/nema_config.yaml  References:     original eel-pond protocol docs, last updated 2015    eel-pond protocol docs, last updated 2016    DIBSI, nonmodel RNAseq workshop, July 2017    SIO-BUG, nonmodel RNAseq workshop, October 2017  intended workflows: \n  -  Read Quality Trimming and Filtering \n  -  Digital Normalization \n  -  Assembly \n  -  Quality Assessment \n  -  Annotation \n  -  Transcript Quantification \n  -  Differential Expression  snakemake style follows  rna-seq-star example workflow",
            "title": "Eel Pond"
        },
        {
            "location": "/QC/",
            "text": "Short read quality and trimming\n\u00b6\n\n\nQuality Assessment\n\u00b6\n\n\nWe\u2019re going to use \nFastQC\n (version 0.11.5) and \nmultiqc\n (version 1.2) to summarize the data before and after adapter trimming. There are several caveats about FastQC - the main one is that it only calculates certain statistics (like duplicated sequences) for subsets of the data (e.g. duplicate sequences are only analyzed for the first 100,000 sequences in each file.\n\n\nMultiqc will summarize individual fastqc output into one output so that you can see all quality information for all files simultaenously.\n\n\nAdapter trim each pair of files\n\u00b6\n\n\nWe use Trimmomatic (version 0.36) to trim off residual Illumina adapters that were left behind after demultiplexing.\n\n\nThis pipeline assumes TruSeq3-PE.fa adapters. However, if running this on your own data, you\u2019ll need to know which Illumina sequencing adapters were used for your library prep in order to trim them off. If they are the right adapters, you should see that some of the reads are trimmed; if they\u2019re not, you won\u2019t see anything get trimmed.\n\n\nSee excellent paper on trimming parameters by \nMacManes 2014\n.\n\n\nBased on these recommendations by MacManes 2014, we use this command in this pipeline:\n\n\nTrimmomaticPE ${base}.fastq.gz ${baseR2}.fastq.gz \\\n    ${base}.qc.fq.gz s1_se \\\n    ${baseR2}.qc.fq.gz s2_se \\\n    ILLUMINACLIP:TruSeq3-PE.fa:2:40:15 \\\n    LEADING:2 TRAILING:2 \\\n    SLIDINGWINDOW:4:2 \\\n    MINLEN:25",
            "title": "Read Quality Trimming and Filtering"
        },
        {
            "location": "/QC/#short-read-quality-and-trimming",
            "text": "",
            "title": "Short read quality and trimming"
        },
        {
            "location": "/QC/#quality-assessment",
            "text": "We\u2019re going to use  FastQC  (version 0.11.5) and  multiqc  (version 1.2) to summarize the data before and after adapter trimming. There are several caveats about FastQC - the main one is that it only calculates certain statistics (like duplicated sequences) for subsets of the data (e.g. duplicate sequences are only analyzed for the first 100,000 sequences in each file.  Multiqc will summarize individual fastqc output into one output so that you can see all quality information for all files simultaenously.",
            "title": "Quality Assessment"
        },
        {
            "location": "/QC/#adapter-trim-each-pair-of-files",
            "text": "We use Trimmomatic (version 0.36) to trim off residual Illumina adapters that were left behind after demultiplexing.  This pipeline assumes TruSeq3-PE.fa adapters. However, if running this on your own data, you\u2019ll need to know which Illumina sequencing adapters were used for your library prep in order to trim them off. If they are the right adapters, you should see that some of the reads are trimmed; if they\u2019re not, you won\u2019t see anything get trimmed.  See excellent paper on trimming parameters by  MacManes 2014 .  Based on these recommendations by MacManes 2014, we use this command in this pipeline:  TrimmomaticPE ${base}.fastq.gz ${baseR2}.fastq.gz \\\n    ${base}.qc.fq.gz s1_se \\\n    ${baseR2}.qc.fq.gz s2_se \\\n    ILLUMINACLIP:TruSeq3-PE.fa:2:40:15 \\\n    LEADING:2 TRAILING:2 \\\n    SLIDINGWINDOW:4:2 \\\n    MINLEN:25",
            "title": "Adapter trim each pair of files"
        },
        {
            "location": "/Annotation/",
            "text": "Annotating de novo transcriptomes with dammit\n\u00b6\n\n\ndammit!\n\n\ndammit\n is an annotation\npipeline written by \nCamille\nScott\n. dammit runs a relatively standard annotation\nprotocol for transcriptomes: it begins by building gene models with \nTransdecoder\n,\nand then\nuses the following protein databases as evidence for annotation:\n\nPfam-A\n, \nRfam\n,\n\nOrthoDB\n,\n\nuniref90\n (uniref is optional with\n\n--full\n).\n\n\nIf a protein dataset is available, this can also be supplied to the\n\ndammit\n pipeline with \n--user-databases\n as optional evidence for\nannotation.\n\n\nIn addition, \nBUSCO\n v3 is run, which will compare the gene content in your transcriptome\nwith a lineage-specific data set. The output is a proportion of your\ntranscriptome that matches with the data set, which can be used as an\nestimate of the completeness of your transcriptome based on evolutionary\nexpectation (\nSimho et al.\n2015\n).\nThere are several lineage-specific datasets available from the authors\nof BUSCO. We will use the \nmetazoa\n dataset for this transcriptome.\n\n\nDatabase Preparation\n\u00b6\n\n\ndammit has two major subcommands: \ndammit databases\n and \ndammit annotate\n. \ndatabases\n\nchecks that the databases are installed and prepared, and if run with the \n--install\n flag,\nwill perform that installation and preparation. If you just run \ndammit databases\n on its\nown, you should get a notification that some database tasks are not up-to-date -- we need\nto install them!\n\n\nUnless we're running short on time, we're going to do a full run. If you want to run a quick\nversion of the pipeline, add a parameter, \n--quick\n, to omit OrthoDB, Uniref, Pfam, and Rfam. \nA \"full\" run will take longer to install and run, but you'll have access to the full annotation pipeline.\n\n\ndammit databases --install --busco-group metazoa # --quick\n\n\n\n\n\nWe used the \"metazoa\" BUSCO group. We can use any of the BUSCO databases, so long as we install\nthem with the \ndammit databases\n subcommand. You can see the whole list by running\n\ndammit databases -h\n. You should try to match your species as closely as possible for the best\nresults. If we want to install another, for example:\n\n\ndammit databases --install --busco-group fungi  # --quick\n\n\n\n\n\nNote: if you have limited space on your instance, you can also install these databases in a different\nlocation (e.g. on an external volume). You would want to run this command \nbefore\n running the database\ninstalls we just ran.\n\n\n#Run  ONLY if you want to install databases in different location. \n#To run, remove the `#` from the front of the following command:\n\n# dammit databases --database-dir /path/to/databases\n\n\n\n\n\nAnnotation\n\u00b6\n\n\nNow we'll download a custom \nNematostella vectensis\n protein database available\nfrom JGI. Here, somebody has already created a proper database for us [1] (it has a reference proteome\navailable through uniprot). If your critter\nis a non-model organism, you will\nlikely need to create your own with proteins from closely-related species. This will rely on your\nknowledge of your system!\n\n\nRun the command:\n\n\ndammit annotate trinity.nema.fasta --busco-group metazoa --user-databases nema.reference.prot.faa --n_threads 6 # --quick\n\n\n\n\n\nWhile dammit runs, it will print out which tasks its running to the terminal. dammit is\nwritten with a library called \npydoit\n, which is a python workflow library similar\nto GNU Make. This not only helps organize the underlying workflow, but also means that if we\ninterrupt it, it will properly resume! \n\n\nAfter a successful run, you'll have a new directory called \ntrinity.nema.fasta.dammit\n. If you\nlook inside, you'll see a lot of files:\n\n\nls trinity.nema.fasta.dammit/\n\n\n\n\n\n    annotate.doit.db                              trinity.nema.fasta.dammit.namemap.csv  trinity.nema.fasta.transdecoder.pep\n    dammit.log                                    trinity.nema.fasta.dammit.stats.json   trinity.nema.fasta.x.nema.reference.prot.faa.crbl.csv\n    run_trinity.nema.fasta.metazoa.busco.results  trinity.nema.fasta.transdecoder.bed    trinity.nema.fasta.x.nema.reference.prot.faa.crbl.gff3\n    tmp                                           trinity.nema.fasta.transdecoder.cds    trinity.nema.fasta.x.nema.reference.prot.faa.crbl.model.csv\n    trinity.nema.fasta                            trinity.nema.fasta.transdecoder_dir    trinity.nema.fasta.x.nema.reference.prot.faa.crbl.model.plot.pdf\n    trinity.nema.fasta.dammit.fasta               trinity.nema.fasta.transdecoder.gff3\n    trinity.nema.fasta.dammit.gff3                trinity.nema.fasta.transdecoder.mRNA\n\n\n\n\n\nThe most important files for you are \ntrinity.nema.fasta.dammit.fasta\n,\n\ntrinity.nema.fasta.dammit.gff3\n, and \ntrinity.nema.fasta.dammit.stats.json\n.\n\n\nIf the above \ndammit\n command is run again, there will be a message:\n\n**Pipeline is already completed!**\n\n\nParse dammit output\n\u00b6\n\n\nCammille wrote dammit in Python, which includes a library to parse gff3 dammit output. To send this output to a useful table, we will need to open the Python environemnt.\n\n\ncd trinity.nema.fasta.dammit\npython\n\n\n\n\n\nThen, using this script, will output a list of gene ID:\n\n\nimport pandas as pd\nfrom dammit.fileio.gff3 import GFF3Parser\ngff_file = \"trinity.nema.fasta.dammit.gff3\"\nannotations = GFF3Parser(filename=gff_file).read()\nnames = annotations.sort_values(by=['seqid', 'score'], ascending=True).query('score < 1e-05').drop_duplicates(subset='seqid')[['seqid', 'Name']]\nnew_file = names.dropna(axis=0,how='all')\nnew_file.head()\nnew_file.to_csv(\"nema_gene_name_id.csv\")\nexit()\n\n\n\n\n\nThis will output a table of genes with 'seqid' and 'Name' in a .csv file: \nnema_gene_name_id.csv\n. Let's take a look at that file:\n\n\nless nema_gene_name_id.csv\n\n\n\n\n\nNotice there are multiple transcripts per gene model prediction. This \n.csv\n file can be used in \ntximport\n in downstream DE analysis.\n\n\nReferences\n\u00b6\n\n\n\n\nPutnam NH, Srivastava M, Hellsten U, Dirks B, Chapman J, Salamov A,\nTerry A, Shapiro H, Lindquist E, Kapitonov VV, Jurka J, Genikhovich G,\nGrigoriev IV, Lucas SM, Steele RE, Finnerty JR, Technau U, Martindale\nMQ, Rokhsar DS. (2007) Sea anemone genome reveals ancestral eumetazoan\ngene repertoire and genomic organization. Science. 317, 86-94.",
            "title": "Annotation"
        },
        {
            "location": "/Annotation/#annotating-de-novo-transcriptomes-with-dammit",
            "text": "dammit!  dammit  is an annotation\npipeline written by  Camille\nScott . dammit runs a relatively standard annotation\nprotocol for transcriptomes: it begins by building gene models with  Transdecoder ,\nand then\nuses the following protein databases as evidence for annotation: Pfam-A ,  Rfam , OrthoDB , uniref90  (uniref is optional with --full ).  If a protein dataset is available, this can also be supplied to the dammit  pipeline with  --user-databases  as optional evidence for\nannotation.  In addition,  BUSCO  v3 is run, which will compare the gene content in your transcriptome\nwith a lineage-specific data set. The output is a proportion of your\ntranscriptome that matches with the data set, which can be used as an\nestimate of the completeness of your transcriptome based on evolutionary\nexpectation ( Simho et al.\n2015 ).\nThere are several lineage-specific datasets available from the authors\nof BUSCO. We will use the  metazoa  dataset for this transcriptome.",
            "title": "Annotating de novo transcriptomes with dammit"
        },
        {
            "location": "/Annotation/#database-preparation",
            "text": "dammit has two major subcommands:  dammit databases  and  dammit annotate .  databases \nchecks that the databases are installed and prepared, and if run with the  --install  flag,\nwill perform that installation and preparation. If you just run  dammit databases  on its\nown, you should get a notification that some database tasks are not up-to-date -- we need\nto install them!  Unless we're running short on time, we're going to do a full run. If you want to run a quick\nversion of the pipeline, add a parameter,  --quick , to omit OrthoDB, Uniref, Pfam, and Rfam. \nA \"full\" run will take longer to install and run, but you'll have access to the full annotation pipeline.  dammit databases --install --busco-group metazoa # --quick  We used the \"metazoa\" BUSCO group. We can use any of the BUSCO databases, so long as we install\nthem with the  dammit databases  subcommand. You can see the whole list by running dammit databases -h . You should try to match your species as closely as possible for the best\nresults. If we want to install another, for example:  dammit databases --install --busco-group fungi  # --quick  Note: if you have limited space on your instance, you can also install these databases in a different\nlocation (e.g. on an external volume). You would want to run this command  before  running the database\ninstalls we just ran.  #Run  ONLY if you want to install databases in different location. \n#To run, remove the `#` from the front of the following command:\n\n# dammit databases --database-dir /path/to/databases",
            "title": "Database Preparation"
        },
        {
            "location": "/Annotation/#annotation",
            "text": "Now we'll download a custom  Nematostella vectensis  protein database available\nfrom JGI. Here, somebody has already created a proper database for us [1] (it has a reference proteome\navailable through uniprot). If your critter\nis a non-model organism, you will\nlikely need to create your own with proteins from closely-related species. This will rely on your\nknowledge of your system!  Run the command:  dammit annotate trinity.nema.fasta --busco-group metazoa --user-databases nema.reference.prot.faa --n_threads 6 # --quick  While dammit runs, it will print out which tasks its running to the terminal. dammit is\nwritten with a library called  pydoit , which is a python workflow library similar\nto GNU Make. This not only helps organize the underlying workflow, but also means that if we\ninterrupt it, it will properly resume!   After a successful run, you'll have a new directory called  trinity.nema.fasta.dammit . If you\nlook inside, you'll see a lot of files:  ls trinity.nema.fasta.dammit/      annotate.doit.db                              trinity.nema.fasta.dammit.namemap.csv  trinity.nema.fasta.transdecoder.pep\n    dammit.log                                    trinity.nema.fasta.dammit.stats.json   trinity.nema.fasta.x.nema.reference.prot.faa.crbl.csv\n    run_trinity.nema.fasta.metazoa.busco.results  trinity.nema.fasta.transdecoder.bed    trinity.nema.fasta.x.nema.reference.prot.faa.crbl.gff3\n    tmp                                           trinity.nema.fasta.transdecoder.cds    trinity.nema.fasta.x.nema.reference.prot.faa.crbl.model.csv\n    trinity.nema.fasta                            trinity.nema.fasta.transdecoder_dir    trinity.nema.fasta.x.nema.reference.prot.faa.crbl.model.plot.pdf\n    trinity.nema.fasta.dammit.fasta               trinity.nema.fasta.transdecoder.gff3\n    trinity.nema.fasta.dammit.gff3                trinity.nema.fasta.transdecoder.mRNA  The most important files for you are  trinity.nema.fasta.dammit.fasta , trinity.nema.fasta.dammit.gff3 , and  trinity.nema.fasta.dammit.stats.json .  If the above  dammit  command is run again, there will be a message: **Pipeline is already completed!**",
            "title": "Annotation"
        },
        {
            "location": "/Annotation/#parse-dammit-output",
            "text": "Cammille wrote dammit in Python, which includes a library to parse gff3 dammit output. To send this output to a useful table, we will need to open the Python environemnt.  cd trinity.nema.fasta.dammit\npython  Then, using this script, will output a list of gene ID:  import pandas as pd\nfrom dammit.fileio.gff3 import GFF3Parser\ngff_file = \"trinity.nema.fasta.dammit.gff3\"\nannotations = GFF3Parser(filename=gff_file).read()\nnames = annotations.sort_values(by=['seqid', 'score'], ascending=True).query('score < 1e-05').drop_duplicates(subset='seqid')[['seqid', 'Name']]\nnew_file = names.dropna(axis=0,how='all')\nnew_file.head()\nnew_file.to_csv(\"nema_gene_name_id.csv\")\nexit()  This will output a table of genes with 'seqid' and 'Name' in a .csv file:  nema_gene_name_id.csv . Let's take a look at that file:  less nema_gene_name_id.csv  Notice there are multiple transcripts per gene model prediction. This  .csv  file can be used in  tximport  in downstream DE analysis.",
            "title": "Parse dammit output"
        },
        {
            "location": "/Annotation/#references",
            "text": "Putnam NH, Srivastava M, Hellsten U, Dirks B, Chapman J, Salamov A,\nTerry A, Shapiro H, Lindquist E, Kapitonov VV, Jurka J, Genikhovich G,\nGrigoriev IV, Lucas SM, Steele RE, Finnerty JR, Technau U, Martindale\nMQ, Rokhsar DS. (2007) Sea anemone genome reveals ancestral eumetazoan\ngene repertoire and genomic organization. Science. 317, 86-94.",
            "title": "References"
        },
        {
            "location": "/Diginorm/",
            "text": "Digital Normalization\n\u00b6\n\n\nIn this section, we\u2019ll apply digital normalization and variable-coverage k-mer abundance trimming to the reads prior to assembly using the \nkhmer software package\n (version 2.1). This has the effect of reducing the computational cost of assembly without negatively affecting the quality of the assembly.\n\n\nThis is all run in one command, taking the trimmed reads as input, uses the orphaned reads that survived while their mated pair did not during adapter and quality trimming. Then, low-abundance reads are trimmed to a coverage of 18 and normalized to a \nk\n-mer (\nk\n = 20) coverage of 20. \n\n\n(interleave-reads.py {}{}.trim_1P.fq {}{}.trim_2P.fq && zcat {}orphans.fq.gz)| \\\\\n(trim-low-abund.py -V -k 20 -Z 18 -C 2 - -o - -M 4e9 --diginorm --diginorm-coverage=20) | \\\\\n(extract-paired-reads.py --gzip -p {}{}.paired.gz -s {}{}.single.gz) > /dev/null\n\n\n\n\n\nThe output files are the remaining reads, grouped as pairs and singles (orphans). Since the Trinity de novo assembly software expects paired reads, we will split them into left \nleft.fq\n and \nright.fq\n read pair files, including the single orphans in the \nleft.fq\n file. \n\n\nfor file in *.paired.gz\ndo\n  split-paired-reads.py ${file}\ndone\n\ncat *.1 > left.fq\ncat *.2 > right.fq\n\ngunzip -c ../diginorm/single.gz >> left.fq",
            "title": "Digital Normalization"
        },
        {
            "location": "/Diginorm/#digital-normalization",
            "text": "In this section, we\u2019ll apply digital normalization and variable-coverage k-mer abundance trimming to the reads prior to assembly using the  khmer software package  (version 2.1). This has the effect of reducing the computational cost of assembly without negatively affecting the quality of the assembly.  This is all run in one command, taking the trimmed reads as input, uses the orphaned reads that survived while their mated pair did not during adapter and quality trimming. Then, low-abundance reads are trimmed to a coverage of 18 and normalized to a  k -mer ( k  = 20) coverage of 20.   (interleave-reads.py {}{}.trim_1P.fq {}{}.trim_2P.fq && zcat {}orphans.fq.gz)| \\\\\n(trim-low-abund.py -V -k 20 -Z 18 -C 2 - -o - -M 4e9 --diginorm --diginorm-coverage=20) | \\\\\n(extract-paired-reads.py --gzip -p {}{}.paired.gz -s {}{}.single.gz) > /dev/null  The output files are the remaining reads, grouped as pairs and singles (orphans). Since the Trinity de novo assembly software expects paired reads, we will split them into left  left.fq  and  right.fq  read pair files, including the single orphans in the  left.fq  file.   for file in *.paired.gz\ndo\n  split-paired-reads.py ${file}\ndone\n\ncat *.1 > left.fq\ncat *.2 > right.fq\n\ngunzip -c ../diginorm/single.gz >> left.fq",
            "title": "Digital Normalization"
        },
        {
            "location": "/Assembly/",
            "text": "Assembling with Trinity\n\u00b6\n\n\nWe use the \nTrinity \nde novo\n transcriptome assembler\n (v2.5.1) to take short, trimmed/diginorm Illumina reads data and assemble (predict) full-length transcripts into a single fasta file output. Each contig in the fasta assembly file represents one unique transcript. The default \nk\n-mer size for the Trinity assembler is \nk\n = 25.\n\n\nThe resulting output assembly fasta file can then be used to align the original, trimmed (not diginorm) short Illumina reads and quantify expression per transcript.\n\n\nThe ID for each transcript is output (version 2.2.0 to current) as follows, where the \nTRINITY\n is constant, the \nDN2202\n is an example of a variable contig/transcript ID, \nc\n stands for component, \ng\n gene and \ni\n isoform:\n\n\nTRINITY_DN2202_c0_g1_i1\n\n\n\n\n\nThis snakemake pipeline will run the following command:\n\n\nTrinity --left left.fq \\\n  --right right.fq --seqType fq --max_memory 10G \\\n  --CPU 4\n\n\n\n\n\nNote, the current version of Trininty (after 2.3.2) is configured to diginorm the input reads before assembly begins. Since we have already applied diginorm to our reads, the result will be a negligible decrease in read counts prior to the assembly. Applying diginorm twice is fine. For data sets with large numbers of reads, applying diginrom as a separate step as we have here may decrease the memory requirements needed by the Trinity pipeline.",
            "title": "Assembly"
        },
        {
            "location": "/Assembly/#assembling-with-trinity",
            "text": "We use the  Trinity  de novo  transcriptome assembler  (v2.5.1) to take short, trimmed/diginorm Illumina reads data and assemble (predict) full-length transcripts into a single fasta file output. Each contig in the fasta assembly file represents one unique transcript. The default  k -mer size for the Trinity assembler is  k  = 25.  The resulting output assembly fasta file can then be used to align the original, trimmed (not diginorm) short Illumina reads and quantify expression per transcript.  The ID for each transcript is output (version 2.2.0 to current) as follows, where the  TRINITY  is constant, the  DN2202  is an example of a variable contig/transcript ID,  c  stands for component,  g  gene and  i  isoform:  TRINITY_DN2202_c0_g1_i1  This snakemake pipeline will run the following command:  Trinity --left left.fq \\\n  --right right.fq --seqType fq --max_memory 10G \\\n  --CPU 4  Note, the current version of Trininty (after 2.3.2) is configured to diginorm the input reads before assembly begins. Since we have already applied diginorm to our reads, the result will be a negligible decrease in read counts prior to the assembly. Applying diginorm twice is fine. For data sets with large numbers of reads, applying diginrom as a separate step as we have here may decrease the memory requirements needed by the Trinity pipeline.",
            "title": "Assembling with Trinity"
        },
        {
            "location": "/Quality/",
            "text": "Evaluating your transcriptome assembly\n\u00b6\n\n\nWe will be using Transrate and BUSCO!\n\n\nBUSCO\n\u00b6\n\n\n\n\nB\nenchmarking \nU\nniversal \nS\ningle \nC\nopy \nO\nrthologs (BUSCO)\n\n\nEukaryota database has 303 genes\n\n\nMetazoa database has 978 genes\n\n\n\"Complete\" lengths are within two standard deviations of the BUSCO group mean length\n\n\n\n\nGenes that make up the BUSCO sets for each major lineage are selected from orthologous groups with genes present as single-copy orthologs in at least 90% of the species. \n\n\n\n\n\n\nUseful links:\n\n\n\n\nWebsite with additional busco databases: \nhttp://busco.ezlab.org/\n\n\nPaper: \nSimao et al. 2015\n\n\nUser Guide\n\n\n\n\nCommand:\n\n\nrun_BUSCO.py \\\n-i Trinity.fixed.fasta \\\n-o nema_busco_metazoa -l ~/busco/metazoa_odb9 \\\n-m transcriptome --cpu 2\n\n\n\n\n\nTransrate\n\u00b6\n\n\nTransrate\n serves two main purposes. It can compare two assemblies to see how similar they are. Or, it can give you a score which represents proportion of input reads that provide positive support for the assembly. We will use transrate to get a score for the assembly. Use the trimmed reads. For a further explanation of metrics and how to run the reference-based transrate, see the \ndocumentation\n and the paper by \nSmith-Unna et al. 2016\n. \n\n\n\n\nHow do two transcriptomes compare with each other?\n\n\n\n\ntransrate --reference=Trinity.fixed.fasta --assembly=trinity-nematostella-raw.fa --output=full_v_subset\ntransrate --reference=trinity-nematostella-raw.fa --assembly=Trinity.fixed.fasta --output=subset_v_full",
            "title": "Quality Assessment"
        },
        {
            "location": "/Quality/#evaluating-your-transcriptome-assembly",
            "text": "We will be using Transrate and BUSCO!",
            "title": "Evaluating your transcriptome assembly"
        },
        {
            "location": "/Quality/#busco",
            "text": "B enchmarking  U niversal  S ingle  C opy  O rthologs (BUSCO)  Eukaryota database has 303 genes  Metazoa database has 978 genes  \"Complete\" lengths are within two standard deviations of the BUSCO group mean length   Genes that make up the BUSCO sets for each major lineage are selected from orthologous groups with genes present as single-copy orthologs in at least 90% of the species.     Useful links:   Website with additional busco databases:  http://busco.ezlab.org/  Paper:  Simao et al. 2015  User Guide   Command:  run_BUSCO.py \\\n-i Trinity.fixed.fasta \\\n-o nema_busco_metazoa -l ~/busco/metazoa_odb9 \\\n-m transcriptome --cpu 2",
            "title": "BUSCO"
        },
        {
            "location": "/Quality/#transrate",
            "text": "Transrate  serves two main purposes. It can compare two assemblies to see how similar they are. Or, it can give you a score which represents proportion of input reads that provide positive support for the assembly. We will use transrate to get a score for the assembly. Use the trimmed reads. For a further explanation of metrics and how to run the reference-based transrate, see the  documentation  and the paper by  Smith-Unna et al. 2016 .    How do two transcriptomes compare with each other?   transrate --reference=Trinity.fixed.fasta --assembly=trinity-nematostella-raw.fa --output=full_v_subset\ntransrate --reference=trinity-nematostella-raw.fa --assembly=Trinity.fixed.fasta --output=subset_v_full",
            "title": "Transrate"
        },
        {
            "location": "/Quant/",
            "text": "Quantification with Salmon\n\u00b6\n\n\nWe will use \nSalmon\n to\nquantify expression. Salmon is a new breed of software for quantifying RNAseq reads that is both really fast and takes\ntranscript length into consideration (\nPatro et al. 2015\n).\n\n\nFor further reading, see\n\n\n\n\nIntro blog post: http://robpatro.com/blog/?p=248\n\n\nA 2016 blog post evaluating and comparing methods \nhere\n\n\nSalmon github repo \nhere\n\n\nhttps://github.com/ngs-docs/2015-nov-adv-rna/blob/master/salmon.rst\n\n\nhttp://angus.readthedocs.io/en/2016/rob_quant/tut.html\n\n\nhttps://2016-aug-nonmodel-rnaseq.readthedocs.io/en/latest/quantification.html\n\n\n\n\nThe two most interesting files are \nsalmon_quant.log\n and\n\nquant.sf\n. The latter contains the counts; the former contains the\nlog information from running things.\n\n\nWe recommend quantifying using the Trinity transcriptome assembly fasta file, which will give expression values for each contig, like this in \nquant.sf\n:\n\n\nName                  Length    EffectiveLength    TPM    NumReads\nTRINITY_DN2202_c0_g1_i1    210    39.818    2.683835    2.000000\nTRINITY_DN2270_c0_g1_i1    213    41.064    0.000000    0.000000\nTRINITY_DN2201_c0_g1_i1    266    69.681    0.766816    1.000000\nTRINITY_DN2222_c0_g1_i1    243    55.794    2.873014    3.000000\nTRINITY_DN2291_c0_g1_i1    245    56.916    0.000000    0.000000\nTRINITY_DN2269_c0_g1_i1    294    89.251    0.000000    0.000000\nTRINITY_DN2269_c1_g1_i1    246    57.479    0.000000    0.000000\nTRINITY_DN2279_c0_g1_i1    426    207.443    0.000000    0.000000\nTRINITY_DN2262_c0_g1_i1    500    280.803    0.190459    1.000912\nTRINITY_DN2253_c0_g1_i1    1523    1303.116    0.164015    4.000000\nTRINITY_DN2287_c0_g1_i1    467    247.962    0.000000    0.000000\nTRINITY_DN2287_c1_g1_i1    325    113.826    0.469425    1.000000\nTRINITY_DN2237_c0_g1_i1    306    98.441    0.542788    1.000000\nTRINITY_DN2237_c0_g2_i1    307    99.229    0.000000    0.000000\nTRINITY_DN2250_c0_g1_i1    368    151.832    0.000000    0.000000\nTRINITY_DN2250_c1_g1_i1    271    72.988    0.000000    0.000000\nTRINITY_DN2208_c0_g1_i1    379    162.080    1.978014    6.000000\nTRINITY_DN2277_c0_g1_i1    269    71.657    0.745677    1.000000\nTRINITY_DN2231_c0_g1_i1    209    39.409    0.000000    0.000000\nTRINITY_DN2231_c1_g1_i1    334    121.411    0.000000    0.000000\nTRINITY_DN2204_c0_g1_i1    287    84.121    0.000000    0.000000\n\n\n\n\n\nThere are two commands for salmon, \nsalmon index\n and \nsalmon quant\n. The first command, \nsalmon index\n will index the transcriptome:\n\n\nsalmon index --index nema --transcripts trinity.nema.full.fasta --type quasi\n\n\n\n\n\nAnd the second command, \nsalmon quant\n will quantify the trimmed reads (not diginormed) using the transcriptome:\n\n\n  for R1 in *R1*.fastq.gz\n  do\n    sample=$(basename $R1 extract.fastq.gz)\n    echo sample is $sample, R1 is $R1\n    R2=${R1/R1/R2}\n    echo R2 is $R2\n    salmon quant -i nema -p 2 -l IU -1 <(gunzip -c $R1) -2 <(gunzip -c $R2) -o ${sample}quant\n  done",
            "title": "Transcript Quantification"
        },
        {
            "location": "/Quant/#quantification-with-salmon",
            "text": "We will use  Salmon  to\nquantify expression. Salmon is a new breed of software for quantifying RNAseq reads that is both really fast and takes\ntranscript length into consideration ( Patro et al. 2015 ).  For further reading, see   Intro blog post: http://robpatro.com/blog/?p=248  A 2016 blog post evaluating and comparing methods  here  Salmon github repo  here  https://github.com/ngs-docs/2015-nov-adv-rna/blob/master/salmon.rst  http://angus.readthedocs.io/en/2016/rob_quant/tut.html  https://2016-aug-nonmodel-rnaseq.readthedocs.io/en/latest/quantification.html   The two most interesting files are  salmon_quant.log  and quant.sf . The latter contains the counts; the former contains the\nlog information from running things.  We recommend quantifying using the Trinity transcriptome assembly fasta file, which will give expression values for each contig, like this in  quant.sf :  Name                  Length    EffectiveLength    TPM    NumReads\nTRINITY_DN2202_c0_g1_i1    210    39.818    2.683835    2.000000\nTRINITY_DN2270_c0_g1_i1    213    41.064    0.000000    0.000000\nTRINITY_DN2201_c0_g1_i1    266    69.681    0.766816    1.000000\nTRINITY_DN2222_c0_g1_i1    243    55.794    2.873014    3.000000\nTRINITY_DN2291_c0_g1_i1    245    56.916    0.000000    0.000000\nTRINITY_DN2269_c0_g1_i1    294    89.251    0.000000    0.000000\nTRINITY_DN2269_c1_g1_i1    246    57.479    0.000000    0.000000\nTRINITY_DN2279_c0_g1_i1    426    207.443    0.000000    0.000000\nTRINITY_DN2262_c0_g1_i1    500    280.803    0.190459    1.000912\nTRINITY_DN2253_c0_g1_i1    1523    1303.116    0.164015    4.000000\nTRINITY_DN2287_c0_g1_i1    467    247.962    0.000000    0.000000\nTRINITY_DN2287_c1_g1_i1    325    113.826    0.469425    1.000000\nTRINITY_DN2237_c0_g1_i1    306    98.441    0.542788    1.000000\nTRINITY_DN2237_c0_g2_i1    307    99.229    0.000000    0.000000\nTRINITY_DN2250_c0_g1_i1    368    151.832    0.000000    0.000000\nTRINITY_DN2250_c1_g1_i1    271    72.988    0.000000    0.000000\nTRINITY_DN2208_c0_g1_i1    379    162.080    1.978014    6.000000\nTRINITY_DN2277_c0_g1_i1    269    71.657    0.745677    1.000000\nTRINITY_DN2231_c0_g1_i1    209    39.409    0.000000    0.000000\nTRINITY_DN2231_c1_g1_i1    334    121.411    0.000000    0.000000\nTRINITY_DN2204_c0_g1_i1    287    84.121    0.000000    0.000000  There are two commands for salmon,  salmon index  and  salmon quant . The first command,  salmon index  will index the transcriptome:  salmon index --index nema --transcripts trinity.nema.full.fasta --type quasi  And the second command,  salmon quant  will quantify the trimmed reads (not diginormed) using the transcriptome:    for R1 in *R1*.fastq.gz\n  do\n    sample=$(basename $R1 extract.fastq.gz)\n    echo sample is $sample, R1 is $R1\n    R2=${R1/R1/R2}\n    echo R2 is $R2\n    salmon quant -i nema -p 2 -l IU -1 <(gunzip -c $R1) -2 <(gunzip -c $R2) -o ${sample}quant\n  done",
            "title": "Quantification with Salmon"
        },
        {
            "location": "/DE/",
            "text": "Differential expression analysis with DESeq2\n\u00b6\n\n\nComparing gene expression differences in samples between experimental conditions. \n\n\nWe will be using \nDESeq2\n.\n\n\nReferences:\n\n \nDocumentation for DESeq2 with example analysis\n\n\n \nLove et al. 2014\n\n* \nLove et al. 2016\n\n\nAdditional links:\n\n \nDE lecture by Jane Khudyakov, July 2017\n\n\n \nExample DE analysis from two populations of killifish! (Fundulus heteroclitus MDPL vs. MDPL)\n\n* \nA Review of Differential Gene Expression Software for mRNA sequencing\n\n\nRStudio!\n\u00b6\n\n\nThe pipeline will be running these commands in an R script. You could run them in R Studio:\n\n\nLoad libraries\n\n\nlibrary(DESeq2)\nlibrary(\"lattice\")\nlibrary(tximport)\nlibrary(readr)\nlibrary(gplots)\nlibrary(RColorBrewer)\nsource('~/plotPCAWithSampleNames.R')\n\n\n\n\n\nTell RStudio where your files are and ask whether they exist:\n\n\nsetwd(\"/mnt/work/quant/salmon_out/\")\ndir<-\"/mnt/work/quant/\"\nfiles_list = list.files()\nfiles <- file.path(dir, \"salmon_out\",files_list, \"quant.sf\")\nnames(files) <- c(\"0Hour_1\",\"0Hour_2\",\"0Hour_3\",\"0Hour_4\",\"0Hour_5\",\"6Hour_1\",\"6Hour_2\",\"6Hour_3\",\"6Hour_4\",\"6Hour_5\")\nfiles\nprint(file.exists(files))\n\n\n\n\n\nGrab the \ngene names\n and transcript ID file to \nsummarize expression at the gene level\n.\n\n\ntx2gene <- read.table(\"~/nema_transcript_gene_id.txt\",sep=\"\\t\")\ncols<-c(\"transcript_id\",\"gene_id\")\ncolnames(tx2gene)<-cols\nhead(tx2gene)\ntxi.salmon <- tximport(files, type = \"salmon\", tx2gene = tx2gene,importer=read.delim)\nhead(txi.salmon$counts)\ndim(txi.salmon$counts)\n\n\n\n\n\nAssign experimental variables:\n\n\ncondition = factor(c(\"0Hour\",\"0Hour\",\"0Hour\",\"0Hour\",\"0Hour\",\"6Hour\",\"6Hour\",\"6Hour\",\"6Hour\",\"6Hour\"))\nExpDesign <- data.frame(row.names=colnames(txi.salmon$counts), condition = condition)\nExpDesign\n\n\n\n\n\nRun DESeq2:\n\n\ndds <- DESeqDataSetFromTximport(txi.salmon, ExpDesign, ~condition)\ndds <- DESeq(dds, betaPrior=FALSE)\n\n\n\n\n\nGet counts:\n\n\ncounts_table = counts( dds, normalized=TRUE )\n\n\n\n\n\nFiltering out low expression transcripts:\n\n\nSee plot from \nLisa Komoroske\n generated with \nRNAseq123\n\n\nfiltered_norm_counts<-counts_table[!rowSums(counts_table==0)>=1, ]\nfiltered_norm_counts<-as.data.frame(filtered_norm_counts)\nGeneID<-rownames(filtered_norm_counts)\nfiltered_norm_counts<-cbind(filtered_norm_counts,GeneID)\ndim(filtered_norm_counts)\nhead(filtered_norm_counts)\n\n\n\n\n\nEstimate dispersion:\n\n\nplotDispEsts(dds)\n\n\n\n\n\nPCA:\n\n\nlog_dds<-rlog(dds)\nplotPCAWithSampleNames(log_dds, intgroup=\"condition\", ntop=40000)\n\n\n\n\n\nGet DE results:\n\n\nres<-results(dds,contrast=c(\"condition\",\"6Hour\",\"0Hour\"))\nhead(res)\nres_ordered<-res[order(res$padj),]\nGeneID<-rownames(res_ordered)\nres_ordered<-as.data.frame(res_ordered)\nres_genes<-cbind(res_ordered,GeneID)\ndim(res_genes)\nhead(res_genes)\ndim(res_genes)\nres_genes_merged <- merge(res_genes,filtered_norm_counts,by=unique(\"GeneID\"))\ndim(res_genes_merged)\nhead(res_genes_merged)\nres_ordered<-res_genes_merged[order(res_genes_merged$padj),]\nwrite.csv(res_ordered, file=\"nema_DESeq_all.csv\" )\n\n\n\n\n\nSet a threshold cutoff of padj<0.05 and \u00b1 log2FC 1:\n\n\nresSig = res_ordered[res_ordered$padj < 0.05, ]\nresSig = resSig[resSig$log2FoldChange > 1 | resSig$log2FoldChange < -1,]\nwrite.csv(resSig,file=\"nema_DESeq_padj0.05_log2FC1.csv\")\n\n\n\n\n\nMA plot with gene names:\n\n\nplot(log2(res_ordered$baseMean), res_ordered$log2FoldChange, col=ifelse(res_ordered$padj < 0.05, \"red\",\"gray67\"),main=\"nema (padj<0.05, log2FC = \u00b11)\",xlim=c(1,20),pch=20,cex=1,ylim=c(-12,12))\nabline(h=c(-1,1), col=\"blue\")\ngenes<-resSig$GeneID\nmygenes <- resSig[,]\nbaseMean_mygenes <- mygenes[,\"baseMean\"]\nlog2FoldChange_mygenes <- mygenes[,\"log2FoldChange\"]\ntext(log2(baseMean_mygenes),log2FoldChange_mygenes,labels=genes,pos=2,cex=0.60)\n\n\n\n\n\nHeatmap\n\n\nd<-resSig\ndim(d)\nhead(d)\ncolnames(d)\nd<-d[,c(8:17)]\nd<-as.matrix(d)\nd<-as.data.frame(d)\nd<-as.matrix(d)\nrownames(d) <- resSig[,1]\nhead(d)\n\nhr <- hclust(as.dist(1-cor(t(d), method=\"pearson\")), method=\"complete\")\nmycl <- cutree(hr, h=max(hr$height/1.5))\nclusterCols <- rainbow(length(unique(mycl)))\nmyClusterSideBar <- clusterCols[mycl]\nmyheatcol <- greenred(75)\nheatmap.2(d, main=\"nema (padj<0.05, log2FC = \u00b11)\", \n          Rowv=as.dendrogram(hr),\n          cexRow=0.75,cexCol=0.8,srtCol= 90,\n          adjCol = c(NA,0),offsetCol=2.5, \n          Colv=NA, dendrogram=\"row\", \n          scale=\"row\", col=myheatcol, \n          density.info=\"none\", \n          trace=\"none\", RowSideColors= myClusterSideBar)",
            "title": "Differential Expression"
        },
        {
            "location": "/DE/#differential-expression-analysis-with-deseq2",
            "text": "Comparing gene expression differences in samples between experimental conditions.   We will be using  DESeq2 .  References:   Documentation for DESeq2 with example analysis    Love et al. 2014 \n*  Love et al. 2016  Additional links:   DE lecture by Jane Khudyakov, July 2017    Example DE analysis from two populations of killifish! (Fundulus heteroclitus MDPL vs. MDPL) \n*  A Review of Differential Gene Expression Software for mRNA sequencing",
            "title": "Differential expression analysis with DESeq2"
        },
        {
            "location": "/DE/#rstudio",
            "text": "The pipeline will be running these commands in an R script. You could run them in R Studio:  Load libraries  library(DESeq2)\nlibrary(\"lattice\")\nlibrary(tximport)\nlibrary(readr)\nlibrary(gplots)\nlibrary(RColorBrewer)\nsource('~/plotPCAWithSampleNames.R')  Tell RStudio where your files are and ask whether they exist:  setwd(\"/mnt/work/quant/salmon_out/\")\ndir<-\"/mnt/work/quant/\"\nfiles_list = list.files()\nfiles <- file.path(dir, \"salmon_out\",files_list, \"quant.sf\")\nnames(files) <- c(\"0Hour_1\",\"0Hour_2\",\"0Hour_3\",\"0Hour_4\",\"0Hour_5\",\"6Hour_1\",\"6Hour_2\",\"6Hour_3\",\"6Hour_4\",\"6Hour_5\")\nfiles\nprint(file.exists(files))  Grab the  gene names  and transcript ID file to  summarize expression at the gene level .  tx2gene <- read.table(\"~/nema_transcript_gene_id.txt\",sep=\"\\t\")\ncols<-c(\"transcript_id\",\"gene_id\")\ncolnames(tx2gene)<-cols\nhead(tx2gene)\ntxi.salmon <- tximport(files, type = \"salmon\", tx2gene = tx2gene,importer=read.delim)\nhead(txi.salmon$counts)\ndim(txi.salmon$counts)  Assign experimental variables:  condition = factor(c(\"0Hour\",\"0Hour\",\"0Hour\",\"0Hour\",\"0Hour\",\"6Hour\",\"6Hour\",\"6Hour\",\"6Hour\",\"6Hour\"))\nExpDesign <- data.frame(row.names=colnames(txi.salmon$counts), condition = condition)\nExpDesign  Run DESeq2:  dds <- DESeqDataSetFromTximport(txi.salmon, ExpDesign, ~condition)\ndds <- DESeq(dds, betaPrior=FALSE)  Get counts:  counts_table = counts( dds, normalized=TRUE )  Filtering out low expression transcripts:  See plot from  Lisa Komoroske  generated with  RNAseq123  filtered_norm_counts<-counts_table[!rowSums(counts_table==0)>=1, ]\nfiltered_norm_counts<-as.data.frame(filtered_norm_counts)\nGeneID<-rownames(filtered_norm_counts)\nfiltered_norm_counts<-cbind(filtered_norm_counts,GeneID)\ndim(filtered_norm_counts)\nhead(filtered_norm_counts)  Estimate dispersion:  plotDispEsts(dds)  PCA:  log_dds<-rlog(dds)\nplotPCAWithSampleNames(log_dds, intgroup=\"condition\", ntop=40000)  Get DE results:  res<-results(dds,contrast=c(\"condition\",\"6Hour\",\"0Hour\"))\nhead(res)\nres_ordered<-res[order(res$padj),]\nGeneID<-rownames(res_ordered)\nres_ordered<-as.data.frame(res_ordered)\nres_genes<-cbind(res_ordered,GeneID)\ndim(res_genes)\nhead(res_genes)\ndim(res_genes)\nres_genes_merged <- merge(res_genes,filtered_norm_counts,by=unique(\"GeneID\"))\ndim(res_genes_merged)\nhead(res_genes_merged)\nres_ordered<-res_genes_merged[order(res_genes_merged$padj),]\nwrite.csv(res_ordered, file=\"nema_DESeq_all.csv\" )  Set a threshold cutoff of padj<0.05 and \u00b1 log2FC 1:  resSig = res_ordered[res_ordered$padj < 0.05, ]\nresSig = resSig[resSig$log2FoldChange > 1 | resSig$log2FoldChange < -1,]\nwrite.csv(resSig,file=\"nema_DESeq_padj0.05_log2FC1.csv\")  MA plot with gene names:  plot(log2(res_ordered$baseMean), res_ordered$log2FoldChange, col=ifelse(res_ordered$padj < 0.05, \"red\",\"gray67\"),main=\"nema (padj<0.05, log2FC = \u00b11)\",xlim=c(1,20),pch=20,cex=1,ylim=c(-12,12))\nabline(h=c(-1,1), col=\"blue\")\ngenes<-resSig$GeneID\nmygenes <- resSig[,]\nbaseMean_mygenes <- mygenes[,\"baseMean\"]\nlog2FoldChange_mygenes <- mygenes[,\"log2FoldChange\"]\ntext(log2(baseMean_mygenes),log2FoldChange_mygenes,labels=genes,pos=2,cex=0.60)  Heatmap  d<-resSig\ndim(d)\nhead(d)\ncolnames(d)\nd<-d[,c(8:17)]\nd<-as.matrix(d)\nd<-as.data.frame(d)\nd<-as.matrix(d)\nrownames(d) <- resSig[,1]\nhead(d)\n\nhr <- hclust(as.dist(1-cor(t(d), method=\"pearson\")), method=\"complete\")\nmycl <- cutree(hr, h=max(hr$height/1.5))\nclusterCols <- rainbow(length(unique(mycl)))\nmyClusterSideBar <- clusterCols[mycl]\nmyheatcol <- greenred(75)\nheatmap.2(d, main=\"nema (padj<0.05, log2FC = \u00b11)\", \n          Rowv=as.dendrogram(hr),\n          cexRow=0.75,cexCol=0.8,srtCol= 90,\n          adjCol = c(NA,0),offsetCol=2.5, \n          Colv=NA, dendrogram=\"row\", \n          scale=\"row\", col=myheatcol, \n          density.info=\"none\", \n          trace=\"none\", RowSideColors= myClusterSideBar)",
            "title": "RStudio!"
        }
    ]
}