{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"elvers \u00b6 ___ .-' `'. / \\ | ; | | ___.--, _.._ |O) ~ (O) | _.---'`__.-( (_. __.--'`_.. '.__.\\ '--. \\_.-' ,.--'` `\"\"` ( ,.--'` ',__ /./; ;, '.__.'` __ _`) ) .---.__.' / | |\\ \\__..--\"\" \"\"\"--.,_ `---' .'.''-._.-'`_./ /\\ '. \\_.-~~~````~~~-.__`-.__.' | | .' _.-' | | \\ \\ '. \\ \\/ .' \\ \\ '. '-._) \\/ / \\ \\ `=.__`-~-. / /\\ `) ) / / `\"\".`\\ , _.-'.'\\ \\ / / ( ( / / `--~` ) ) .-' .' '.'. | ( (/` ( (` ) ) `-; ` '--; (' elvers is an automated workflow system, designed to facilitate running a number of tools on a set of data without the need to babysit each individual program. elvers is free and open source, and relies solely on programs that are also free, open source, and reasonably installable (which we consider part of being open). Why use elvers? \u00b6 Some parts of bioinformatic analysis are standardized and just need to be run. For others, we may want to compare results between different input parameters or different programs at individual steps. Both of these cases are facilitated by using a workflow system which 1) handles all installations, 2) executes steps in a standard and repeatable manner, 3) keeps track of all steps that have been run, and 4) provides a way to pick up from where you left off, should any issues arise during execution. elvers is designed to be easy to begin executing default workflows, but flexible and extensible, allowing full configuration of program parameters and programs within each workflow. What do I need? \u00b6 To run elvers , you need to input some data, either reads, a reference transcriptome, or both . To run, you'll need access to a linux system via a command-line interface. Most programs also work on Mac, but a few are troublesome. If you'll be running de novo assembly, we recomment a high-memory machine. What programs are run? \u00b6 elvers is a workflow system or workflow playbook , meaning that it provides a number of different workflows that can be run. Each workflow consists of one or more programs to run an analysis on your data. The workflow(s) you choose will depend on your data and analysis needs. A list of available workflows and tools can be found on the workflows page. Name disambiguation \u00b6 elvers , formerly eelpond , is an automated workflow system. It evolved from a snakemake update of the \"Eel Pond\" protocol for de novo RNAseq analysis, previously developed by CT Brown and members of the dib-lab. An automated version of this protocol is the default workflow in elvers , and the older, step-by-step Eel Pond protocol can be found here . Authors \u00b6 elvers was developed by N Tessa Pierce with invaluable support and feedback from CT Brown, Charles Reid, Lisa Johnson, Taylor Reiter, Luiz Irber, and the entire dib-lab. Citation \u00b6 coming soon , please contact Tessa at ntpierce@gmail.com if you need to cite and this hasn't been filled in yet!","title":"elvers"},{"location":"#elvers","text":"___ .-' `'. / \\ | ; | | ___.--, _.._ |O) ~ (O) | _.---'`__.-( (_. __.--'`_.. '.__.\\ '--. \\_.-' ,.--'` `\"\"` ( ,.--'` ',__ /./; ;, '.__.'` __ _`) ) .---.__.' / | |\\ \\__..--\"\" \"\"\"--.,_ `---' .'.''-._.-'`_./ /\\ '. \\_.-~~~````~~~-.__`-.__.' | | .' _.-' | | \\ \\ '. \\ \\/ .' \\ \\ '. '-._) \\/ / \\ \\ `=.__`-~-. / /\\ `) ) / / `\"\".`\\ , _.-'.'\\ \\ / / ( ( / / `--~` ) ) .-' .' '.'. | ( (/` ( (` ) ) `-; ` '--; (' elvers is an automated workflow system, designed to facilitate running a number of tools on a set of data without the need to babysit each individual program. elvers is free and open source, and relies solely on programs that are also free, open source, and reasonably installable (which we consider part of being open).","title":"elvers"},{"location":"#why-use-elvers","text":"Some parts of bioinformatic analysis are standardized and just need to be run. For others, we may want to compare results between different input parameters or different programs at individual steps. Both of these cases are facilitated by using a workflow system which 1) handles all installations, 2) executes steps in a standard and repeatable manner, 3) keeps track of all steps that have been run, and 4) provides a way to pick up from where you left off, should any issues arise during execution. elvers is designed to be easy to begin executing default workflows, but flexible and extensible, allowing full configuration of program parameters and programs within each workflow.","title":"Why use elvers?"},{"location":"#what-do-i-need","text":"To run elvers , you need to input some data, either reads, a reference transcriptome, or both . To run, you'll need access to a linux system via a command-line interface. Most programs also work on Mac, but a few are troublesome. If you'll be running de novo assembly, we recomment a high-memory machine.","title":"What do I need?"},{"location":"#what-programs-are-run","text":"elvers is a workflow system or workflow playbook , meaning that it provides a number of different workflows that can be run. Each workflow consists of one or more programs to run an analysis on your data. The workflow(s) you choose will depend on your data and analysis needs. A list of available workflows and tools can be found on the workflows page.","title":"What programs are run?"},{"location":"#name-disambiguation","text":"elvers , formerly eelpond , is an automated workflow system. It evolved from a snakemake update of the \"Eel Pond\" protocol for de novo RNAseq analysis, previously developed by CT Brown and members of the dib-lab. An automated version of this protocol is the default workflow in elvers , and the older, step-by-step Eel Pond protocol can be found here .","title":"Name disambiguation"},{"location":"#authors","text":"elvers was developed by N Tessa Pierce with invaluable support and feedback from CT Brown, Charles Reid, Lisa Johnson, Taylor Reiter, Luiz Irber, and the entire dib-lab.","title":"Authors"},{"location":"#citation","text":"coming soon , please contact Tessa at ntpierce@gmail.com if you need to cite and this hasn't been filled in yet!","title":"Citation"},{"location":"about/","text":"About elvers \u00b6 elvers uses snakemake for workflow management and conda for software installation. The code can be found here .","title":"About"},{"location":"about/#about-elvers","text":"elvers uses snakemake for workflow management and conda for software installation. The code can be found here .","title":"About elvers"},{"location":"advanced_usage/","text":"Advanced Usage \u00b6 Each independent step is split into a smaller workflow that can be run independently, if desired, e.g. elvers examples/nema preprocess . Individual tools can also be run independently, see Advanced Usage . See the help, here: elvers -h available workflows: preprocess: Read Quality Trimming and Filtering (fastqc, trimmomatic) kmer_trim: Kmer Trimming and/or Digital Normalization (khmer) assemble: Transcriptome Assembly (trinity) get_reference: Specify assembly for downstream steps annotate : Annotate the transcriptome (dammit, sourmash) quantify: Quantify transcripts (salmon) full: preprocess, kmer_trim, assemble, annotate, quantify You can see the available workflows (and which programs they run) by using the --print_workflows flag: elvers examples/nema.yaml --print_workflows more info coming soon","title":"Advanced Usage"},{"location":"advanced_usage/#advanced-usage","text":"Each independent step is split into a smaller workflow that can be run independently, if desired, e.g. elvers examples/nema preprocess . Individual tools can also be run independently, see Advanced Usage . See the help, here: elvers -h available workflows: preprocess: Read Quality Trimming and Filtering (fastqc, trimmomatic) kmer_trim: Kmer Trimming and/or Digital Normalization (khmer) assemble: Transcriptome Assembly (trinity) get_reference: Specify assembly for downstream steps annotate : Annotate the transcriptome (dammit, sourmash) quantify: Quantify transcripts (salmon) full: preprocess, kmer_trim, assemble, annotate, quantify You can see the available workflows (and which programs they run) by using the --print_workflows flag: elvers examples/nema.yaml --print_workflows more info coming soon","title":"Advanced Usage"},{"location":"annotate/","text":"Annotate Subworkflow \u00b6 Subworkflows combine tools in the right order to facilitate file targeting withing elvers . The \"annotate\" subworkflow annotates a transcriptome. It requires an assembly to be provided, either by running an assembly or providing one in your configfile. At the moment, this workflow consists of: dammit Quickstart \u00b6 If you're starting a new elvers run using a reference file (even if you have previously built a de novo assembly via elvers ), you need to help elvers find that assembly. Scenario 1: You're starting from your own reference file: You need to provide the reference in your `config.yaml` file: ``` get_reference: reference: input reference fasta REQUIRED gene_trans_map: OPTIONAL: provide a gene to transcript map for input transcriptome reference_extension: '_input' OPTIONAL, changes naming download_ref: the reference entry above is a link that needs to be downloaded use_ftp: download via ftp instead of http ``` Once you add this to your configfile (e.g. `my_config.yaml`), you can run a reference/assembly-based workflow such as annotate. ``` elvers my_config.yaml annotate ``` For more details on reference specification, see the [get_reference documentation](get_reference.md). For annotation configuration, see below. Scenario 2: You've previously run an assembly program via `elvers`: You _can_ provide the built reference in the same manner as above. However, if you're running more workflows in the same directory, you can also just specify the name of the assembly program that you used to generate the assembly. This will *not* rerun the assembly (unless you provide new input files). Instead, this will allow `elvers` to know where to look for your previously-generated reference file. Because we enable multiple referene generation programs, we don't want to assume which reference you'd like to use for downstream steps (in fact, if you provide multiple references, elvers will run the downstream steps on all references, assuming you provide unique reference_extension parameters so that the references are uniquely named. Example: You've previously run the trinity assembly, and want to annotate it. ``` elvers examples/nema.yaml assemble annotate ``` Here, the `assemble` workflow just enables `elvers` to locate your assembly file for `annotate`. Output files: \u00b6 Your main output directory will be determined by your config file: by default it is BASENAME_out (you specify BASENAME). Dammit will output files in the annotation subdirectory of this output directory. The annotated fasta file will be ASSEMBLY.dammit.fasta and the annotation gff3 file will be ASSEMBLY.dammit.gff3 . Dammit will also produce a number of intermediate files that will be contained within an ASSEMBLY.fasta.dammit folder. Configuring the annotate subworkflow \u00b6 To set up your sample info and build a configfile, see Understanding and Configuring Workflows . If you want to add the annotate program parameters to a previously built configfile, run: elvers config.yaml annotate --print_params A small set of parameters should print to your console: #################### annotate #################### dammit: busco_group: # specify all busco groups below here - metazoa - eukaryota db_dir: databases # specify location for databases (or previously-installed databases) db_install_only: False # just install databases, don't run annotation db_extra: annot_extra: ' --quick ' ####################################################### Override default params for any program by placing these lines in your yaml config file, and modifying values as desired. For more details, see Understanding and Configuring Workflows .For more on what parameters are available, see the docs for each specific program or utility rule: dammit","title":"annotate"},{"location":"annotate/#annotate-subworkflow","text":"Subworkflows combine tools in the right order to facilitate file targeting withing elvers . The \"annotate\" subworkflow annotates a transcriptome. It requires an assembly to be provided, either by running an assembly or providing one in your configfile. At the moment, this workflow consists of: dammit","title":"Annotate Subworkflow"},{"location":"annotate/#quickstart","text":"If you're starting a new elvers run using a reference file (even if you have previously built a de novo assembly via elvers ), you need to help elvers find that assembly. Scenario 1: You're starting from your own reference file: You need to provide the reference in your `config.yaml` file: ``` get_reference: reference: input reference fasta REQUIRED gene_trans_map: OPTIONAL: provide a gene to transcript map for input transcriptome reference_extension: '_input' OPTIONAL, changes naming download_ref: the reference entry above is a link that needs to be downloaded use_ftp: download via ftp instead of http ``` Once you add this to your configfile (e.g. `my_config.yaml`), you can run a reference/assembly-based workflow such as annotate. ``` elvers my_config.yaml annotate ``` For more details on reference specification, see the [get_reference documentation](get_reference.md). For annotation configuration, see below. Scenario 2: You've previously run an assembly program via `elvers`: You _can_ provide the built reference in the same manner as above. However, if you're running more workflows in the same directory, you can also just specify the name of the assembly program that you used to generate the assembly. This will *not* rerun the assembly (unless you provide new input files). Instead, this will allow `elvers` to know where to look for your previously-generated reference file. Because we enable multiple referene generation programs, we don't want to assume which reference you'd like to use for downstream steps (in fact, if you provide multiple references, elvers will run the downstream steps on all references, assuming you provide unique reference_extension parameters so that the references are uniquely named. Example: You've previously run the trinity assembly, and want to annotate it. ``` elvers examples/nema.yaml assemble annotate ``` Here, the `assemble` workflow just enables `elvers` to locate your assembly file for `annotate`.","title":"Quickstart"},{"location":"annotate/#output-files","text":"Your main output directory will be determined by your config file: by default it is BASENAME_out (you specify BASENAME). Dammit will output files in the annotation subdirectory of this output directory. The annotated fasta file will be ASSEMBLY.dammit.fasta and the annotation gff3 file will be ASSEMBLY.dammit.gff3 . Dammit will also produce a number of intermediate files that will be contained within an ASSEMBLY.fasta.dammit folder.","title":"Output files:"},{"location":"annotate/#configuring-the-annotate-subworkflow","text":"To set up your sample info and build a configfile, see Understanding and Configuring Workflows . If you want to add the annotate program parameters to a previously built configfile, run: elvers config.yaml annotate --print_params A small set of parameters should print to your console: #################### annotate #################### dammit: busco_group: # specify all busco groups below here - metazoa - eukaryota db_dir: databases # specify location for databases (or previously-installed databases) db_install_only: False # just install databases, don't run annotation db_extra: annot_extra: ' --quick ' ####################################################### Override default params for any program by placing these lines in your yaml config file, and modifying values as desired. For more details, see Understanding and Configuring Workflows .For more on what parameters are available, see the docs for each specific program or utility rule: dammit","title":"Configuring the annotate subworkflow"},{"location":"assemble/","text":"Assemble Subworkflow \u00b6 Subworkflows combine tools in the right order to facilitate file targeting withing elvers . The \"assemble\"\" subworkflow conducts read quality trimming, kmer trimming, and assembly. At the moment, this workflow consists of: get_data - an elvers utility trimmomatic fastqc , run on both pre-trim and post-trim data khmer trinity Quickstart \u00b6 To run the assemble subworkflow, run: elvers examples/nema.yaml assemble Configuring the assemble subworkflow \u00b6 To set up your sample info and build a configfile, see Understanding and Configuring Workflows . If you want to add the assemble program parameters to a previously built configfile, run: elvers config.yaml assemble --print_params A small set of parameters should print to your console: #################### assemble #################### get_data: download_data: false use_ftp: false trimmomatic: adapter_file: pe_path: ep_utils/TruSeq3-PE-2.fa se_path: ep_utils/TruSeq3-SE.fa extra: '' trim_cmd: ILLUMINACLIP:{}:2:40:15 LEADING:2 TRAILING:2 SLIDINGWINDOW:4:15 MINLEN:25 fastqc: extra: '' khmer: C: 3 Z: 18 coverage: 20 diginorm: true extra: '' ksize: 20 memory: 4e9 trinity: add_single_to_paired: false input_kmer_trimmed: true input_trimmomatic_trimmed: false max_memory: 30G seqtype: fq extra: '' ####################################################### Override default params for any program by placing these lines in your yaml config file, and modifying values as desired. For more details, see Understanding and Configuring Workflows .For more on what parameters are available, see the docs for each specific program or utility rule: get_data trimmomatic fastqc khmer trinity","title":"assemble"},{"location":"assemble/#assemble-subworkflow","text":"Subworkflows combine tools in the right order to facilitate file targeting withing elvers . The \"assemble\"\" subworkflow conducts read quality trimming, kmer trimming, and assembly. At the moment, this workflow consists of: get_data - an elvers utility trimmomatic fastqc , run on both pre-trim and post-trim data khmer trinity","title":"Assemble Subworkflow"},{"location":"assemble/#quickstart","text":"To run the assemble subworkflow, run: elvers examples/nema.yaml assemble","title":"Quickstart"},{"location":"assemble/#configuring-the-assemble-subworkflow","text":"To set up your sample info and build a configfile, see Understanding and Configuring Workflows . If you want to add the assemble program parameters to a previously built configfile, run: elvers config.yaml assemble --print_params A small set of parameters should print to your console: #################### assemble #################### get_data: download_data: false use_ftp: false trimmomatic: adapter_file: pe_path: ep_utils/TruSeq3-PE-2.fa se_path: ep_utils/TruSeq3-SE.fa extra: '' trim_cmd: ILLUMINACLIP:{}:2:40:15 LEADING:2 TRAILING:2 SLIDINGWINDOW:4:15 MINLEN:25 fastqc: extra: '' khmer: C: 3 Z: 18 coverage: 20 diginorm: true extra: '' ksize: 20 memory: 4e9 trinity: add_single_to_paired: false input_kmer_trimmed: true input_trimmomatic_trimmed: false max_memory: 30G seqtype: fq extra: '' ####################################################### Override default params for any program by placing these lines in your yaml config file, and modifying values as desired. For more details, see Understanding and Configuring Workflows .For more on what parameters are available, see the docs for each specific program or utility rule: get_data trimmomatic fastqc khmer trinity","title":"Configuring the assemble subworkflow"},{"location":"configure/","text":"Understanding and Configuring Eelpond Workflows \u00b6 elvers is designed to facilitate running standard workflows and analyses for sequence data. It integrates snakemake rules for commonly used tools, and provides several end-to-end protocols for analyzing RNAseq data. Workflows are highly customizable, and all command-line options for each tool are available for modification via the configuration file. elvers uses the yaml Yet Another Markup Language format to specify data inputs and modify run parameters. The only required information for this file is the location of the data inputs (reads, assembly, or both). Specifying Input Data \u00b6 Let's start with the input data. There are two types of data that can go into elvers: read data (gzipped fastq files), and assembly files (fasta) files. Read Input \u00b6 To specify input data, we need to build a tab-separated samples file, e.g. my-samples.tsv . This file tells elvers a name for each samples, and provides a location for the fastq files (local file path, or downloadable link). If we had our test data within the data folder in the main elvers directory, the file would look like this: ``` sample unit fq1 fq2 condition 0Hour 001 data/0Hour_ATCACG_L002_R1_001.extract.fastq.gz data/0Hour_ATCACG_L002_R2_001.extract.fastq.gz time0 0Hour 002 data/0Hour_ATCACG_L002_R1_002.extract.fastq.gz data/0Hour_ATCACG_L002_R2_002.extract.fastq.gz time0 6Hour 001 data/6Hour_CGATGT_L002_R1_001.extract.fastq.gz data/6Hour_CGATGT_L002_R2_001.extract.fastq.gz time6 6Hour 002 data/6Hour_CGATGT_L002_R1_002.extract.fastq.gz data/6Hour_CGATGT_L002_R2_002.extract.fastq.gz time6 ``` If we want to download the test data instead: ``` sample unit fq1 fq2 condition 0Hour 001 https://osf.io/vw4dt/download https://osf.io/b47s2/download time0 0Hour 002 https://osf.io/92jr6/download https://osf.io/qzea8/download time0 6Hour 001 https://osf.io/ft3am/download https://osf.io/jqmsx/download time6 6Hour 002 https://osf.io/rnkq3/download https://osf.io/bt9vh/download time6 ``` Note that proper formatting means all columns must be separated by tabs, not spaces! Now, you need to provide the name and location of this file to elvers . We do this in the config.yaml file, like so: get_data: samples: /path/to/my-samples.tsv The default functionality is to link data from another location on your computer into elvers 's output directory. However, if you want to download data instead, you'll need to provide links in the my-samples.tsv file, and add a few lines to your config.yaml : get_data: samples: /path/to/my-samples.tsv download_data: True use_ftp: False # set to true if you want to use FTP instead of HTTP About the TSV and Input Reads: The sample names provided here are used to name file outputs throughout the workflow. The \"unit\" column is designed to facilitate combining samples that were sequenced over multiple lanes or in different batches. If you do not have \"unit\" information, please add a short placeholder, such as \"a\". for now, the column headers must be: sample , unit , fq1 , fq2 , condition . Additional headers are not a problem, but will not be used. If you have single-end samples, please be sure to include the fq2 column, but leave the column blank the condition column is used for differential expression comparisions in DESeq2. The \"condition\" values can be anything, but they need to correspond with the \"contrast\" information you pass into DESeq2. If using the diffexp workflow, see our docs here and be sure to add the differential expression information into your yaml configuration file. Formatting the tsv can be a bit annoying. It's slightly easier if you start from a working version by copying the sample data to a new file (see below). At the moment, elvers assumes all input data is gzipped , so please input gzipped data! If you'd like to start from a working version, copy (and then modify) the sample data: cp examples/nema.samples.tsv my_samples.tsv To run any read-based workflow, we need to get reads (and any assemblies already generated) into the right format for elvers . We can either (1) link the data from another location on your machine, or (2) download the data via http or ftp. This is done through a utility rule called get_data . Reference Input \u00b6 If you're starting a new elvers run using a reference file (even if you have previously built a de novo assembly via elvers ), you need to help elvers find that assembly. Scenario 1: You're starting from your own reference file: You need to provide the reference in your `config.yaml` file: ``` get_reference: reference: input reference fasta REQUIRED gene_trans_map: OPTIONAL: provide a gene to transcript map for input transcriptome reference_extension: '_input' OPTIONAL, changes naming download_ref: the reference entry above is a link that needs to be downloaded use_ftp: download via ftp instead of http ``` Once you add this to your configfile (e.g. `my_config.yaml`), you can run a reference/assembly-based workflow such as annotate. ``` elvers my_config.yaml annotate ``` For more details on reference specification, see the [get_reference documentation](get_reference.md). For annotation configuration, see below. Scenario 2: You've previously run an assembly program via `elvers`: You _can_ provide the built reference in the same manner as above. However, if you're running more workflows in the same directory, you can also just specify the name of the assembly program that you used to generate the assembly. This will *not* rerun the assembly (unless you provide new input files). Instead, this will allow `elvers` to know where to look for your previously-generated reference file. Because we enable multiple referene generation programs, we don't want to assume which reference you'd like to use for downstream steps (in fact, if you provide multiple references, elvers will run the downstream steps on all references, assuming you provide unique reference_extension parameters so that the references are uniquely named. Example: You've previously run the trinity assembly, and want to annotate it. ``` elvers examples/nema.yaml assemble annotate ``` Here, the `assemble` workflow just enables `elvers` to locate your assembly file for `annotate`. Choosing and running a workflow \u00b6 We offer a number of workflows, including end-to-end workflows that conduct assembly through differential expression analysis. The default workflow is the Eel Pond RNAseq workflow , which conducts de novo transcriptome assembly, annotation, and quick differential expression analysis on a set of short-read Illumina data using a single command. Available Workflows \u00b6 Currently, all workflows require a properly-formatted read inputs tsv file as input. Some workflows, e.g. annotation , can work on either on a de novo transcriptome generated by elvers , or on previously-generated assemblies. To add an assembly as input, specify it via get_reference in the yaml config file, as described above. workflows preprocess: Read Quality Trimming and Filtering (fastqc, trimmomatic) kmer_trim: Kmer Trimming and/or Digital Normalization (khmer) assemble: Transcriptome Assembly (trinity) get_reference: Specify assembly for downstream steps annotate : Annotate the transcriptome (dammit) sourmash_compute: Build sourmash signatures for the reads and assembly (sourmash) quantify: Quantify transcripts (salmon) diffexp: Conduct differential expression (DESeq2) plass_assemble: assemble at the protein level with PLASS paladin_map: map to a protein assembly using paladin end-to-end workflows: default : preprocess, kmer_trim, assemble, annotate, quantify protein assembly : preprocess, kmer_trim, plass_assemble, paladin_map You can see the available workflows (and which programs they run) by using the --print_workflows flag: elvers examples/nema.yaml --print_workflows Each included tool can also be run independently, if appropriate input files are provided. This is not always intuitive, so please see our documentation for running each tools for details (described as \"Advanced Usage\"). To see all available tools, run: elvers examples/nema.yaml --print_rules Configuring Parameters for a workflow \u00b6 For any workflow, we need to provide a configuration file that specifies the path to your samples file or get_reference (discussed above). We can generate this file either by: Adding just the desired parameters Allowing elvers to build a (long) full configfile for us, and modifying as desired. Option 1: Adding just the required parameters The configuration file primarily provides the location of the input data and/or input assembly. The simplest config file contains just this information. get_data: samples: samples.tsv or get_reference: reference: assembly.fasta There are a few other options we can add to customize the name of the output directory and files. basename: NAME : helps determine file names and output directory (by default: BASENAME_out ) experiment: EXPERIMENT : some additional \"experiment\" info to add to the output directory name ( outdir: BASENAME_EXPERIMENT_out ) out_path: /full/path : if you want to redirect the output to some location not under the elvers directory.` Now, if you'd like to run any particular program with non-default parameters, or you're running differential expression analysis, you'll need to add some info to the config. For any (each) program, follow this format to see program params: elvers config.yaml <PROGRAM> --print_params For example, for deseq2: elvers config.yaml deseq2 --print_params Then copy and paste the parameters that show up in your terminal into your config file. Please see each program's documentation for additional info on what (and how) to modify each program. For example, if running an assembly, we definitely recommend modifying the max_memory parameter of Trinity. Option 2: Use elvers to add all parameters for our workflow of choice To get a configfile for the default \"eel pond\" workflow, that you can modify, run: elvers my_workflow.yaml --build_config The output should be a yaml configfile. At the top, you should see: # # elvers workflow configuration # basename: elvers experiment: _experiment1 First, change the samples.tsv name to your my_samples.tsv file. Then, modify the basename and any experiment info you'd like to add. The default output directory will be: basename_experiment_out within the main elvers directory. If you'd like, you can add one more parameter to the top section: out_path: OUTPUT_PATH , if you'd like the output to go somewhere other than the elvers directory. Note the basename and experiment are still used to determine the output directory name. Customizing program parameters: Below this section, you should see some parameters for each program run in this workflow. For example, here's the first few programs: a utility to download or link your data, quality trim with trimmomtic, and assess quality with fastqc. Program parameters do not always show up in order in this file - order in this file does not affect program run order. get_data: samples: samples.tsv download_data: false use_ftp: false trimmomatic: adapter_file: pe_path: ep_utils/TruSeq3-PE-2.fa se_path: ep_utils/TruSeq3-SE.fa trim_cmd: ILLUMINACLIP:{}:2:40:15 LEADING:2 TRAILING:2 SLIDINGWINDOW:4:15 MINLEN:25 extra: '' fastqc: extra: '' Override default params for any program by modifying the values for any of the paramters under that program name. For example, if you'd like to download data instead of link it from another location on your machine, modify download_data to True under the get_data program. We provide an extra parameter wherever possible to give you access to additional command-line parameters for each program that we have not specifically enabled changes for. For example, under the trimmomatic section, you can modify the \"extra\" param to pass any extra trimmomatic parameters, e.g.: trimmomatic: extra: 'HEADCROP:5' # to remove the first 5 bases at the front of the read. For more on what parameters are available, see the docs for each specific program or utility rule under the \"Available Workflows\" --> \"Programs Used\" navigation tab.","title":"Choosing and Configuring Workflows"},{"location":"configure/#understanding-and-configuring-eelpond-workflows","text":"elvers is designed to facilitate running standard workflows and analyses for sequence data. It integrates snakemake rules for commonly used tools, and provides several end-to-end protocols for analyzing RNAseq data. Workflows are highly customizable, and all command-line options for each tool are available for modification via the configuration file. elvers uses the yaml Yet Another Markup Language format to specify data inputs and modify run parameters. The only required information for this file is the location of the data inputs (reads, assembly, or both).","title":"Understanding and Configuring Eelpond Workflows"},{"location":"configure/#specifying-input-data","text":"Let's start with the input data. There are two types of data that can go into elvers: read data (gzipped fastq files), and assembly files (fasta) files.","title":"Specifying Input Data"},{"location":"configure/#read-input","text":"To specify input data, we need to build a tab-separated samples file, e.g. my-samples.tsv . This file tells elvers a name for each samples, and provides a location for the fastq files (local file path, or downloadable link). If we had our test data within the data folder in the main elvers directory, the file would look like this: ``` sample unit fq1 fq2 condition 0Hour 001 data/0Hour_ATCACG_L002_R1_001.extract.fastq.gz data/0Hour_ATCACG_L002_R2_001.extract.fastq.gz time0 0Hour 002 data/0Hour_ATCACG_L002_R1_002.extract.fastq.gz data/0Hour_ATCACG_L002_R2_002.extract.fastq.gz time0 6Hour 001 data/6Hour_CGATGT_L002_R1_001.extract.fastq.gz data/6Hour_CGATGT_L002_R2_001.extract.fastq.gz time6 6Hour 002 data/6Hour_CGATGT_L002_R1_002.extract.fastq.gz data/6Hour_CGATGT_L002_R2_002.extract.fastq.gz time6 ``` If we want to download the test data instead: ``` sample unit fq1 fq2 condition 0Hour 001 https://osf.io/vw4dt/download https://osf.io/b47s2/download time0 0Hour 002 https://osf.io/92jr6/download https://osf.io/qzea8/download time0 6Hour 001 https://osf.io/ft3am/download https://osf.io/jqmsx/download time6 6Hour 002 https://osf.io/rnkq3/download https://osf.io/bt9vh/download time6 ``` Note that proper formatting means all columns must be separated by tabs, not spaces! Now, you need to provide the name and location of this file to elvers . We do this in the config.yaml file, like so: get_data: samples: /path/to/my-samples.tsv The default functionality is to link data from another location on your computer into elvers 's output directory. However, if you want to download data instead, you'll need to provide links in the my-samples.tsv file, and add a few lines to your config.yaml : get_data: samples: /path/to/my-samples.tsv download_data: True use_ftp: False # set to true if you want to use FTP instead of HTTP About the TSV and Input Reads: The sample names provided here are used to name file outputs throughout the workflow. The \"unit\" column is designed to facilitate combining samples that were sequenced over multiple lanes or in different batches. If you do not have \"unit\" information, please add a short placeholder, such as \"a\". for now, the column headers must be: sample , unit , fq1 , fq2 , condition . Additional headers are not a problem, but will not be used. If you have single-end samples, please be sure to include the fq2 column, but leave the column blank the condition column is used for differential expression comparisions in DESeq2. The \"condition\" values can be anything, but they need to correspond with the \"contrast\" information you pass into DESeq2. If using the diffexp workflow, see our docs here and be sure to add the differential expression information into your yaml configuration file. Formatting the tsv can be a bit annoying. It's slightly easier if you start from a working version by copying the sample data to a new file (see below). At the moment, elvers assumes all input data is gzipped , so please input gzipped data! If you'd like to start from a working version, copy (and then modify) the sample data: cp examples/nema.samples.tsv my_samples.tsv To run any read-based workflow, we need to get reads (and any assemblies already generated) into the right format for elvers . We can either (1) link the data from another location on your machine, or (2) download the data via http or ftp. This is done through a utility rule called get_data .","title":"Read Input"},{"location":"configure/#reference-input","text":"If you're starting a new elvers run using a reference file (even if you have previously built a de novo assembly via elvers ), you need to help elvers find that assembly. Scenario 1: You're starting from your own reference file: You need to provide the reference in your `config.yaml` file: ``` get_reference: reference: input reference fasta REQUIRED gene_trans_map: OPTIONAL: provide a gene to transcript map for input transcriptome reference_extension: '_input' OPTIONAL, changes naming download_ref: the reference entry above is a link that needs to be downloaded use_ftp: download via ftp instead of http ``` Once you add this to your configfile (e.g. `my_config.yaml`), you can run a reference/assembly-based workflow such as annotate. ``` elvers my_config.yaml annotate ``` For more details on reference specification, see the [get_reference documentation](get_reference.md). For annotation configuration, see below. Scenario 2: You've previously run an assembly program via `elvers`: You _can_ provide the built reference in the same manner as above. However, if you're running more workflows in the same directory, you can also just specify the name of the assembly program that you used to generate the assembly. This will *not* rerun the assembly (unless you provide new input files). Instead, this will allow `elvers` to know where to look for your previously-generated reference file. Because we enable multiple referene generation programs, we don't want to assume which reference you'd like to use for downstream steps (in fact, if you provide multiple references, elvers will run the downstream steps on all references, assuming you provide unique reference_extension parameters so that the references are uniquely named. Example: You've previously run the trinity assembly, and want to annotate it. ``` elvers examples/nema.yaml assemble annotate ``` Here, the `assemble` workflow just enables `elvers` to locate your assembly file for `annotate`.","title":"Reference Input"},{"location":"configure/#choosing-and-running-a-workflow","text":"We offer a number of workflows, including end-to-end workflows that conduct assembly through differential expression analysis. The default workflow is the Eel Pond RNAseq workflow , which conducts de novo transcriptome assembly, annotation, and quick differential expression analysis on a set of short-read Illumina data using a single command.","title":"Choosing and running a workflow"},{"location":"configure/#available-workflows","text":"Currently, all workflows require a properly-formatted read inputs tsv file as input. Some workflows, e.g. annotation , can work on either on a de novo transcriptome generated by elvers , or on previously-generated assemblies. To add an assembly as input, specify it via get_reference in the yaml config file, as described above. workflows preprocess: Read Quality Trimming and Filtering (fastqc, trimmomatic) kmer_trim: Kmer Trimming and/or Digital Normalization (khmer) assemble: Transcriptome Assembly (trinity) get_reference: Specify assembly for downstream steps annotate : Annotate the transcriptome (dammit) sourmash_compute: Build sourmash signatures for the reads and assembly (sourmash) quantify: Quantify transcripts (salmon) diffexp: Conduct differential expression (DESeq2) plass_assemble: assemble at the protein level with PLASS paladin_map: map to a protein assembly using paladin end-to-end workflows: default : preprocess, kmer_trim, assemble, annotate, quantify protein assembly : preprocess, kmer_trim, plass_assemble, paladin_map You can see the available workflows (and which programs they run) by using the --print_workflows flag: elvers examples/nema.yaml --print_workflows Each included tool can also be run independently, if appropriate input files are provided. This is not always intuitive, so please see our documentation for running each tools for details (described as \"Advanced Usage\"). To see all available tools, run: elvers examples/nema.yaml --print_rules","title":"Available Workflows"},{"location":"configure/#configuring-parameters-for-a-workflow","text":"For any workflow, we need to provide a configuration file that specifies the path to your samples file or get_reference (discussed above). We can generate this file either by: Adding just the desired parameters Allowing elvers to build a (long) full configfile for us, and modifying as desired. Option 1: Adding just the required parameters The configuration file primarily provides the location of the input data and/or input assembly. The simplest config file contains just this information. get_data: samples: samples.tsv or get_reference: reference: assembly.fasta There are a few other options we can add to customize the name of the output directory and files. basename: NAME : helps determine file names and output directory (by default: BASENAME_out ) experiment: EXPERIMENT : some additional \"experiment\" info to add to the output directory name ( outdir: BASENAME_EXPERIMENT_out ) out_path: /full/path : if you want to redirect the output to some location not under the elvers directory.` Now, if you'd like to run any particular program with non-default parameters, or you're running differential expression analysis, you'll need to add some info to the config. For any (each) program, follow this format to see program params: elvers config.yaml <PROGRAM> --print_params For example, for deseq2: elvers config.yaml deseq2 --print_params Then copy and paste the parameters that show up in your terminal into your config file. Please see each program's documentation for additional info on what (and how) to modify each program. For example, if running an assembly, we definitely recommend modifying the max_memory parameter of Trinity. Option 2: Use elvers to add all parameters for our workflow of choice To get a configfile for the default \"eel pond\" workflow, that you can modify, run: elvers my_workflow.yaml --build_config The output should be a yaml configfile. At the top, you should see: # # elvers workflow configuration # basename: elvers experiment: _experiment1 First, change the samples.tsv name to your my_samples.tsv file. Then, modify the basename and any experiment info you'd like to add. The default output directory will be: basename_experiment_out within the main elvers directory. If you'd like, you can add one more parameter to the top section: out_path: OUTPUT_PATH , if you'd like the output to go somewhere other than the elvers directory. Note the basename and experiment are still used to determine the output directory name. Customizing program parameters: Below this section, you should see some parameters for each program run in this workflow. For example, here's the first few programs: a utility to download or link your data, quality trim with trimmomtic, and assess quality with fastqc. Program parameters do not always show up in order in this file - order in this file does not affect program run order. get_data: samples: samples.tsv download_data: false use_ftp: false trimmomatic: adapter_file: pe_path: ep_utils/TruSeq3-PE-2.fa se_path: ep_utils/TruSeq3-SE.fa trim_cmd: ILLUMINACLIP:{}:2:40:15 LEADING:2 TRAILING:2 SLIDINGWINDOW:4:15 MINLEN:25 extra: '' fastqc: extra: '' Override default params for any program by modifying the values for any of the paramters under that program name. For example, if you'd like to download data instead of link it from another location on your machine, modify download_data to True under the get_data program. We provide an extra parameter wherever possible to give you access to additional command-line parameters for each program that we have not specifically enabled changes for. For example, under the trimmomatic section, you can modify the \"extra\" param to pass any extra trimmomatic parameters, e.g.: trimmomatic: extra: 'HEADCROP:5' # to remove the first 5 bases at the front of the read. For more on what parameters are available, see the docs for each specific program or utility rule under the \"Available Workflows\" --> \"Programs Used\" navigation tab.","title":"Configuring Parameters for a workflow"},{"location":"dammit/","text":"Annotating de novo transcriptomes with dammit \u00b6 dammit is an annotation pipeline written by Camille Scott . dammit runs a relatively standard annotation protocol for transcriptomes: it begins by building gene models with Transdecoder , and then uses the following protein databases as evidence for annotation: Swiss-Prot (manually reviewed and curated) Pfam-A Rfam OrthoDB uniref90 (uniref is optional with --full ). nr (nr is optional with --nr ). If a protein dataset is available, this can also be supplied to the dammit pipeline with --user-databases as optional evidence for annotation. In addition, BUSCO v3 is run, which will compare the gene content in your transcriptome with a lineage-specific data set. The output is a proportion of your transcriptome that matches with the data set, which can be used as an estimate of the completeness of your transcriptome based on evolutionary expectation ( Simho et al.2015 ). There are several lineage-specific datasets available from the authors of BUSCO. We will use the metazoa dataset for this transcriptome. Computational Requirements \u00b6 For the standard pipeline, dammit needs ~18GB of storage space to store its prepared databases, plus a few hundred MB per BUSCO database. For the standard annotation pipeline, we recommend at least 16GB of RAM. This can be reduced by editing LAST parameters via a custom configuration file. The full pipeline, which uses uniref90, needs several hundred GB of space and considerable RAM to prepare the databases. You'll also want either a fat internet connection or a big cup of patience to download uniref. For some species, we have found that the amount of RAM required can be proportional to the size of the transcriptome being annotated. While dammit runs, it will print out which tasks its running to the terminal. dammit is written with a library called pydoit , which is a python workflow library similar to GNU Make and Snakemake. This not only helps organize the underlying workflow, but also means that if we interrupt it, it should properly resume! Caveat: if your job dies, without properly shutting down, snakemake will leave your directory \"locked\" (a safety feature to prevent two runs/programs from editing the same file simultaneously). If this happens, you'll need to run elvers with the --unlock flag. Dammit Commands \u00b6 dammit has two major subcommands: dammit databases and dammit annotate . databases checks that the databases are installed and prepared, and if run with the --install flag, will perform that installation and preparation. annotate then performs the annotation using installed tools and databases. These commands are automated and integrated into a single rule in elvers . At some point, these may be split into databases and annotate if that functionality is desired, but it's not in our immediate plans. By default, databases are placed at elvers/databases . Both databases and annotate have a --quick option, that only installs or runs a \"quick\" version of the pipeline: transdecoder On the command line, the commands we would run are as follows: Databases \u00b6 #install databases dammit databases --install --busco-group metazoa --busco-group eukaryota Note: if you have limited space on your instance, you can also install these databases in a different location (e.g. on an external volume) by adding --database-dir /path/to/databases . You can also use a custom protein database for your species. If your critter is a non-model organism, you will likely need to create your own with proteins from closely-related species. This will rely on your knowledge of your system! Annotation \u00b6 dammit annotate trinity.nema.fasta --busco-group metazoa --user-databases <your-database-here> --n_threads 6 If you want to run a quick version of the pipeline, add a parameter, --quick , to omit OrthoDB, Uniref, Pfam, and Rfam. A \"full\" run will take longer to install and run, but you'll have access to the full annotation pipeline. Running Dammit \u00b6 Run Dammit via the \"default\" Eel Pond workflow or via the annotate subworkflow . To run Dammit as a standalone program, see \"Advanced Usage\" section below. Modifying Params for Dammit: \u00b6 Be sure to set up your sample info and build a configfile first (see Understanding and Configuring Workflows ). To see the available parameters for the dammit rule, run elvers config dammit --print_params This will print the following: #################### dammit #################### dammit: busco_group: # specify all busco groups below here - metazoa - eukaryota db_dir: databases # specify location for databases (or previously installed databases) db_install_only: False # just install databases, don't run annotation db_extra: '' annot_extra: ' --quick ' ##################################################### In addition to changing parameters we've specifically enabled, you can modify the extra param to pass any extra parameters. In dammit, both databases and annotation take an extra param: db_extra: '--someflag someparam --someotherflag thatotherparam' annot_extra: '--someflag someparam --someotherflag thatotherparam' Within the \"default\" Eel Pond workflow , this will annotate the Trinity assembly. Since Dammit requires an assembly as input, see the annotate subworkflow instructions for how to run dammit as a standalone rule. See the dammit documentation for all params you can pass into Dammit. Dammit Output \u00b6 Your main output directory will be determined by your config file: by default it is BASENAME_out (you specify BASENAME). Dammit will output files in the annotation subdirectory of this output directory. The annotated fasta file will be ASSEMBLY.dammit.fasta and the annotation gff3 file will be ASSEMBLY.dammit.gff3 . Dammit will also produce a number of intermediate files that will be contained within an ASSEMBLY.fasta.dammit folder. After a successful run, you'll have a new directory called BASENAME.fasta.dammit . If you look inside, you'll see a lot of files. For example, for a transcriptome with basename trinity.nema , the folder trinity.nema.fasta.dammit should contain the following files after a standard (not --quick ) run: ls trinity.nema.fasta.dammit/ annotate.doit.db trinity.nema.fasta.dammit.namemap.csv trinity.nema.fasta.transdecoder.pep dammit.log trinity.nema.fasta.dammit.stats.json trinity.nema.fasta.x.nema.reference.prot.faa.crbl.csv run_trinity.nema.fasta.metazoa.busco.results trinity.nema.fasta.transdecoder.bed trinity.nema.fasta.x.nema.reference.prot.faa.crbl.gff3 tmp trinity.nema.fasta.transdecoder.cds trinity.nema.fasta.x.nema.reference.prot.faa.crbl.model.csv trinity.nema.fasta trinity.nema.fasta.transdecoder_dir trinity.nema.fasta.x.nema.reference.prot.faa.crbl.model.plot.pdf trinity.nema.fasta.dammit.fasta trinity.nema.fasta.transdecoder.gff3 trinity.nema.fasta.dammit.gff3 trinity.nema.fasta.transdecoder.mRNA As part of elvers, we copy the two most important files, trinity.nema.fasta.dammit.fasta and trinity.nema.fasta.dammit.gff3 into the main annotation directory. trinity.nema.fasta.dammit.stats.json also gives summary stats that are quite useful. If the above dammit command is run again, there will be a message: **Pipeline is already completed!** If you'd like to rerun the dammit pipeline, you'll need to use the --forceall flag, like so: elvers examples/nema.yaml annotation --forceall Additional Notes (non-elvers): Parsing Dammit GFF3 files \u00b6 Camille wrote dammit in Python, which includes a library to parse gff3 dammit output. If you want to work with this gff3 downstream, use htis parsing library: First, enter in a dammit environment. you can find the one elvers uses, but it might also be easier to just make a new one using the dammit environment file: # from within the elvers directory conda env create -n dammit --file rules/dammit/dammit-env.yaml source activate dammit Remember you can exit your conda environments with source deactivate Then: cd trinity.nema.fasta.dammit python Use gff3 librarys to output a list of gene IDs: import pandas as pd from dammit.fileio.gff3 import GFF3Parser gff_file = \"trinity.nema.fasta.dammit.gff3\" annotations = GFF3Parser(filename=gff_file).read() names = annotations.sort_values(by=['seqid', 'score'], ascending=True).query('score < 1e-05').drop_duplicates(subset='seqid')[['seqid', 'Name']] new_file = names.dropna(axis=0,how='all') new_file.head() new_file.to_csv(\"nema_gene_name_id.csv\") exit() This will output a table of genes with 'seqid' and 'Name' in a .csv file: nema_gene_name_id.csv . Let's take a look at that file: less nema_gene_name_id.csv Notice there are multiple transcripts per gene model prediction. This .csv file can be used in tximport in downstream DE analysis.","title":"dammit"},{"location":"dammit/#annotating-de-novo-transcriptomes-with-dammit","text":"dammit is an annotation pipeline written by Camille Scott . dammit runs a relatively standard annotation protocol for transcriptomes: it begins by building gene models with Transdecoder , and then uses the following protein databases as evidence for annotation: Swiss-Prot (manually reviewed and curated) Pfam-A Rfam OrthoDB uniref90 (uniref is optional with --full ). nr (nr is optional with --nr ). If a protein dataset is available, this can also be supplied to the dammit pipeline with --user-databases as optional evidence for annotation. In addition, BUSCO v3 is run, which will compare the gene content in your transcriptome with a lineage-specific data set. The output is a proportion of your transcriptome that matches with the data set, which can be used as an estimate of the completeness of your transcriptome based on evolutionary expectation ( Simho et al.2015 ). There are several lineage-specific datasets available from the authors of BUSCO. We will use the metazoa dataset for this transcriptome.","title":"Annotating de novo transcriptomes with dammit"},{"location":"dammit/#computational-requirements","text":"For the standard pipeline, dammit needs ~18GB of storage space to store its prepared databases, plus a few hundred MB per BUSCO database. For the standard annotation pipeline, we recommend at least 16GB of RAM. This can be reduced by editing LAST parameters via a custom configuration file. The full pipeline, which uses uniref90, needs several hundred GB of space and considerable RAM to prepare the databases. You'll also want either a fat internet connection or a big cup of patience to download uniref. For some species, we have found that the amount of RAM required can be proportional to the size of the transcriptome being annotated. While dammit runs, it will print out which tasks its running to the terminal. dammit is written with a library called pydoit , which is a python workflow library similar to GNU Make and Snakemake. This not only helps organize the underlying workflow, but also means that if we interrupt it, it should properly resume! Caveat: if your job dies, without properly shutting down, snakemake will leave your directory \"locked\" (a safety feature to prevent two runs/programs from editing the same file simultaneously). If this happens, you'll need to run elvers with the --unlock flag.","title":"Computational Requirements"},{"location":"dammit/#dammit-commands","text":"dammit has two major subcommands: dammit databases and dammit annotate . databases checks that the databases are installed and prepared, and if run with the --install flag, will perform that installation and preparation. annotate then performs the annotation using installed tools and databases. These commands are automated and integrated into a single rule in elvers . At some point, these may be split into databases and annotate if that functionality is desired, but it's not in our immediate plans. By default, databases are placed at elvers/databases . Both databases and annotate have a --quick option, that only installs or runs a \"quick\" version of the pipeline: transdecoder On the command line, the commands we would run are as follows:","title":"Dammit Commands"},{"location":"dammit/#databases","text":"#install databases dammit databases --install --busco-group metazoa --busco-group eukaryota Note: if you have limited space on your instance, you can also install these databases in a different location (e.g. on an external volume) by adding --database-dir /path/to/databases . You can also use a custom protein database for your species. If your critter is a non-model organism, you will likely need to create your own with proteins from closely-related species. This will rely on your knowledge of your system!","title":"Databases"},{"location":"dammit/#annotation","text":"dammit annotate trinity.nema.fasta --busco-group metazoa --user-databases <your-database-here> --n_threads 6 If you want to run a quick version of the pipeline, add a parameter, --quick , to omit OrthoDB, Uniref, Pfam, and Rfam. A \"full\" run will take longer to install and run, but you'll have access to the full annotation pipeline.","title":"Annotation"},{"location":"dammit/#running-dammit","text":"Run Dammit via the \"default\" Eel Pond workflow or via the annotate subworkflow . To run Dammit as a standalone program, see \"Advanced Usage\" section below.","title":"Running Dammit"},{"location":"dammit/#modifying-params-for-dammit","text":"Be sure to set up your sample info and build a configfile first (see Understanding and Configuring Workflows ). To see the available parameters for the dammit rule, run elvers config dammit --print_params This will print the following: #################### dammit #################### dammit: busco_group: # specify all busco groups below here - metazoa - eukaryota db_dir: databases # specify location for databases (or previously installed databases) db_install_only: False # just install databases, don't run annotation db_extra: '' annot_extra: ' --quick ' ##################################################### In addition to changing parameters we've specifically enabled, you can modify the extra param to pass any extra parameters. In dammit, both databases and annotation take an extra param: db_extra: '--someflag someparam --someotherflag thatotherparam' annot_extra: '--someflag someparam --someotherflag thatotherparam' Within the \"default\" Eel Pond workflow , this will annotate the Trinity assembly. Since Dammit requires an assembly as input, see the annotate subworkflow instructions for how to run dammit as a standalone rule. See the dammit documentation for all params you can pass into Dammit.","title":"Modifying Params for Dammit:"},{"location":"dammit/#dammit-output","text":"Your main output directory will be determined by your config file: by default it is BASENAME_out (you specify BASENAME). Dammit will output files in the annotation subdirectory of this output directory. The annotated fasta file will be ASSEMBLY.dammit.fasta and the annotation gff3 file will be ASSEMBLY.dammit.gff3 . Dammit will also produce a number of intermediate files that will be contained within an ASSEMBLY.fasta.dammit folder. After a successful run, you'll have a new directory called BASENAME.fasta.dammit . If you look inside, you'll see a lot of files. For example, for a transcriptome with basename trinity.nema , the folder trinity.nema.fasta.dammit should contain the following files after a standard (not --quick ) run: ls trinity.nema.fasta.dammit/ annotate.doit.db trinity.nema.fasta.dammit.namemap.csv trinity.nema.fasta.transdecoder.pep dammit.log trinity.nema.fasta.dammit.stats.json trinity.nema.fasta.x.nema.reference.prot.faa.crbl.csv run_trinity.nema.fasta.metazoa.busco.results trinity.nema.fasta.transdecoder.bed trinity.nema.fasta.x.nema.reference.prot.faa.crbl.gff3 tmp trinity.nema.fasta.transdecoder.cds trinity.nema.fasta.x.nema.reference.prot.faa.crbl.model.csv trinity.nema.fasta trinity.nema.fasta.transdecoder_dir trinity.nema.fasta.x.nema.reference.prot.faa.crbl.model.plot.pdf trinity.nema.fasta.dammit.fasta trinity.nema.fasta.transdecoder.gff3 trinity.nema.fasta.dammit.gff3 trinity.nema.fasta.transdecoder.mRNA As part of elvers, we copy the two most important files, trinity.nema.fasta.dammit.fasta and trinity.nema.fasta.dammit.gff3 into the main annotation directory. trinity.nema.fasta.dammit.stats.json also gives summary stats that are quite useful. If the above dammit command is run again, there will be a message: **Pipeline is already completed!** If you'd like to rerun the dammit pipeline, you'll need to use the --forceall flag, like so: elvers examples/nema.yaml annotation --forceall","title":"Dammit Output"},{"location":"dammit/#additional-notes-non-elvers-parsing-dammit-gff3-files","text":"Camille wrote dammit in Python, which includes a library to parse gff3 dammit output. If you want to work with this gff3 downstream, use htis parsing library: First, enter in a dammit environment. you can find the one elvers uses, but it might also be easier to just make a new one using the dammit environment file: # from within the elvers directory conda env create -n dammit --file rules/dammit/dammit-env.yaml source activate dammit Remember you can exit your conda environments with source deactivate Then: cd trinity.nema.fasta.dammit python Use gff3 librarys to output a list of gene IDs: import pandas as pd from dammit.fileio.gff3 import GFF3Parser gff_file = \"trinity.nema.fasta.dammit.gff3\" annotations = GFF3Parser(filename=gff_file).read() names = annotations.sort_values(by=['seqid', 'score'], ascending=True).query('score < 1e-05').drop_duplicates(subset='seqid')[['seqid', 'Name']] new_file = names.dropna(axis=0,how='all') new_file.head() new_file.to_csv(\"nema_gene_name_id.csv\") exit() This will output a table of genes with 'seqid' and 'Name' in a .csv file: nema_gene_name_id.csv . Let's take a look at that file: less nema_gene_name_id.csv Notice there are multiple transcripts per gene model prediction. This .csv file can be used in tximport in downstream DE analysis.","title":"Additional Notes (non-elvers): Parsing Dammit GFF3 files"},{"location":"deseq2/","text":"Differential expression analysis with DESeq2 \u00b6 We can use DESeq2 to compare gene expression differences in samples between experimental conditions. Quickstart: Running DESeq2 via elvers \u00b6 We recommend you run deseq2 via the diffexp subworkflow. If you want to run it as a standalone program instead, you need to have generated read quantification data via salmon . 1) If you have salmon results, run: elvers examples/nema.yaml deseq2 2) If not, you need to run salmon and any other missing steps. It's probably best to run the diffexp subworkflow, but you can also try: elvers examples/nema.yaml salmon deseq2 DESeq2 Commands \u00b6 This pipeline uses snakemake to run a few R scripts to conduct basic differential expression analysis. We read in transcript abundance information (generated with salmon ) via tximport . Note that in the salmon step, we combine files of all \"units\" within a sample in order to then conduct differential expression at the sample level. We assume the assembly has a gene-to-transcript map, such as the one produced via trinity. This is a tab separated file ( transcript \\t gene ) that enables count data to be aggregated at the gene level prior to differntial expression analysis. This is recommended, see Soneson et al, 2016 . However, if you do not have this mapping, we provide an option to conduct differential expression at the transcript level via the config (see \"Customizing DESeq2 Parameters\" section, below). After reading in count data, we take in two additional pieces of information: first, the sample names in the samples.tsv document, and second the desired contrast , provided as part of the DESeq2 parameters, below. We store all data in an .rds r data format to support easy reloading of this data for additional user analyses. In addition, we plot a PCA of the normalized counts and perform a standard DESeq2 analysis and print a tsv of results for each contrast specified in the deseq2 params. You can find these R scripts in the elvers github repo . The snakemake rules and scripts were modified from rna-seq-star-deseq2 workflow and our own data analysis and workshops, e.g. DIBSI-RNAseq . Modifying Params for DESeq2 \u00b6 Be sure to set up your sample info and build a configfile first (see Understanding and Configuring Workflows ). To see the available parameters for the deseq2 rule, run elvers config deseq2 --print_params This will print the following: #################### deseq2 #################### deseq2: contrasts: time0-vs-time6: - time0 - time6 gene_trans_map: true pca: labels: - condition ##################################################### The default contrasts reflect the condition information in the test data nema_samples.tsv . Please modify the contrasts to the reflect your data. Multiple contrasts should be supported: each contrast needs a name, and a list below it specifying the conditions to compare, e.g.: contrasts: my-contrast: - conditionA - conditionB The pca labels should not be changed unless you need to change the name of the condition column in the samples.tsv . This functionality hasn't been extensively tested, so file an issue if something goes wrong! Be sure the modified lines go into the config file you're using to run elvers (see Understanding and Configuring Workflows ). References \u00b6 Documentation for DESeq2 with example analysis Love et al. 2014 Love et al. 2016 Additional links: DE lecture by Jane Khudyakov, July 2017 Example DE analysis from two populations of killifish! (Fundulus heteroclitus MDPL vs. MDPL) A Review of Differential Gene Expression Software for mRNA sequencing Snakemake Rules \u00b6 For snakemake afficionados, see the deseq2 rules on github .","title":"deseq2"},{"location":"deseq2/#differential-expression-analysis-with-deseq2","text":"We can use DESeq2 to compare gene expression differences in samples between experimental conditions.","title":"Differential expression analysis with DESeq2"},{"location":"deseq2/#quickstart-running-deseq2-via-elvers","text":"We recommend you run deseq2 via the diffexp subworkflow. If you want to run it as a standalone program instead, you need to have generated read quantification data via salmon . 1) If you have salmon results, run: elvers examples/nema.yaml deseq2 2) If not, you need to run salmon and any other missing steps. It's probably best to run the diffexp subworkflow, but you can also try: elvers examples/nema.yaml salmon deseq2","title":"Quickstart: Running DESeq2 via elvers"},{"location":"deseq2/#deseq2-commands","text":"This pipeline uses snakemake to run a few R scripts to conduct basic differential expression analysis. We read in transcript abundance information (generated with salmon ) via tximport . Note that in the salmon step, we combine files of all \"units\" within a sample in order to then conduct differential expression at the sample level. We assume the assembly has a gene-to-transcript map, such as the one produced via trinity. This is a tab separated file ( transcript \\t gene ) that enables count data to be aggregated at the gene level prior to differntial expression analysis. This is recommended, see Soneson et al, 2016 . However, if you do not have this mapping, we provide an option to conduct differential expression at the transcript level via the config (see \"Customizing DESeq2 Parameters\" section, below). After reading in count data, we take in two additional pieces of information: first, the sample names in the samples.tsv document, and second the desired contrast , provided as part of the DESeq2 parameters, below. We store all data in an .rds r data format to support easy reloading of this data for additional user analyses. In addition, we plot a PCA of the normalized counts and perform a standard DESeq2 analysis and print a tsv of results for each contrast specified in the deseq2 params. You can find these R scripts in the elvers github repo . The snakemake rules and scripts were modified from rna-seq-star-deseq2 workflow and our own data analysis and workshops, e.g. DIBSI-RNAseq .","title":"DESeq2 Commands"},{"location":"deseq2/#modifying-params-for-deseq2","text":"Be sure to set up your sample info and build a configfile first (see Understanding and Configuring Workflows ). To see the available parameters for the deseq2 rule, run elvers config deseq2 --print_params This will print the following: #################### deseq2 #################### deseq2: contrasts: time0-vs-time6: - time0 - time6 gene_trans_map: true pca: labels: - condition ##################################################### The default contrasts reflect the condition information in the test data nema_samples.tsv . Please modify the contrasts to the reflect your data. Multiple contrasts should be supported: each contrast needs a name, and a list below it specifying the conditions to compare, e.g.: contrasts: my-contrast: - conditionA - conditionB The pca labels should not be changed unless you need to change the name of the condition column in the samples.tsv . This functionality hasn't been extensively tested, so file an issue if something goes wrong! Be sure the modified lines go into the config file you're using to run elvers (see Understanding and Configuring Workflows ).","title":"Modifying Params for DESeq2"},{"location":"deseq2/#references","text":"Documentation for DESeq2 with example analysis Love et al. 2014 Love et al. 2016 Additional links: DE lecture by Jane Khudyakov, July 2017 Example DE analysis from two populations of killifish! (Fundulus heteroclitus MDPL vs. MDPL) A Review of Differential Gene Expression Software for mRNA sequencing","title":"References"},{"location":"deseq2/#snakemake-rules","text":"For snakemake afficionados, see the deseq2 rules on github .","title":"Snakemake Rules"},{"location":"dev_tips/","text":"Notes for Developers \u00b6 Admin: updating mkdocs documentation update the docs and commit your changes mkdocs build to update the docs note: if you don't already have mkdocs, install with: conda install -c conda-forge mkdocs if you haven't already, install ghp-import: conda install -c conda-forge ghp-import use ghp-import to push the updated to docs to the gh-pages branch ghp-import site -p Some useful conda, snakemake, workflow hints: optional: to make conda installs simpler, set up conda configuration \u00b6 conda config --set always_yes yes --set changeps1 no conda config --add channels conda-forge conda config --add channels defaults conda config --add channels bioconda If you need to modify a conda package: \u00b6 you'll need to work with a local install of that package. Here's how to use conda to install the dependenciesfrom the conda recipe. see also https://conda.io/docs/user-guide/tutorials/build-pkgs.html#building-and-installing install conda-build ``` conda install conda-build ``` clone the repo of interest and cd into it ``` git clone dammit-repo cd dammit-repo ``` There should be a folder called recipe. Use conda-build to build it. ``` conda build recipe ``` Install the local code ``` ## not working conda install dammit --use-local # or, you can use pip: # pip install -e . \u2014no-deps # NOW, use: conda develop . pip install -e . ```","title":"Notes for Developers"},{"location":"dev_tips/#notes-for-developers","text":"Admin: updating mkdocs documentation update the docs and commit your changes mkdocs build to update the docs note: if you don't already have mkdocs, install with: conda install -c conda-forge mkdocs if you haven't already, install ghp-import: conda install -c conda-forge ghp-import use ghp-import to push the updated to docs to the gh-pages branch ghp-import site -p Some useful conda, snakemake, workflow hints:","title":"Notes for Developers"},{"location":"dev_tips/#optional-to-make-conda-installs-simpler-set-up-conda-configuration","text":"conda config --set always_yes yes --set changeps1 no conda config --add channels conda-forge conda config --add channels defaults conda config --add channels bioconda","title":"optional: to make conda installs simpler, set up conda configuration"},{"location":"dev_tips/#if-you-need-to-modify-a-conda-package","text":"you'll need to work with a local install of that package. Here's how to use conda to install the dependenciesfrom the conda recipe. see also https://conda.io/docs/user-guide/tutorials/build-pkgs.html#building-and-installing install conda-build ``` conda install conda-build ``` clone the repo of interest and cd into it ``` git clone dammit-repo cd dammit-repo ``` There should be a folder called recipe. Use conda-build to build it. ``` conda build recipe ``` Install the local code ``` ## not working conda install dammit --use-local # or, you can use pip: # pip install -e . \u2014no-deps # NOW, use: conda develop . pip install -e . ```","title":"If you need to modify a conda package:"},{"location":"diffexp/","text":"Diffexp Subworkflow \u00b6 Subworkflows combine tools in the right order to facilitate file targeting withing elvers . The \"diffexp\" subworkflow conducts read quality trimming, salmon quantification and differential expression analysis. It requires an assembly to be provided, either by running an assembly or providing one in your configfile. If you have a gene-to-transcript map, this will also need to be specified via the get_reference parameter. If not, you'll need to modify the deseq2 parameters so that the workflow will not expect this file. At the moment, this workflow consists of: get_data - an elvers utility trimmomatic salmon deseq2 Quickstart \u00b6 If you've generated an assembly, even if you've already run elvers examples/nema.yaml assemble : 1) \"Run\" trinity assembly at the same time. If you've already run the assembly, elvers will just locate your assembly file for diffexp . elvers examples/nema.yaml assemble diffexp 2) OR, Pass an assembly in via get_reference with an assembly in your yaml configfile, e.g.: elvers get_reference diffexp In the configfile: ``` get_reference: reference: examples/nema.assemblyfasta gene_trans_map: examples/nema.assembly.fasta.gene_trans_map #optional ``` This is commented out in the test data yaml, but go ahead and uncomment (remove leading `#`) in order to use this option. If you have a gene to transcript map, please specify it as well. If not, delete this line from your `config`, and be sure to tell `deseq2` not to use this file by setting `gene_trans_map: False` (see config below). The `assembly_extension` parameter is important: this is what allows us to build assemblies from several different assemblers on the same dataset. Feel free to use `_input`, as specified above, or pick something equally simple yet more informative. **Note: Please don't use additional underscores (`_`) in this extension!**. For more details, see the [get_reference documentation](get_reference.md). Configuring the diffexp subworkflow \u00b6 To set up your sample info and build a configfile, see Understanding and Configuring Workflows . If you want to add the diffexp program parameters to a previously built configfile, run: elvers config.yaml diffexp --print_params A small set of parameters should print to your console: #################### diffexp #################### get_data: download_data: false use_ftp: false trimmomatic: adapter_file: pe_path: ep_utils/TruSeq3-PE-2.fa se_path: ep_utils/TruSeq3-SE.fa extra: '' trim_cmd: ILLUMINACLIP:{}:2:40:15 LEADING:2 TRAILING:2 SLIDINGWINDOW:4:15 MINLEN:25 salmon: index_params: extra: '' quant_params: libtype: A extra: '' deseq2: gene_trans_map: True # contrasts for the deseq2 results method contrasts: time0-vs-time6: - time0 - time6 pca: labels: # columns of sample sheet to use for PCA - condition ####################################################### Override default params for any program by placing these lines in your yaml config file, and modifying values as desired. For more details, see Understanding and Configuring Workflows .For more on what parameters are available, see the docs for each specific program or utility rule: get_data trimmomatic salmon deseq2","title":"diffexp"},{"location":"diffexp/#diffexp-subworkflow","text":"Subworkflows combine tools in the right order to facilitate file targeting withing elvers . The \"diffexp\" subworkflow conducts read quality trimming, salmon quantification and differential expression analysis. It requires an assembly to be provided, either by running an assembly or providing one in your configfile. If you have a gene-to-transcript map, this will also need to be specified via the get_reference parameter. If not, you'll need to modify the deseq2 parameters so that the workflow will not expect this file. At the moment, this workflow consists of: get_data - an elvers utility trimmomatic salmon deseq2","title":"Diffexp Subworkflow"},{"location":"diffexp/#quickstart","text":"If you've generated an assembly, even if you've already run elvers examples/nema.yaml assemble : 1) \"Run\" trinity assembly at the same time. If you've already run the assembly, elvers will just locate your assembly file for diffexp . elvers examples/nema.yaml assemble diffexp 2) OR, Pass an assembly in via get_reference with an assembly in your yaml configfile, e.g.: elvers get_reference diffexp In the configfile: ``` get_reference: reference: examples/nema.assemblyfasta gene_trans_map: examples/nema.assembly.fasta.gene_trans_map #optional ``` This is commented out in the test data yaml, but go ahead and uncomment (remove leading `#`) in order to use this option. If you have a gene to transcript map, please specify it as well. If not, delete this line from your `config`, and be sure to tell `deseq2` not to use this file by setting `gene_trans_map: False` (see config below). The `assembly_extension` parameter is important: this is what allows us to build assemblies from several different assemblers on the same dataset. Feel free to use `_input`, as specified above, or pick something equally simple yet more informative. **Note: Please don't use additional underscores (`_`) in this extension!**. For more details, see the [get_reference documentation](get_reference.md).","title":"Quickstart"},{"location":"diffexp/#configuring-the-diffexp-subworkflow","text":"To set up your sample info and build a configfile, see Understanding and Configuring Workflows . If you want to add the diffexp program parameters to a previously built configfile, run: elvers config.yaml diffexp --print_params A small set of parameters should print to your console: #################### diffexp #################### get_data: download_data: false use_ftp: false trimmomatic: adapter_file: pe_path: ep_utils/TruSeq3-PE-2.fa se_path: ep_utils/TruSeq3-SE.fa extra: '' trim_cmd: ILLUMINACLIP:{}:2:40:15 LEADING:2 TRAILING:2 SLIDINGWINDOW:4:15 MINLEN:25 salmon: index_params: extra: '' quant_params: libtype: A extra: '' deseq2: gene_trans_map: True # contrasts for the deseq2 results method contrasts: time0-vs-time6: - time0 - time6 pca: labels: # columns of sample sheet to use for PCA - condition ####################################################### Override default params for any program by placing these lines in your yaml config file, and modifying values as desired. For more details, see Understanding and Configuring Workflows .For more on what parameters are available, see the docs for each specific program or utility rule: get_data trimmomatic salmon deseq2","title":"Configuring the diffexp subworkflow"},{"location":"eel_pond_workflow/","text":"Eel Pond Protocol Workflow \u00b6 The Eel Pond protocol (which inspired the elvers name) included line-by-line commands that the user could follow along with using a test dataset provided in the instructions. We have re-implemented the protocol here to enable automated de novo transcriptome assembly, annotation, and quick differential expression analysis on a set of short-read Illumina data using a single command. See more about this protocol here . The \"Eel Pond\" Protocol for RNAseq consists of: trimmomatic adapter and read quality trimming fastqc read qc evaluation khmer k-mer trimming and (optional) digital normalization trinity de novo assembly dammit annotation salmon read quantification to the trinity assembly deseq2 differential expression analysis Running Test Data \u00b6 This is the default workflow. To run: elvers examples/nema.yaml (You can be explicit and run the full default workflow with elvers examples/nema.yaml default ) This will run a small set of Nematostella vectensis test data (from Tulin et al., 2013 ). Running Your Own Data \u00b6 Set sample info and build a configfile first (see Understanding and Configuring Workflows ). To build a config, run: elvers ep.yaml --build_config The resulting ep.yaml configfile for this workflow will look something like this. The order of the parameters may be different and does not affect the order in which steps are run. Please see the documentation file for each individual program (linked above) for what parameters to modify. #################### Eelpond Pipeline Configfile #################### basename: elvers experiment: _experiment1 samples: samples.tsv ### PATH TO YOUR SAMPLE FILE GOES HERE #################### assemble #################### get_data: download_data: false khmer: C: 3 Z: 18 coverage: 20 diginorm: true extra: '' ksize: 20 memory: 4e9 trimmomatic: adapter_file: pe_name: TruSeq3-PE.fa pe_url: https://raw.githubusercontent.com/timflutre/trimmomatic/master/adapters/TruSeq3-PE-2.fa se_name: TruSeq3-SE.fa se_url: https://raw.githubusercontent.com/timflutre/trimmomatic/master/adapters/TruSeq3-SE.fa extra: '' trim_cmd: ILLUMINACLIP:{}:2:40:15 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35 trinity: add_single_to_paired: false extra: '' input_kmer_trimmed: true input_trimmomatic_trimmed: false max_memory: 30G seqtype: fq #################### annotate #################### dammit: busco_group: - metazoa - eukaryota db_dir: databases db_extra: '' sourmash: extra: '' #################### quantify #################### salmon: input_trimmomatic_trimmed: True index_params: extra: '' quant_params: extra: '' libtype: A #################### diffexp #################### deseq2: contrasts: time0-vs-time6: - time0 - time6 gene_trans_map: true pca: labels: - condition","title":"Eel Pond Protocol"},{"location":"eel_pond_workflow/#eel-pond-protocol-workflow","text":"The Eel Pond protocol (which inspired the elvers name) included line-by-line commands that the user could follow along with using a test dataset provided in the instructions. We have re-implemented the protocol here to enable automated de novo transcriptome assembly, annotation, and quick differential expression analysis on a set of short-read Illumina data using a single command. See more about this protocol here . The \"Eel Pond\" Protocol for RNAseq consists of: trimmomatic adapter and read quality trimming fastqc read qc evaluation khmer k-mer trimming and (optional) digital normalization trinity de novo assembly dammit annotation salmon read quantification to the trinity assembly deseq2 differential expression analysis","title":"Eel Pond Protocol Workflow"},{"location":"eel_pond_workflow/#running-test-data","text":"This is the default workflow. To run: elvers examples/nema.yaml (You can be explicit and run the full default workflow with elvers examples/nema.yaml default ) This will run a small set of Nematostella vectensis test data (from Tulin et al., 2013 ).","title":"Running Test Data"},{"location":"eel_pond_workflow/#running-your-own-data","text":"Set sample info and build a configfile first (see Understanding and Configuring Workflows ). To build a config, run: elvers ep.yaml --build_config The resulting ep.yaml configfile for this workflow will look something like this. The order of the parameters may be different and does not affect the order in which steps are run. Please see the documentation file for each individual program (linked above) for what parameters to modify. #################### Eelpond Pipeline Configfile #################### basename: elvers experiment: _experiment1 samples: samples.tsv ### PATH TO YOUR SAMPLE FILE GOES HERE #################### assemble #################### get_data: download_data: false khmer: C: 3 Z: 18 coverage: 20 diginorm: true extra: '' ksize: 20 memory: 4e9 trimmomatic: adapter_file: pe_name: TruSeq3-PE.fa pe_url: https://raw.githubusercontent.com/timflutre/trimmomatic/master/adapters/TruSeq3-PE-2.fa se_name: TruSeq3-SE.fa se_url: https://raw.githubusercontent.com/timflutre/trimmomatic/master/adapters/TruSeq3-SE.fa extra: '' trim_cmd: ILLUMINACLIP:{}:2:40:15 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35 trinity: add_single_to_paired: false extra: '' input_kmer_trimmed: true input_trimmomatic_trimmed: false max_memory: 30G seqtype: fq #################### annotate #################### dammit: busco_group: - metazoa - eukaryota db_dir: databases db_extra: '' sourmash: extra: '' #################### quantify #################### salmon: input_trimmomatic_trimmed: True index_params: extra: '' quant_params: extra: '' libtype: A #################### diffexp #################### deseq2: contrasts: time0-vs-time6: - time0 - time6 gene_trans_map: true pca: labels: - condition","title":"Running Your Own Data"},{"location":"fastqc/","text":"FastQC \u00b6 We use FastQC to assess quality of sequencing data before and after adapter trimming. From the FastQC documentation : Modern high throughput sequencers can generate hundreds of millions of sequences in a single run. Before analysing this sequence to draw biological conclusions you should always perform some simple quality control checks to ensure that the raw data looks good and there are no problems or biases in your data which may affect how you can usefully use it. Most sequencers will generate a QC report as part of their analysis pipeline, but this is usually only focused on identifying problems which were generated by the sequencer itself. FastQC aims to provide a QC report which can spot problems which originate either in the sequencer or in the starting library material. FastQC can be run in one of two modes. It can either run as a stand alone interactive application for the immediate analysis of small numbers of FastQ files, or it can be run in a non-interactive mode where it would be suitable for integrating into a larger analysis pipeline for the systematic processing of large numbers of files. Check out these examples of good and bad Illumina data. Note that FastQC only calculates certain statistics (like duplicated sequences) for subsets of the data (e.g. duplicate sequences are only analyzed for the first 100,000 sequences in each file). Quickstart \u00b6 Run FastQC via the \"default\" Eel Pond workflow or via the preprocess subworkflow . To run FastQC as a standalone program, see \"Advanced Usage\" section below. Output files: \u00b6 Your main output directory will be determined by your config file: by default it is BASENAME_out (you specify BASENAME). FastQC will output quality control files in the preprocess/fastqc subdirectory of this output directory. All outputs will contain *.fastqc.html or *.fastqc.zip . Modifying Params for FastQC: \u00b6 Be sure to set up your sample info and build a configfile first (see Understanding and Configuring Workflows ). To see the available parameters for the fastqc rule, run elvers config fastqc --print_params In here, you'll see a section for \"fastqc\" parameters that looks like this: #################### fastqc #################### fastc: extra: ##################################################### There's almost nothing in here because we use default params. However, you can modify the extra param to pass any extra fastqc parameters, e.g.: extra: '--someflag someparam --someotherflag thatotherparam' See the FastQC documentation for any options you could add. Be sure the modified lines go into the config file you're using to run elvers (see Understanding and Configuring Workflows ). Advanced Usage: Running FastQC as a standalone rule \u00b6 You can run fastqc as a standalone rule, instead of withing a larger elvers workflow. However, to do this, you need to make sure the input files are available. For FastQC, the input files are your input data - either downloaded or linked into the input_data directory via get_data , and the files that have been quality trimmed. If you've already done this, you can run: elvers my_config fastqc If not, you can run both at once to make sure fastqc can run properly. elvers my_config get_data trimmomatic fastqc Snakemake rule \u00b6 We use a local copy of the fastqc snakemake wrapper to run FastQC. For snakemake afficionados, see the rule on github .","title":"fastqc"},{"location":"fastqc/#fastqc","text":"We use FastQC to assess quality of sequencing data before and after adapter trimming. From the FastQC documentation : Modern high throughput sequencers can generate hundreds of millions of sequences in a single run. Before analysing this sequence to draw biological conclusions you should always perform some simple quality control checks to ensure that the raw data looks good and there are no problems or biases in your data which may affect how you can usefully use it. Most sequencers will generate a QC report as part of their analysis pipeline, but this is usually only focused on identifying problems which were generated by the sequencer itself. FastQC aims to provide a QC report which can spot problems which originate either in the sequencer or in the starting library material. FastQC can be run in one of two modes. It can either run as a stand alone interactive application for the immediate analysis of small numbers of FastQ files, or it can be run in a non-interactive mode where it would be suitable for integrating into a larger analysis pipeline for the systematic processing of large numbers of files. Check out these examples of good and bad Illumina data. Note that FastQC only calculates certain statistics (like duplicated sequences) for subsets of the data (e.g. duplicate sequences are only analyzed for the first 100,000 sequences in each file).","title":"FastQC"},{"location":"fastqc/#quickstart","text":"Run FastQC via the \"default\" Eel Pond workflow or via the preprocess subworkflow . To run FastQC as a standalone program, see \"Advanced Usage\" section below.","title":"Quickstart"},{"location":"fastqc/#output-files","text":"Your main output directory will be determined by your config file: by default it is BASENAME_out (you specify BASENAME). FastQC will output quality control files in the preprocess/fastqc subdirectory of this output directory. All outputs will contain *.fastqc.html or *.fastqc.zip .","title":"Output files:"},{"location":"fastqc/#modifying-params-for-fastqc","text":"Be sure to set up your sample info and build a configfile first (see Understanding and Configuring Workflows ). To see the available parameters for the fastqc rule, run elvers config fastqc --print_params In here, you'll see a section for \"fastqc\" parameters that looks like this: #################### fastqc #################### fastc: extra: ##################################################### There's almost nothing in here because we use default params. However, you can modify the extra param to pass any extra fastqc parameters, e.g.: extra: '--someflag someparam --someotherflag thatotherparam' See the FastQC documentation for any options you could add. Be sure the modified lines go into the config file you're using to run elvers (see Understanding and Configuring Workflows ).","title":"Modifying Params for FastQC:"},{"location":"fastqc/#advanced-usage-running-fastqc-as-a-standalone-rule","text":"You can run fastqc as a standalone rule, instead of withing a larger elvers workflow. However, to do this, you need to make sure the input files are available. For FastQC, the input files are your input data - either downloaded or linked into the input_data directory via get_data , and the files that have been quality trimmed. If you've already done this, you can run: elvers my_config fastqc If not, you can run both at once to make sure fastqc can run properly. elvers my_config get_data trimmomatic fastqc","title":"Advanced Usage: Running FastQC as a standalone rule"},{"location":"fastqc/#snakemake-rule","text":"We use a local copy of the fastqc snakemake wrapper to run FastQC. For snakemake afficionados, see the rule on github .","title":"Snakemake rule"},{"location":"get_data/","text":"Get Data Utility Rule \u00b6 To run any workflow, we need to get reads (and any assemblies already generated) into the right format for elvers . We can either (1) link the data from another location on your machine, or (2) download the data via http or ftp. This is done through a utility rule called get_data . At the moment, get_data only works with gzipped files. Checks and improved functionality coming soon Modifying Params for get_data: \u00b6 Be sure to set up your sample info and build a configfile first (see Understanding and Configuring Workflows ). To see the available parameters for the get_data utility rule, run elvers config get_data --print_params In here, you'll see a section for \"get_data\" parameters that looks like this: #################### get_data #################### get_data: download_data: false use_ftp: false ##################################################### If you want to download data, you'll want to change the download_data and use_ftp parameters appropriately. To download via http, set download_data: True . To download via ftp, set download_data: True and use_ftp: True . Output Files \u00b6 The output of the get_data step is all your input data files in a subdirectory ( input_data ) within your output directory (basename_out). These will either be links or downloaded files, depending on the options you specified in the config file.","title":"get_data"},{"location":"get_data/#get-data-utility-rule","text":"To run any workflow, we need to get reads (and any assemblies already generated) into the right format for elvers . We can either (1) link the data from another location on your machine, or (2) download the data via http or ftp. This is done through a utility rule called get_data . At the moment, get_data only works with gzipped files. Checks and improved functionality coming soon","title":"Get Data Utility Rule"},{"location":"get_data/#modifying-params-for-get_data","text":"Be sure to set up your sample info and build a configfile first (see Understanding and Configuring Workflows ). To see the available parameters for the get_data utility rule, run elvers config get_data --print_params In here, you'll see a section for \"get_data\" parameters that looks like this: #################### get_data #################### get_data: download_data: false use_ftp: false ##################################################### If you want to download data, you'll want to change the download_data and use_ftp parameters appropriately. To download via http, set download_data: True . To download via ftp, set download_data: True and use_ftp: True .","title":"Modifying Params for get_data:"},{"location":"get_data/#output-files","text":"The output of the get_data step is all your input data files in a subdirectory ( input_data ) within your output directory (basename_out). These will either be links or downloaded files, depending on the options you specified in the config file.","title":"Output Files"},{"location":"get_reference/","text":"Get Reference Utility Rule \u00b6 For elvers workflows that start with or otherwise utilize a reference, the keyword get_reference can be used to provide that reference (and for transcriptomes, an optional tab-separated gene-to-transcript map). If you provide the get_reference keyword and information in your configuration file, the get_reference utility rule with either download or softlink your reference into your elvers directory so it can be utilized for any specified workflows. Specify Input reference \u00b6 The default get_reference parameters are as follows: get_reference: reference: examples/nema.assembly.fasta gene_trans_map: examples/nema.assembly.fasta.gene_trans_map reference_extension: _input download_ref: false # download the reference using http (or ftp) use_ftp: false # switch download method from http to ftp ( to see these, you can run elvers config get_reference --print_params ) The default filenames correspond to a test Trinity transcriptome we provide for the nema test data. To use your own files, specify the file path to your fasta file via the reference parameter. If you have a gene to transcript mapi (transcriptomes only), please specify it as well. If not, do not include the gene_trans_map line in your config . The reference_extension parameter is an optional parameter that allows us to use multiple references or assemblies. For example, you can build a de novo transcriptome with Trinity and input a reference transcriptome and do all downstream steps (e.g. quantification and differential expression) on both assemblies. All assemblies generated via elvers rules will have an extension corresponding to the program they were generated with. If you choose to add a reference_extension , feel free to use _input , as specified above, or pick something equally simple yet more informative, but please don't use additional underscores ( _ ) in this extension! Output Files \u00b6 The output of the get_reference step is your reference (and optional gene-transcript map) copied into a subdirectory ( reference ) within your output directory (BASENAME_out).","title":"get_reference"},{"location":"get_reference/#get-reference-utility-rule","text":"For elvers workflows that start with or otherwise utilize a reference, the keyword get_reference can be used to provide that reference (and for transcriptomes, an optional tab-separated gene-to-transcript map). If you provide the get_reference keyword and information in your configuration file, the get_reference utility rule with either download or softlink your reference into your elvers directory so it can be utilized for any specified workflows.","title":"Get Reference Utility Rule"},{"location":"get_reference/#specify-input-reference","text":"The default get_reference parameters are as follows: get_reference: reference: examples/nema.assembly.fasta gene_trans_map: examples/nema.assembly.fasta.gene_trans_map reference_extension: _input download_ref: false # download the reference using http (or ftp) use_ftp: false # switch download method from http to ftp ( to see these, you can run elvers config get_reference --print_params ) The default filenames correspond to a test Trinity transcriptome we provide for the nema test data. To use your own files, specify the file path to your fasta file via the reference parameter. If you have a gene to transcript mapi (transcriptomes only), please specify it as well. If not, do not include the gene_trans_map line in your config . The reference_extension parameter is an optional parameter that allows us to use multiple references or assemblies. For example, you can build a de novo transcriptome with Trinity and input a reference transcriptome and do all downstream steps (e.g. quantification and differential expression) on both assemblies. All assemblies generated via elvers rules will have an extension corresponding to the program they were generated with. If you choose to add a reference_extension , feel free to use _input , as specified above, or pick something equally simple yet more informative, but please don't use additional underscores ( _ ) in this extension!","title":"Specify Input reference"},{"location":"get_reference/#output-files","text":"The output of the get_reference step is your reference (and optional gene-transcript map) copied into a subdirectory ( reference ) within your output directory (BASENAME_out).","title":"Output Files"},{"location":"khmer/","text":"Khmer k-mer trimming and (optional) diginorm \u00b6 Before running transcriptome assembly, we recommend doing some kmer spectral error trimming on your dataset, and if you have lots of reads, also performing digital normalization. This has the effect of reducing the computational cost of assembly without negatively affecting the quality of the assembly. We use khmer for both of these tasks. Khmer Command \u00b6 Here's the command as it would look on the command line: (interleave-reads.py sample_1.trim.fq sample_2.trim.fq )| \\\\ (trim-low-abund.py -V -k 20 -Z 18 -C 2 - -o - -M 4e9 --diginorm --diginorm-coverage=20) | \\\\ (extract-paired-reads.py --gzip -p sample.paired.gz -s sample.single.gz) > /dev/null Trimmed reads are used as input. trim-low-abund.py trims low-abundance reads to a coverage of 18. Here, we also perform digital normalization to a k -mer ( k = 20) coverage of 20.The output files are the remaining reads, grouped as pairs and singles (orphans). For more on trim-low-abund , see this recipe , Finally, since Trinity expects separate left and right files, we use split-paired-reads.py to split the interleaved pairs into two files. split-paired-reads.py sample.paired.gz Quickstart \u00b6 Run Khmer via the \"default\" Eel Pond workflow or via the kmer_trim subworkflow . To run Khmer as a standalone program, see \"Advanced Usage\" section below. Output files: \u00b6 Your main output directory will be determined by your config file: by default it is BASENAME_out (you specify BASENAME). Khmer will output files in the preprocess subdirectory of this output directory. All outputs will contain *.khmer.fq.gz . Modifying Params for Khmer: \u00b6 Be sure to set up your sample info and build a configfile first (see Understanding and Configuring Workflows ). To see the available parameters for the khmer rule, run elvers config khmer --print_params In here, you'll see a section for \"khmer\" parameters that looks like this: #################### khmer #################### khmer: C: 3 Z: 18 coverage: 20 diginorm: true ksize: 20 memory: 4e9 ##################################################### See the Khmer documentation to learn more about these parameters. Be sure the modified lines go into the config file you're using to run elvers (see Understanding and Configuring Workflows ). Advanced Usage: Running Khmer as a standalone rule \u00b6 You can run khmer as a standalone rule, instead of withing a larger elvers workflow. However, to do this, you need to make sure the input files are available. For khmer, the input files are trimmed input data (e.g. output of trimmomatic). If you've already done this, you can run: elvers my_config khmer If not, you can run the prior steps at the same time to make sure khmer can find these input files: elvers my_config get_data trimmomatic khmer Snakemake Rule \u00b6 For snakemake afficionados, see the khmer rule on github .","title":"khmer"},{"location":"khmer/#khmer-k-mer-trimming-and-optional-diginorm","text":"Before running transcriptome assembly, we recommend doing some kmer spectral error trimming on your dataset, and if you have lots of reads, also performing digital normalization. This has the effect of reducing the computational cost of assembly without negatively affecting the quality of the assembly. We use khmer for both of these tasks.","title":"Khmer k-mer trimming and (optional) diginorm"},{"location":"khmer/#khmer-command","text":"Here's the command as it would look on the command line: (interleave-reads.py sample_1.trim.fq sample_2.trim.fq )| \\\\ (trim-low-abund.py -V -k 20 -Z 18 -C 2 - -o - -M 4e9 --diginorm --diginorm-coverage=20) | \\\\ (extract-paired-reads.py --gzip -p sample.paired.gz -s sample.single.gz) > /dev/null Trimmed reads are used as input. trim-low-abund.py trims low-abundance reads to a coverage of 18. Here, we also perform digital normalization to a k -mer ( k = 20) coverage of 20.The output files are the remaining reads, grouped as pairs and singles (orphans). For more on trim-low-abund , see this recipe , Finally, since Trinity expects separate left and right files, we use split-paired-reads.py to split the interleaved pairs into two files. split-paired-reads.py sample.paired.gz","title":"Khmer Command"},{"location":"khmer/#quickstart","text":"Run Khmer via the \"default\" Eel Pond workflow or via the kmer_trim subworkflow . To run Khmer as a standalone program, see \"Advanced Usage\" section below.","title":"Quickstart"},{"location":"khmer/#output-files","text":"Your main output directory will be determined by your config file: by default it is BASENAME_out (you specify BASENAME). Khmer will output files in the preprocess subdirectory of this output directory. All outputs will contain *.khmer.fq.gz .","title":"Output files:"},{"location":"khmer/#modifying-params-for-khmer","text":"Be sure to set up your sample info and build a configfile first (see Understanding and Configuring Workflows ). To see the available parameters for the khmer rule, run elvers config khmer --print_params In here, you'll see a section for \"khmer\" parameters that looks like this: #################### khmer #################### khmer: C: 3 Z: 18 coverage: 20 diginorm: true ksize: 20 memory: 4e9 ##################################################### See the Khmer documentation to learn more about these parameters. Be sure the modified lines go into the config file you're using to run elvers (see Understanding and Configuring Workflows ).","title":"Modifying Params for Khmer:"},{"location":"khmer/#advanced-usage-running-khmer-as-a-standalone-rule","text":"You can run khmer as a standalone rule, instead of withing a larger elvers workflow. However, to do this, you need to make sure the input files are available. For khmer, the input files are trimmed input data (e.g. output of trimmomatic). If you've already done this, you can run: elvers my_config khmer If not, you can run the prior steps at the same time to make sure khmer can find these input files: elvers my_config get_data trimmomatic khmer","title":"Advanced Usage: Running Khmer as a standalone rule"},{"location":"khmer/#snakemake-rule","text":"For snakemake afficionados, see the khmer rule on github .","title":"Snakemake Rule"},{"location":"kmer_trim/","text":"Kmer_trim Subworkflow \u00b6 Subworkflows combine tools in the right order to facilitate file targeting withing elvers . The \"kmer_trim\" subworkflow conducts read quality trimming and kmer trimming. At the moment, this workflow consists of: get_data - an elvers utility trimmomatic fastqc , run on both pre-trim and post-trim data khmer Quickstart \u00b6 To run the kmer_trim subworkflow, run: elvers examples/nema.yaml kmer_trim Configuring the kmer_trim subworkflow \u00b6 To set up your sample info and build a configfile, see Understanding and Configuring Workflows . If you want to add the kmer_trim program parameters to a previously built configfile, run: elvers config.yaml kmer_trim --print_params A small set of parameters should print to your console: #################### kmer_trim #################### get_data: download_data: false use_ftp: false trimmomatic: adapter_file: pe_path: ep_utils/TruSeq3-PE-2.fa se_path: ep_utils/TruSeq3-SE.fa extra: '' trim_cmd: ILLUMINACLIP:{}:2:40:15 LEADING:2 TRAILING:2 SLIDINGWINDOW:4:15 MINLEN:25 fastqc: extra: '' khmer: C: 3 Z: 18 coverage: 20 diginorm: true extra: '' ksize: 20 memory: 4e9 ####################################################### Override default params for any program by placing these lines in your yaml config file, and modifying values as desired. For more details, see Understanding and Configuring Workflows .For more on what parameters are available, see the docs for each specific program or utility rule: get_data trimmomatic fastqc khmer","title":"kmer_trim"},{"location":"kmer_trim/#kmer_trim-subworkflow","text":"Subworkflows combine tools in the right order to facilitate file targeting withing elvers . The \"kmer_trim\" subworkflow conducts read quality trimming and kmer trimming. At the moment, this workflow consists of: get_data - an elvers utility trimmomatic fastqc , run on both pre-trim and post-trim data khmer","title":"Kmer_trim Subworkflow"},{"location":"kmer_trim/#quickstart","text":"To run the kmer_trim subworkflow, run: elvers examples/nema.yaml kmer_trim","title":"Quickstart"},{"location":"kmer_trim/#configuring-the-kmer_trim-subworkflow","text":"To set up your sample info and build a configfile, see Understanding and Configuring Workflows . If you want to add the kmer_trim program parameters to a previously built configfile, run: elvers config.yaml kmer_trim --print_params A small set of parameters should print to your console: #################### kmer_trim #################### get_data: download_data: false use_ftp: false trimmomatic: adapter_file: pe_path: ep_utils/TruSeq3-PE-2.fa se_path: ep_utils/TruSeq3-SE.fa extra: '' trim_cmd: ILLUMINACLIP:{}:2:40:15 LEADING:2 TRAILING:2 SLIDINGWINDOW:4:15 MINLEN:25 fastqc: extra: '' khmer: C: 3 Z: 18 coverage: 20 diginorm: true extra: '' ksize: 20 memory: 4e9 ####################################################### Override default params for any program by placing these lines in your yaml config file, and modifying values as desired. For more details, see Understanding and Configuring Workflows .For more on what parameters are available, see the docs for each specific program or utility rule: get_data trimmomatic fastqc khmer","title":"Configuring the kmer_trim subworkflow"},{"location":"paladin/","text":"Mapping to Protein Assemblies using Paladin \u00b6 We use Paladin to map back to protein assemblies ( plass ). Paladin solves the problem of mapping nucleotide sequences back to amino acid sequences. From the documentation : Protein ALignment And Detection INterface PALADIN is a protein sequence alignment tool designed for the accurate functional characterization of metagenomes. PALADIN is based on BWA, and aligns sequences via read-mapping using BWT. PALADIN, however, offers the novel approach of aligning in the protein space. During the index phase, it processes the reference genome's nucleotide sequences and GTF/GFF annotation containing CDS entries, first converting these transcripts into the corresponding protein sequences, then creating the BWT and suffix array from these proteins. The process of translatation is skiped when providing a protein reference file (e.g., UniProt) for mapping. During the alignment phase, it attempts to find ORFs in the read sequences, then converts these to protein sequences, and aligns to the reference protein sequences. PALADIN currently only supports single-end reads (or reads merged with FLASH, PEAR, abyss-mergepairs), and BWA-MEM based alignment. It makes use of many BWA parameters and is therefore compatible with many of its command line arguments. PALADIN may output a standard SAM file, or a text file containing a UniProt-generated functional profile. This text file may be used for all downstream characterizations. Quickstart: Running Paladin with elvers \u00b6 We recommend you run paladin as part of the Protein Assembly or paladin_map workflows. elvers examples/nema.yaml protein_assembly This will run trimmomatic trimming prior to PEAR merging (for paired end reads) and paladin mapping. Note, PEAR only works on paired end reads, as there's nothing to do for single end reads. By default, SE reads are taken directly from trimmomatic output. Paladin Commands \u00b6 Eelpond first indexes the protein assembly, and then maps reads back. On the command line, these commands would look like this: paladin index 3 index_basename \\ gff_annotation extra We have not yet added dammit annotation gff at this step, but this could be done :). You can also add any gff3 annotation via the params. paladin align -f 250 -t snakemake.threads \\ index_basename reads_file | samtools view -Sb - > output.bam Modifying Params for Paladin: \u00b6 Be sure to set up your sample info and build a configfile first (see Understanding and Configuring Workflows ). To see the available parameters for the Paladin rule, run elvers config paladin --print_params This will print the following: #################### paladin #################### paladin: alignment_params: extra: '' f: 125 index_params: reference_type: '3' gff_file: '' ##################################################### In addition to changing parameters we've specifically enabled, you can modify the extra param to pass in additional parameters to paladin align , e.g.: extra: ' --some_param that_param ' Please see the Paladin documentation for info on the params you can pass into paladin . Be sure the modified lines go into the config file you're using to run elvers (see Understanding and Configuring Workflows ). Advanced Usage: Running PALADIN as a standalone rule \u00b6 You can run paladin as a standalone rule, instead of withing a larger elvers workflow. However, to do this, you need to make sure the input files are available. For paladin, you need both 1) an assembly, and 2) trimmed (and merged) input files. The assembly can be generated via another workflow, or passed to elvers via the configfile. Specifying an assembly: 1) If you've already run read trimming and want to use a Trinity assembly generated via elvers , run the following: elvers my_config plass_assemble paladin # elvers will run or locate the plass assembly 2) Alternatively, you can input an assembly via the get_reference utility rule, with an assembly in your yaml configfile. elvers get_reference paladin In config file: get_reference: referemce: examples/nema.assembly.fasta gene_trans_map: examples/nema.assembly.fasta.gene_trans_map #optional reference_extension: '_plass' This is commented out in the test data yaml, but go ahead and uncomment (remove leading # ) in order to use this option. If you have a gene to transcript map, please specify it as well. If not, delete this line from your config . The assembly_extension parameter is important: this is what allows us to build assemblies from several different assemblers on the same dataset. Make sure you set the assembly_extension parameter to plass, as paladin only works on plassassemblies (for the moment). Note: Please don't use additional underscores ( _ ) in this extension! . For more details, see the get_reference documentation . Specifying input reads: If you haven't yet run read trimming and merging, you'll also need to run those steps: elvers my_config get_data trimmomatic pear paladin with one of the options to specify an assembly (above). PALADIN elvers rule \u00b6 We wrote new snakemake wrappers for paladin index and paladin align to run PALADIN via snakemake. These wrappers have not been added to the official repo yet, but feel free to use as needed. For snakemake afficionados, see our paladin rule on github . Citation \u00b6 If you use PALADIN, please cite Westbrook et al., 2017 .","title":"paladin"},{"location":"paladin/#mapping-to-protein-assemblies-using-paladin","text":"We use Paladin to map back to protein assemblies ( plass ). Paladin solves the problem of mapping nucleotide sequences back to amino acid sequences. From the documentation : Protein ALignment And Detection INterface PALADIN is a protein sequence alignment tool designed for the accurate functional characterization of metagenomes. PALADIN is based on BWA, and aligns sequences via read-mapping using BWT. PALADIN, however, offers the novel approach of aligning in the protein space. During the index phase, it processes the reference genome's nucleotide sequences and GTF/GFF annotation containing CDS entries, first converting these transcripts into the corresponding protein sequences, then creating the BWT and suffix array from these proteins. The process of translatation is skiped when providing a protein reference file (e.g., UniProt) for mapping. During the alignment phase, it attempts to find ORFs in the read sequences, then converts these to protein sequences, and aligns to the reference protein sequences. PALADIN currently only supports single-end reads (or reads merged with FLASH, PEAR, abyss-mergepairs), and BWA-MEM based alignment. It makes use of many BWA parameters and is therefore compatible with many of its command line arguments. PALADIN may output a standard SAM file, or a text file containing a UniProt-generated functional profile. This text file may be used for all downstream characterizations.","title":"Mapping to Protein Assemblies using Paladin"},{"location":"paladin/#quickstart-running-paladin-with-elvers","text":"We recommend you run paladin as part of the Protein Assembly or paladin_map workflows. elvers examples/nema.yaml protein_assembly This will run trimmomatic trimming prior to PEAR merging (for paired end reads) and paladin mapping. Note, PEAR only works on paired end reads, as there's nothing to do for single end reads. By default, SE reads are taken directly from trimmomatic output.","title":"Quickstart: Running Paladin with elvers"},{"location":"paladin/#paladin-commands","text":"Eelpond first indexes the protein assembly, and then maps reads back. On the command line, these commands would look like this: paladin index 3 index_basename \\ gff_annotation extra We have not yet added dammit annotation gff at this step, but this could be done :). You can also add any gff3 annotation via the params. paladin align -f 250 -t snakemake.threads \\ index_basename reads_file | samtools view -Sb - > output.bam","title":"Paladin Commands"},{"location":"paladin/#modifying-params-for-paladin","text":"Be sure to set up your sample info and build a configfile first (see Understanding and Configuring Workflows ). To see the available parameters for the Paladin rule, run elvers config paladin --print_params This will print the following: #################### paladin #################### paladin: alignment_params: extra: '' f: 125 index_params: reference_type: '3' gff_file: '' ##################################################### In addition to changing parameters we've specifically enabled, you can modify the extra param to pass in additional parameters to paladin align , e.g.: extra: ' --some_param that_param ' Please see the Paladin documentation for info on the params you can pass into paladin . Be sure the modified lines go into the config file you're using to run elvers (see Understanding and Configuring Workflows ).","title":"Modifying Params for Paladin:"},{"location":"paladin/#advanced-usage-running-paladin-as-a-standalone-rule","text":"You can run paladin as a standalone rule, instead of withing a larger elvers workflow. However, to do this, you need to make sure the input files are available. For paladin, you need both 1) an assembly, and 2) trimmed (and merged) input files. The assembly can be generated via another workflow, or passed to elvers via the configfile. Specifying an assembly: 1) If you've already run read trimming and want to use a Trinity assembly generated via elvers , run the following: elvers my_config plass_assemble paladin # elvers will run or locate the plass assembly 2) Alternatively, you can input an assembly via the get_reference utility rule, with an assembly in your yaml configfile. elvers get_reference paladin In config file: get_reference: referemce: examples/nema.assembly.fasta gene_trans_map: examples/nema.assembly.fasta.gene_trans_map #optional reference_extension: '_plass' This is commented out in the test data yaml, but go ahead and uncomment (remove leading # ) in order to use this option. If you have a gene to transcript map, please specify it as well. If not, delete this line from your config . The assembly_extension parameter is important: this is what allows us to build assemblies from several different assemblers on the same dataset. Make sure you set the assembly_extension parameter to plass, as paladin only works on plassassemblies (for the moment). Note: Please don't use additional underscores ( _ ) in this extension! . For more details, see the get_reference documentation . Specifying input reads: If you haven't yet run read trimming and merging, you'll also need to run those steps: elvers my_config get_data trimmomatic pear paladin with one of the options to specify an assembly (above).","title":"Advanced Usage: Running PALADIN as a standalone rule"},{"location":"paladin/#paladin-elvers-rule","text":"We wrote new snakemake wrappers for paladin index and paladin align to run PALADIN via snakemake. These wrappers have not been added to the official repo yet, but feel free to use as needed. For snakemake afficionados, see our paladin rule on github .","title":"PALADIN elvers rule"},{"location":"paladin/#citation","text":"If you use PALADIN, please cite Westbrook et al., 2017 .","title":"Citation"},{"location":"paladin_map/","text":"Paladin_Map Subworkflow \u00b6 Subworkflows combine tools in the right order to facilitate file targeting within elvers . The \"paladin_map\" subworkflow conducts read quality trimming and paladin mapping to a protein reference. It requires an assembly to be provided, either by running an assembly or providing one in your configfile. At the moment, this workflow consists of: get_data - an elvers utility trimmomatic pear paladin Quickstart \u00b6 If you've generated an assembly, even if you've already run elvers examples/nema.yaml assemble : 1) \"Run\" trinity assembly at the same time. If you've already run the assembly, elvers will just locateyour assembly file for paladin_map . elvers examples/nema.yaml assemble paladin_map 2) OR, Pass an assembly in via get_reference , with an assembly in your yaml configfile, e.g.: elvers get_reference paladin_map In the configfile: get_reference: reference: examples/nema.assembly.fasta gene_trans_map: examples/nema.assembly.fasta.gene_trans_map #optional reference_extension: '_plass' This is commented out in the test data yaml, but go ahead and uncomment (remove leading # ) in order to use this option. If you have a gene to transcript map, please specify it as well. If not, delete this line from your config . The assembly_extension parameter is important: this is what allows us to build assemblies from several different assemblers on the same dataset. Feel free to use _input , as specified above, or pick something equally simple yet more informative. Note: Please don't use additional underscores ( _ ) in this extension! . For more details, see the get_reference documentation . Configuring the paladin_map subworkflow \u00b6 To set up your sample info and build a configfile, see Understanding and Configuring Workflows . If you want to add the paladin_map program parameters to a previously built configfile, run: elvers config.yaml paladin_map --print_params A small set of parameters should print to your console: #################### paladin_map #################### get_data: download_data: false use_ftp: false trimmomatic: adapter_file: pe_path: ep_utils/TruSeq3-PE-2.fa se_path: ep_utils/TruSeq3-SE.fa extra: '' trim_cmd: ILLUMINACLIP:{}:2:40:15 LEADING:2 TRAILING:2 SLIDINGWINDOW:4:15 MINLEN:25 pear: input_kmer_trimmed: false input_trimmomatic_trimmed: true max_memory: 4G pval: 0.01 extra: '' paladin: alignment_params: extra: '' f: 125 index_params: reference_type: '3' gff_file: '' ####################################################### Override default params for any program by placing these lines in your yaml config file, and modifying values as desired. For more details, see Understanding and Configuring Workflows .For more on what parameters are available, see the docs for each specific program or utility rule: get_data trimmomatic pear paladin","title":"paladin_map"},{"location":"paladin_map/#paladin_map-subworkflow","text":"Subworkflows combine tools in the right order to facilitate file targeting within elvers . The \"paladin_map\" subworkflow conducts read quality trimming and paladin mapping to a protein reference. It requires an assembly to be provided, either by running an assembly or providing one in your configfile. At the moment, this workflow consists of: get_data - an elvers utility trimmomatic pear paladin","title":"Paladin_Map Subworkflow"},{"location":"paladin_map/#quickstart","text":"If you've generated an assembly, even if you've already run elvers examples/nema.yaml assemble : 1) \"Run\" trinity assembly at the same time. If you've already run the assembly, elvers will just locateyour assembly file for paladin_map . elvers examples/nema.yaml assemble paladin_map 2) OR, Pass an assembly in via get_reference , with an assembly in your yaml configfile, e.g.: elvers get_reference paladin_map In the configfile: get_reference: reference: examples/nema.assembly.fasta gene_trans_map: examples/nema.assembly.fasta.gene_trans_map #optional reference_extension: '_plass' This is commented out in the test data yaml, but go ahead and uncomment (remove leading # ) in order to use this option. If you have a gene to transcript map, please specify it as well. If not, delete this line from your config . The assembly_extension parameter is important: this is what allows us to build assemblies from several different assemblers on the same dataset. Feel free to use _input , as specified above, or pick something equally simple yet more informative. Note: Please don't use additional underscores ( _ ) in this extension! . For more details, see the get_reference documentation .","title":"Quickstart"},{"location":"paladin_map/#configuring-the-paladin_map-subworkflow","text":"To set up your sample info and build a configfile, see Understanding and Configuring Workflows . If you want to add the paladin_map program parameters to a previously built configfile, run: elvers config.yaml paladin_map --print_params A small set of parameters should print to your console: #################### paladin_map #################### get_data: download_data: false use_ftp: false trimmomatic: adapter_file: pe_path: ep_utils/TruSeq3-PE-2.fa se_path: ep_utils/TruSeq3-SE.fa extra: '' trim_cmd: ILLUMINACLIP:{}:2:40:15 LEADING:2 TRAILING:2 SLIDINGWINDOW:4:15 MINLEN:25 pear: input_kmer_trimmed: false input_trimmomatic_trimmed: true max_memory: 4G pval: 0.01 extra: '' paladin: alignment_params: extra: '' f: 125 index_params: reference_type: '3' gff_file: '' ####################################################### Override default params for any program by placing these lines in your yaml config file, and modifying values as desired. For more details, see Understanding and Configuring Workflows .For more on what parameters are available, see the docs for each specific program or utility rule: get_data trimmomatic pear paladin","title":"Configuring the paladin_map subworkflow"},{"location":"pear/","text":"Merging Read Pairs with PEAR \u00b6 In order to map PE nucleotide reads to protein assemblies with Paladin , which cannot yet use paired end reads, we use PEAR to merge PE reads prior to mapping. From the PEAR documentation : PEAR is an ultrafast, memory-efficient and highly accurate pair-end read merger. It is fully parallelized and can run with as low as just a few kilobytes of memory. PEAR evaluates all possible paired-end read overlaps and without requiring the target fragment size as input. In addition, it implements a statistical test for minimizing false-positive results. Together with a highly optimized implementation, it can merge millions of paired end reads within a couple of minutes on a standard desktop computer. Quickstart: Running PEAR with elvers \u00b6 We recommend you run pear as part of the paladin_map subworkflow elvers examples/nema.yaml paladin_map This will run trimmomatic trimming prior to PEAR merging of paired end reads and then paladin mapping. If you'd like to just run PEAR , see \"Advanced Usage\" below. PEAR Command \u00b6 On the command line, the command elvers runs for each set of files is approximately: pear -f forward_reads -r reverse_reads \\ -p p_value -j snakemake.threads -y max_memory \\ extra -o output_basename Output files: \u00b6 Your main output directory will be determined by your config file: by default it is BASENAME_out (you specify BASENAME). PEAR will output files in the preprocess/pear subdirectory of this output directory. PEAR output will have the same sample name as input, but end with .pear.fq.gz . Modifying Params for PEAR: \u00b6 Be sure to set up your sample info and build a configfile first (see Understanding and Configuring Workflows ). To see the available parameters for the PEAR rule, run elvers config pear --print_params This will print the following: #################### pear #################### pear: input_kmer_trimmed: false input_trimmomatic_trimmed: true max_memory: 4G pval: 0.01 extra: '' ##################################################### Please modify the max_memory parameter to fit the needs of your reads and system. Within the Protein Assembly workflow or the paladin_map subworkflow , these options enable you to choose kmer-trimmed, quality-trimmed, or raw sequencing data as input. We recommend using quality-trimmed reads as input. If both input_kmer_trimmed and input_trimmomatic_trimmed are False , we will just use raw reads from the samples.tsv file. In addition to changing parameters we've specifically enabled, you can modify the extra param to pass in additional parameters, e.g.: extra: ' --some_param that_param ' Please see the PEAR documentation for info on the params you can pass into PEAR. Be sure the modified lines go into the config file you're using to run elvers (see Understanding and Configuring Workflows ). Advanced Usage: Running PEAR as a standalone rule \u00b6 You can run pear as a standalone rule, instead of withing a larger elvers workflow. However, to do this, you need to make sure the input files are available. For pear, the default input files are quality-trimmed input data (e.g. output of trimmomatic). If you've already done this, you can run: elvers my_config pear If not, you can run the prior steps at the same time to make sure pear can find these input files: elvers my_config get_data trimmomatic pear Note, PEAR only works on paired end reads, as there's nothing to do for single end reads! Snakemake Rule \u00b6 We wrote a new PEAR snakemake wrapper to run PEAR via snakemake. This wrapper has not been added to the official snakemake-wrappers repo yet, but feel free to use as needed. For snakemake afficionados, see our pear rule on github .","title":"pear"},{"location":"pear/#merging-read-pairs-with-pear","text":"In order to map PE nucleotide reads to protein assemblies with Paladin , which cannot yet use paired end reads, we use PEAR to merge PE reads prior to mapping. From the PEAR documentation : PEAR is an ultrafast, memory-efficient and highly accurate pair-end read merger. It is fully parallelized and can run with as low as just a few kilobytes of memory. PEAR evaluates all possible paired-end read overlaps and without requiring the target fragment size as input. In addition, it implements a statistical test for minimizing false-positive results. Together with a highly optimized implementation, it can merge millions of paired end reads within a couple of minutes on a standard desktop computer.","title":"Merging Read Pairs with PEAR"},{"location":"pear/#quickstart-running-pear-with-elvers","text":"We recommend you run pear as part of the paladin_map subworkflow elvers examples/nema.yaml paladin_map This will run trimmomatic trimming prior to PEAR merging of paired end reads and then paladin mapping. If you'd like to just run PEAR , see \"Advanced Usage\" below.","title":"Quickstart: Running PEAR with elvers"},{"location":"pear/#pear-command","text":"On the command line, the command elvers runs for each set of files is approximately: pear -f forward_reads -r reverse_reads \\ -p p_value -j snakemake.threads -y max_memory \\ extra -o output_basename","title":"PEAR Command"},{"location":"pear/#output-files","text":"Your main output directory will be determined by your config file: by default it is BASENAME_out (you specify BASENAME). PEAR will output files in the preprocess/pear subdirectory of this output directory. PEAR output will have the same sample name as input, but end with .pear.fq.gz .","title":"Output files:"},{"location":"pear/#modifying-params-for-pear","text":"Be sure to set up your sample info and build a configfile first (see Understanding and Configuring Workflows ). To see the available parameters for the PEAR rule, run elvers config pear --print_params This will print the following: #################### pear #################### pear: input_kmer_trimmed: false input_trimmomatic_trimmed: true max_memory: 4G pval: 0.01 extra: '' ##################################################### Please modify the max_memory parameter to fit the needs of your reads and system. Within the Protein Assembly workflow or the paladin_map subworkflow , these options enable you to choose kmer-trimmed, quality-trimmed, or raw sequencing data as input. We recommend using quality-trimmed reads as input. If both input_kmer_trimmed and input_trimmomatic_trimmed are False , we will just use raw reads from the samples.tsv file. In addition to changing parameters we've specifically enabled, you can modify the extra param to pass in additional parameters, e.g.: extra: ' --some_param that_param ' Please see the PEAR documentation for info on the params you can pass into PEAR. Be sure the modified lines go into the config file you're using to run elvers (see Understanding and Configuring Workflows ).","title":"Modifying Params for PEAR:"},{"location":"pear/#advanced-usage-running-pear-as-a-standalone-rule","text":"You can run pear as a standalone rule, instead of withing a larger elvers workflow. However, to do this, you need to make sure the input files are available. For pear, the default input files are quality-trimmed input data (e.g. output of trimmomatic). If you've already done this, you can run: elvers my_config pear If not, you can run the prior steps at the same time to make sure pear can find these input files: elvers my_config get_data trimmomatic pear Note, PEAR only works on paired end reads, as there's nothing to do for single end reads!","title":"Advanced Usage: Running PEAR as a standalone rule"},{"location":"pear/#snakemake-rule","text":"We wrote a new PEAR snakemake wrapper to run PEAR via snakemake. This wrapper has not been added to the official snakemake-wrappers repo yet, but feel free to use as needed. For snakemake afficionados, see our pear rule on github .","title":"Snakemake Rule"},{"location":"plass/","text":"Protein Assembly with PLASS \u00b6 The basic idea with any transcriptome or metatranscriptome assembly is you feed in your reads and you get out a bunch of contigs that represent transcripts, or stretches of RNA present in the reads. You run a transcriptome assembly program using the adapter & k-mer trimmed reads as input and get out a pile of assembled RNA. Most assemblers work in nucleotide space. This means that they find direct overlaps between the As, Ts, Cs, and Gs in the reads and use information about those overlaps to make contigs. Although this is a powerful method, it often fails when: There is sequencing errors There are repetitive regions There is strain variation When assembly fails, contigs either break or don't assemble at all. Given that nucleotide sequences are far more variable than protein sequences (third base pair wobble in strain variation in particular), assembling in amino acid space can overcome a lot of the difficulties encountered when assembling in nucleotide space. PLASS is a new assembler that assembles in amino acid space. Unlike many other assemblers, including Trinity , it does not have built in error correction and so it is best to adapter and k-mer trim the reads before using it. It sometimes performs better than nucleotide assemblers and so is good to test on samples to see what type of improvement it can give. Quickstart \u00b6 Run PLASS via the Protein Assembly workflow or via the plass_assemble subworkflow . These workflows will run preprocessing and kmer-trimming for you prior to assembly, and may run additional downstream steps. To run PLASS as a standalone program, see \"Advanced Usage\" section below. elvers examples/nema.yaml plass_assemble PLASS Command \u00b6 On the command line, the command elvers runs is approximately: plass assemble input_1.fq input_2.fq \\ outputi_plass.fasta --threads snakemake.threads extra_params Output files: \u00b6 Your main output directory will be determined by your config file: by default it is BASENAME_out (you specify BASENAME). PLASS will output files in the assembly subdirectory of this output directory. The fasta file will be BASENAME_plass.fasta . Modifying Params for PLASS: \u00b6 Be sure to set up your sample info and build a configfile first (see Understanding and Configuring Workflows ). To see the available parameters for the plass rule, run elvers config plass --print_params This will print the following: #################### plass #################### plass: input_kmer_trimmed: true input_trimmomatic_trimmed: false add_single_to_paired: false # would you like to add the orphaned reads to the plass assembly? extra: '' ##################################################### Within the Protein Assembly workflow or the plass_assemble subworkflow , these options enable you to choose kmer-trimmed, quality-trimmed, or raw sequencing data as input. We recommend using kmer-trimmed reads as input. If both input_kmer_trimmed and input_trimmomatic_trimmed are False , we will just use raw reads from the samples.tsv file. In addition to changing parameters we've specifically enabled, you can modify the extra param to pass any extra plass parameters, e.g.: extra: '--someflag someparam --someotherflag thatotherparam' See the PLASS documentation to learn more about the parameters you can pass to PLASS . Be sure the modified lines go into the config file you're using to run elvers (see Understanding and Configuring Workflows ). Advanced Usage: Running PLASS as a standalone rule \u00b6 You can run plass as a standalone rule, instead of withing a larger elvers workflow. However, to do this, you need to make sure the input files are available. For plass, the default input files are kmer-trimmed input data (e.g. output of khmer). If you've already done this, you can run: elvers my_config plass If not, you can run the prior steps at the same time to make sure khmer can find these input files: elvers my_config get_data trimmomatic khmer plass Snakemake Rules \u00b6 We wrote a PLASS snakemake wrapper to run PLASS via snakemake. This wrapper has not yet been submitted to the snakemake-wrappers repository, but feel free to use it as needed. For snakemake afficionados, see our PLASS rule on github .","title":"plass"},{"location":"plass/#protein-assembly-with-plass","text":"The basic idea with any transcriptome or metatranscriptome assembly is you feed in your reads and you get out a bunch of contigs that represent transcripts, or stretches of RNA present in the reads. You run a transcriptome assembly program using the adapter & k-mer trimmed reads as input and get out a pile of assembled RNA. Most assemblers work in nucleotide space. This means that they find direct overlaps between the As, Ts, Cs, and Gs in the reads and use information about those overlaps to make contigs. Although this is a powerful method, it often fails when: There is sequencing errors There are repetitive regions There is strain variation When assembly fails, contigs either break or don't assemble at all. Given that nucleotide sequences are far more variable than protein sequences (third base pair wobble in strain variation in particular), assembling in amino acid space can overcome a lot of the difficulties encountered when assembling in nucleotide space. PLASS is a new assembler that assembles in amino acid space. Unlike many other assemblers, including Trinity , it does not have built in error correction and so it is best to adapter and k-mer trim the reads before using it. It sometimes performs better than nucleotide assemblers and so is good to test on samples to see what type of improvement it can give.","title":"Protein Assembly with PLASS"},{"location":"plass/#quickstart","text":"Run PLASS via the Protein Assembly workflow or via the plass_assemble subworkflow . These workflows will run preprocessing and kmer-trimming for you prior to assembly, and may run additional downstream steps. To run PLASS as a standalone program, see \"Advanced Usage\" section below. elvers examples/nema.yaml plass_assemble","title":"Quickstart"},{"location":"plass/#plass-command","text":"On the command line, the command elvers runs is approximately: plass assemble input_1.fq input_2.fq \\ outputi_plass.fasta --threads snakemake.threads extra_params","title":"PLASS Command"},{"location":"plass/#output-files","text":"Your main output directory will be determined by your config file: by default it is BASENAME_out (you specify BASENAME). PLASS will output files in the assembly subdirectory of this output directory. The fasta file will be BASENAME_plass.fasta .","title":"Output files:"},{"location":"plass/#modifying-params-for-plass","text":"Be sure to set up your sample info and build a configfile first (see Understanding and Configuring Workflows ). To see the available parameters for the plass rule, run elvers config plass --print_params This will print the following: #################### plass #################### plass: input_kmer_trimmed: true input_trimmomatic_trimmed: false add_single_to_paired: false # would you like to add the orphaned reads to the plass assembly? extra: '' ##################################################### Within the Protein Assembly workflow or the plass_assemble subworkflow , these options enable you to choose kmer-trimmed, quality-trimmed, or raw sequencing data as input. We recommend using kmer-trimmed reads as input. If both input_kmer_trimmed and input_trimmomatic_trimmed are False , we will just use raw reads from the samples.tsv file. In addition to changing parameters we've specifically enabled, you can modify the extra param to pass any extra plass parameters, e.g.: extra: '--someflag someparam --someotherflag thatotherparam' See the PLASS documentation to learn more about the parameters you can pass to PLASS . Be sure the modified lines go into the config file you're using to run elvers (see Understanding and Configuring Workflows ).","title":"Modifying Params for PLASS:"},{"location":"plass/#advanced-usage-running-plass-as-a-standalone-rule","text":"You can run plass as a standalone rule, instead of withing a larger elvers workflow. However, to do this, you need to make sure the input files are available. For plass, the default input files are kmer-trimmed input data (e.g. output of khmer). If you've already done this, you can run: elvers my_config plass If not, you can run the prior steps at the same time to make sure khmer can find these input files: elvers my_config get_data trimmomatic khmer plass","title":"Advanced Usage: Running PLASS as a standalone rule"},{"location":"plass/#snakemake-rules","text":"We wrote a PLASS snakemake wrapper to run PLASS via snakemake. This wrapper has not yet been submitted to the snakemake-wrappers repository, but feel free to use it as needed. For snakemake afficionados, see our PLASS rule on github .","title":"Snakemake Rules"},{"location":"plass_assemble/","text":"Protein Assembly Subworkflow \u00b6 Subworkflows combine tools in the right order to facilitate file targeting withing elvers . The \"plass_assemble\"\" subworkflow conducts read quality trimming, kmer trimming, and protein-level assembly with plass . At the moment, this workflow consists of: get_data - an elvers utility trimmomatic fastqc , run on both pre-trim and post-trim data khmer plass Quickstart \u00b6 To run the plass_assemble subworkflow, run: elvers examples/nema.yaml plass_assemble Configuring the assemble subworkflow \u00b6 To set up your sample info and build a configfile, see Understanding and Configuring Workflows . If you want to add the plass_assemble program parameters to a previously built configfile, run: elvers config.yaml plass_assemble --print_params A small set of parameters should print to your console: #################### plass_assemble #################### get_data: download_data: false use_ftp: false trimmomatic: adapter_file: pe_path: ep_utils/TruSeq3-PE-2.fa se_path: ep_utils/TruSeq3-SE.fa extra: '' trim_cmd: ILLUMINACLIP:{}:2:40:15 LEADING:2 TRAILING:2 SLIDINGWINDOW:4:15 MINLEN:25 fastqc: extra: '' khmer: C: 3 Z: 18 coverage: 20 diginorm: true extra: '' ksize: 20 memory: 4e9 plass: input_kmer_trimmed: true input_trimmomatic_trimmed: false add_single_to_paired: false # would you like to add the orphaned reads to the plass assembly? extra: '' ####################################################### Override default params for any program by placing these lines in your yaml config file, and modifying values as desired. For more details, see Understanding and Configuring Workflows .For more on what parameters are available, see the docs for each specific program or utility rule: get_data trimmomatic fastqc khmer plass","title":"plass_assemble"},{"location":"plass_assemble/#protein-assembly-subworkflow","text":"Subworkflows combine tools in the right order to facilitate file targeting withing elvers . The \"plass_assemble\"\" subworkflow conducts read quality trimming, kmer trimming, and protein-level assembly with plass . At the moment, this workflow consists of: get_data - an elvers utility trimmomatic fastqc , run on both pre-trim and post-trim data khmer plass","title":"Protein Assembly Subworkflow"},{"location":"plass_assemble/#quickstart","text":"To run the plass_assemble subworkflow, run: elvers examples/nema.yaml plass_assemble","title":"Quickstart"},{"location":"plass_assemble/#configuring-the-assemble-subworkflow","text":"To set up your sample info and build a configfile, see Understanding and Configuring Workflows . If you want to add the plass_assemble program parameters to a previously built configfile, run: elvers config.yaml plass_assemble --print_params A small set of parameters should print to your console: #################### plass_assemble #################### get_data: download_data: false use_ftp: false trimmomatic: adapter_file: pe_path: ep_utils/TruSeq3-PE-2.fa se_path: ep_utils/TruSeq3-SE.fa extra: '' trim_cmd: ILLUMINACLIP:{}:2:40:15 LEADING:2 TRAILING:2 SLIDINGWINDOW:4:15 MINLEN:25 fastqc: extra: '' khmer: C: 3 Z: 18 coverage: 20 diginorm: true extra: '' ksize: 20 memory: 4e9 plass: input_kmer_trimmed: true input_trimmomatic_trimmed: false add_single_to_paired: false # would you like to add the orphaned reads to the plass assembly? extra: '' ####################################################### Override default params for any program by placing these lines in your yaml config file, and modifying values as desired. For more details, see Understanding and Configuring Workflows .For more on what parameters are available, see the docs for each specific program or utility rule: get_data trimmomatic fastqc khmer plass","title":"Configuring the assemble subworkflow"},{"location":"preprocess/","text":"Preprocess Subworkflow \u00b6 Subworkflows combine tools in the right order to facilitate file targeting withing elvers . The \"preprocess\" subworkflow conducts read quality trimming. At the moment, this workflow consists of: get_data - an elvers utility trimmomatic fastqc , run on both pre-trim and post-trim data Quickstart \u00b6 To run the preprocess subworkflow, run: elvers examples/nema.yaml preprocess Configuring the preprocess workflow \u00b6 To set up your sample info and build a configfile, see Understanding and Configuring Workflows . If you want to add the preprocess program parameters to a previously built configfile, run: elvers config.yaml preprocess --print_params A small set of parameters should print to your console: #################### preprocess #################### get_data: download_data: false use_ftp: false trimmomatic: adapter_file: pe_path: ep_utils/TruSeq3-PE-2.fa se_path: ep_utils/TruSeq3-SE.fa trim_cmd: ILLUMINACLIP:{}:2:40:15 LEADING:2 TRAILING:2 SLIDINGWINDOW:4:15 MINLEN:25 extra: '' fastqc: extra: '' Override default params for any program by placing these lines in your yaml config file, and modifying values as desired. For more details, see Understanding and Configuring Workflows .For more on what parameters are available, see the docs for each specific program or utility rule: get_data trimmomatic fastqc","title":"preprocess"},{"location":"preprocess/#preprocess-subworkflow","text":"Subworkflows combine tools in the right order to facilitate file targeting withing elvers . The \"preprocess\" subworkflow conducts read quality trimming. At the moment, this workflow consists of: get_data - an elvers utility trimmomatic fastqc , run on both pre-trim and post-trim data","title":"Preprocess Subworkflow"},{"location":"preprocess/#quickstart","text":"To run the preprocess subworkflow, run: elvers examples/nema.yaml preprocess","title":"Quickstart"},{"location":"preprocess/#configuring-the-preprocess-workflow","text":"To set up your sample info and build a configfile, see Understanding and Configuring Workflows . If you want to add the preprocess program parameters to a previously built configfile, run: elvers config.yaml preprocess --print_params A small set of parameters should print to your console: #################### preprocess #################### get_data: download_data: false use_ftp: false trimmomatic: adapter_file: pe_path: ep_utils/TruSeq3-PE-2.fa se_path: ep_utils/TruSeq3-SE.fa trim_cmd: ILLUMINACLIP:{}:2:40:15 LEADING:2 TRAILING:2 SLIDINGWINDOW:4:15 MINLEN:25 extra: '' fastqc: extra: '' Override default params for any program by placing these lines in your yaml config file, and modifying values as desired. For more details, see Understanding and Configuring Workflows .For more on what parameters are available, see the docs for each specific program or utility rule: get_data trimmomatic fastqc","title":"Configuring the preprocess workflow"},{"location":"protein_assembly_workflow/","text":"Protein Assembly Workflow \u00b6 The protein assembly workflow relies on the PLASS assembler and some downstream protein mapping tools. It consists of: fastqc trimmomatic khmer PLASS pear paladin Running Test Data \u00b6 elvers examples/nema.yaml protein_assembly This will run a small set of Nematostella vectensis test data (from Tulin et al., 2013 ). Running Your Own Data \u00b6 Set up your sample info and build a configfile first (see Understanding and Configuring Workflows ). To build a config, run: elvers prot.yaml protein_assembly --build_config The resulting prot.yaml configfile for this workflow will look something like this. The order of the parameters may be different and does not affect the order in which steps are run. Please see the documentation file for each individual program (linked above) for what parameters to modify. #################### Eelpond Pipeline Configfile #################### basename: elvers experiment: _experiment1 samples: samples.tsv #################### protein_assembly #################### fastqc: extra: '' get_data: download_data: false khmer: C: 3 Z: 18 coverage: 20 diginorm: true extra: '' ksize: 20 memory: 4e9 paladin: alignment_params: extra: '' f: 125 index_params: reference_type: '3' pear: extra: '' input_kmer_trimmed: false input_trimmomatic_trimmed: true max_memory: 4G pval: 0.01 plass: add_single_to_paired: false extra: '' input_kmer_trimmed: true input_trimmomatic_trimmed: false sourmash: extra: '' k_size: 31 scaled: 1000 trimmomatic: adapter_file: pe_path: ep_utils/TruSeq3-PE-2.fa se_path: ep_utils/TruSeq3-SE.fa extra: '' trim_cmd: ILLUMINACLIP:{}:2:40:15 LEADING:2 TRAILING:2 SLIDINGWINDOW:4:15 MINLEN:25","title":"Protein-level Assembly Workflow"},{"location":"protein_assembly_workflow/#protein-assembly-workflow","text":"The protein assembly workflow relies on the PLASS assembler and some downstream protein mapping tools. It consists of: fastqc trimmomatic khmer PLASS pear paladin","title":"Protein Assembly Workflow"},{"location":"protein_assembly_workflow/#running-test-data","text":"elvers examples/nema.yaml protein_assembly This will run a small set of Nematostella vectensis test data (from Tulin et al., 2013 ).","title":"Running Test Data"},{"location":"protein_assembly_workflow/#running-your-own-data","text":"Set up your sample info and build a configfile first (see Understanding and Configuring Workflows ). To build a config, run: elvers prot.yaml protein_assembly --build_config The resulting prot.yaml configfile for this workflow will look something like this. The order of the parameters may be different and does not affect the order in which steps are run. Please see the documentation file for each individual program (linked above) for what parameters to modify. #################### Eelpond Pipeline Configfile #################### basename: elvers experiment: _experiment1 samples: samples.tsv #################### protein_assembly #################### fastqc: extra: '' get_data: download_data: false khmer: C: 3 Z: 18 coverage: 20 diginorm: true extra: '' ksize: 20 memory: 4e9 paladin: alignment_params: extra: '' f: 125 index_params: reference_type: '3' pear: extra: '' input_kmer_trimmed: false input_trimmomatic_trimmed: true max_memory: 4G pval: 0.01 plass: add_single_to_paired: false extra: '' input_kmer_trimmed: true input_trimmomatic_trimmed: false sourmash: extra: '' k_size: 31 scaled: 1000 trimmomatic: adapter_file: pe_path: ep_utils/TruSeq3-PE-2.fa se_path: ep_utils/TruSeq3-SE.fa extra: '' trim_cmd: ILLUMINACLIP:{}:2:40:15 LEADING:2 TRAILING:2 SLIDINGWINDOW:4:15 MINLEN:25","title":"Running Your Own Data"},{"location":"quantify/","text":"Quantify Subworkflow \u00b6 Subworkflows combine tools in the right order to facilitate file targeting withing elvers . The \"quantify\" subworkflow conducts read quality trimming and salmon quantification. It requires an assembly to be provided, either by running an assembly or providing one in your configfile. At the moment, this workflow consists of: get_data - an elvers utility trimmomatic salmon Quickstart \u00b6 If you've generated an assembly, even if you've already run elvers examples/nema.yaml assemble : 1) \"Run\" trinity assembly at the same time. If you've already run the assembly, elvers will just locateyour assembly file for quantify . elvers examples/nema.yaml assemble quantify 2) OR, Pass an assembly in via get_reference with an assembly in your yaml configfile, e.g.: elvers get_reference quantify In the configfile: get_reference: reference: examples/nema.assembly.fasta gene_trans_map: examples/nema.assembly.fasta.gene_trans_map #optional reference_extension: '_input' This is commented out in the test data yaml, but go ahead and uncomment (remove leading `#`) in order to use this option. If you have a gene to transcript map, please specify it as well. If not, delete this line from your `config`. The `assembly_extension` parameter is important: this is what allows us to build assemblies from several different assemblers on the same dataset. Feel free to use `_input`, as specified above, or pick something equally simple yet more informative. **Note: Please don't use additional underscores (`_`) in this extension!**. For more details, see the [get_reference documentation](get_reference.md). Configuring the quantify subworkflow \u00b6 To set up your sample info and build a configfile, see Understanding and Configuring Workflows . If you want to add the quantify program parameters to a previously built configfile, run: elvers config.yaml quantify --print_params A small set of parameters should print to your console: #################### quantify #################### get_data: download_data: false use_ftp: false trimmomatic: adapter_file: pe_path: ep_utils/TruSeq3-PE-2.fa se_path: ep_utils/TruSeq3-SE.fa extra: '' trim_cmd: ILLUMINACLIP:{}:2:40:15 LEADING:2 TRAILING:2 SLIDINGWINDOW:4:15 MINLEN:25 salmon: input_trimmomatic_trimmed: True index_params: extra: '' quant_params: libtype: A extra: '' ####################################################### Override default params for any program by placing these lines in your yaml config file, and modifying values as desired. For more details, see Understanding and Configuring Workflows .For more on what parameters are available, see the docs for each specific program or utility rule: get_data trimmomatic salmon","title":"quantify"},{"location":"quantify/#quantify-subworkflow","text":"Subworkflows combine tools in the right order to facilitate file targeting withing elvers . The \"quantify\" subworkflow conducts read quality trimming and salmon quantification. It requires an assembly to be provided, either by running an assembly or providing one in your configfile. At the moment, this workflow consists of: get_data - an elvers utility trimmomatic salmon","title":"Quantify Subworkflow"},{"location":"quantify/#quickstart","text":"If you've generated an assembly, even if you've already run elvers examples/nema.yaml assemble : 1) \"Run\" trinity assembly at the same time. If you've already run the assembly, elvers will just locateyour assembly file for quantify . elvers examples/nema.yaml assemble quantify 2) OR, Pass an assembly in via get_reference with an assembly in your yaml configfile, e.g.: elvers get_reference quantify In the configfile: get_reference: reference: examples/nema.assembly.fasta gene_trans_map: examples/nema.assembly.fasta.gene_trans_map #optional reference_extension: '_input' This is commented out in the test data yaml, but go ahead and uncomment (remove leading `#`) in order to use this option. If you have a gene to transcript map, please specify it as well. If not, delete this line from your `config`. The `assembly_extension` parameter is important: this is what allows us to build assemblies from several different assemblers on the same dataset. Feel free to use `_input`, as specified above, or pick something equally simple yet more informative. **Note: Please don't use additional underscores (`_`) in this extension!**. For more details, see the [get_reference documentation](get_reference.md).","title":"Quickstart"},{"location":"quantify/#configuring-the-quantify-subworkflow","text":"To set up your sample info and build a configfile, see Understanding and Configuring Workflows . If you want to add the quantify program parameters to a previously built configfile, run: elvers config.yaml quantify --print_params A small set of parameters should print to your console: #################### quantify #################### get_data: download_data: false use_ftp: false trimmomatic: adapter_file: pe_path: ep_utils/TruSeq3-PE-2.fa se_path: ep_utils/TruSeq3-SE.fa extra: '' trim_cmd: ILLUMINACLIP:{}:2:40:15 LEADING:2 TRAILING:2 SLIDINGWINDOW:4:15 MINLEN:25 salmon: input_trimmomatic_trimmed: True index_params: extra: '' quant_params: libtype: A extra: '' ####################################################### Override default params for any program by placing these lines in your yaml config file, and modifying values as desired. For more details, see Understanding and Configuring Workflows .For more on what parameters are available, see the docs for each specific program or utility rule: get_data trimmomatic salmon","title":"Configuring the quantify subworkflow"},{"location":"quickstart/","text":"Quickstart \u00b6 Linux is the recommended OS. Nearly everything also works on MacOSX, but some programs (FastQC, Trinity) are troublesome. Installations: Conda \u00b6 elvers uses conda , an open source package and environment management system, to manage tool installations. The quickest way to get started with conda is to install miniconda . If you don't have conda yet, install miniconda (for Ubuntu 16.04 Jetstream image ): wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh Be sure to answer 'yes' to all yes/no questions. You'll need to restart your terminal for conda to be active. As a side note, if you're working on a cloud computing system, you may be able to find an image where conda has been pre-installed (such as this Ubuntu 16.04 Jetstream image ). Create a working environment and install Elvers! \u00b6 elvers needs a few programs installed in order to run properly. To handle this, we run elvers within a conda environment that contains all dependencies. Get the elvers code git clone https://github.com/dib-lab/elvers.git cd elvers When you first get elvers , you'll need to create this environment on your machine: conda env create --file environment.yml -n elvers-env Now, activate that environment: conda activate elvers-env To deactivate after you've finished running elvers , type conda deactivate . You'll need to reactivate this environment anytime you want to run elvers. Now. install the elvers package pip install -e '.' Now you can start running workflows on test data! Running test data \u00b6 The Eel Pond protocol (which inspired the elvers name) included line-by-line commands that the user could follow along with using a test dataset provided in the instructions. We have re-implemented the protocol here to enable automated de novo transcriptome assembly, annotation, and quick differential expression analysis on a set of short-read Illumina data using a single command. See more about this protocol here . To test the default workflow: elvers examples/nema.yaml default This will download and run a small set of Nematostella vectensis test data (from Tulin et al., 2013 ) Running Your Own Data \u00b6 To run your own data, you'll need to create two files: a tsv file containing your sample info a yaml file containing basic configuration info Generate these by following instructions here: Understanding and Configuring Workflows . Available Workflows \u00b6 workflows preprocess: Read Quality Trimming and Filtering (fastqc, trimmomatic) kmer_trim: Kmer Trimming and/or Digital Normalization (khmer) assemble: Transcriptome Assembly (trinity) get_reference: Specify assembly for downstream steps annotate : Annotate the transcriptome (dammit) sourmash_compute: Build sourmash signatures for the reads and assembly (sourmash) quantify: Quantify transcripts (salmon) diffexp: Conduct differential expression (DESeq2) plass_assemble: assemble at the protein level with PLASS paladin_map: map to a protein assembly using paladin end-to-end workflows: default : preprocess, kmer_trim, assemble, annotate, quantify protein assembly : preprocess, kmer_trim, plass_assemble, paladin_map You can see the available workflows (and which programs they run) by using the --print_workflows flag: elvers examples/nema.yaml --print_workflows Each included tool can also be run independently, if appropriate input files are provided. This is not always intuitive, so please see our documentation for running each tools for details (described as \"Advanced Usage\"). To see all available tools, run: elvers examples/nema.yaml --print_rules Additional Info \u00b6 See the help, here: elvers -h References: original eel-pond protocol docs, last updated 2015 eel-pond protocol docs, last updated 2016 DIBSI, nonmodel RNAseq workshop, July 2017 SIO-BUG, nonmodel RNAseq workshop, October 2017","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"Linux is the recommended OS. Nearly everything also works on MacOSX, but some programs (FastQC, Trinity) are troublesome.","title":"Quickstart"},{"location":"quickstart/#installations-conda","text":"elvers uses conda , an open source package and environment management system, to manage tool installations. The quickest way to get started with conda is to install miniconda . If you don't have conda yet, install miniconda (for Ubuntu 16.04 Jetstream image ): wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh Be sure to answer 'yes' to all yes/no questions. You'll need to restart your terminal for conda to be active. As a side note, if you're working on a cloud computing system, you may be able to find an image where conda has been pre-installed (such as this Ubuntu 16.04 Jetstream image ).","title":"Installations: Conda"},{"location":"quickstart/#create-a-working-environment-and-install-elvers","text":"elvers needs a few programs installed in order to run properly. To handle this, we run elvers within a conda environment that contains all dependencies. Get the elvers code git clone https://github.com/dib-lab/elvers.git cd elvers When you first get elvers , you'll need to create this environment on your machine: conda env create --file environment.yml -n elvers-env Now, activate that environment: conda activate elvers-env To deactivate after you've finished running elvers , type conda deactivate . You'll need to reactivate this environment anytime you want to run elvers. Now. install the elvers package pip install -e '.' Now you can start running workflows on test data!","title":"Create a working environment and install Elvers!"},{"location":"quickstart/#running-test-data","text":"The Eel Pond protocol (which inspired the elvers name) included line-by-line commands that the user could follow along with using a test dataset provided in the instructions. We have re-implemented the protocol here to enable automated de novo transcriptome assembly, annotation, and quick differential expression analysis on a set of short-read Illumina data using a single command. See more about this protocol here . To test the default workflow: elvers examples/nema.yaml default This will download and run a small set of Nematostella vectensis test data (from Tulin et al., 2013 )","title":"Running test data"},{"location":"quickstart/#running-your-own-data","text":"To run your own data, you'll need to create two files: a tsv file containing your sample info a yaml file containing basic configuration info Generate these by following instructions here: Understanding and Configuring Workflows .","title":"Running Your Own Data"},{"location":"quickstart/#available-workflows","text":"workflows preprocess: Read Quality Trimming and Filtering (fastqc, trimmomatic) kmer_trim: Kmer Trimming and/or Digital Normalization (khmer) assemble: Transcriptome Assembly (trinity) get_reference: Specify assembly for downstream steps annotate : Annotate the transcriptome (dammit) sourmash_compute: Build sourmash signatures for the reads and assembly (sourmash) quantify: Quantify transcripts (salmon) diffexp: Conduct differential expression (DESeq2) plass_assemble: assemble at the protein level with PLASS paladin_map: map to a protein assembly using paladin end-to-end workflows: default : preprocess, kmer_trim, assemble, annotate, quantify protein assembly : preprocess, kmer_trim, plass_assemble, paladin_map You can see the available workflows (and which programs they run) by using the --print_workflows flag: elvers examples/nema.yaml --print_workflows Each included tool can also be run independently, if appropriate input files are provided. This is not always intuitive, so please see our documentation for running each tools for details (described as \"Advanced Usage\"). To see all available tools, run: elvers examples/nema.yaml --print_rules","title":"Available Workflows"},{"location":"quickstart/#additional-info","text":"See the help, here: elvers -h References: original eel-pond protocol docs, last updated 2015 eel-pond protocol docs, last updated 2016 DIBSI, nonmodel RNAseq workshop, July 2017 SIO-BUG, nonmodel RNAseq workshop, October 2017","title":"Additional Info"},{"location":"rcorrector/","text":"Correcting reads with Rcorrector \u00b6 coming soon*","title":"rcorrector"},{"location":"rcorrector/#correcting-reads-with-rcorrector","text":"coming soon*","title":"Correcting reads with Rcorrector"},{"location":"salmon/","text":"Quantification with Salmon \u00b6 We can use Salmon to quantify expression. Salmon is a (relatively) new breed of software for quantifying RNAseq reads that is both really fast and takes transcript length into consideration ( Patro et al. 2015 ). Quickstart \u00b6 We recommend that you run salmon quantification via the \"default\" Eel Pond workflow or the quantify subworkflow . See \"Advanced Usage\" below for running salmon as a standalone rule. Salmon Commands \u00b6 There are two commands for salmon, salmon index and salmon quant . The first command, salmon index will index the transcriptome: salmon index --index nema --transcripts nema_trinity.fasta --type quasi And the second command, salmon quant will quantify the trimmed reads (not diginormed) using the transcriptome. For each pair of reads for a sample, we run: salmon quant -i nema -l A -1 <(gunzip -c $R1) -2 <(gunzip -c $R2) -o ${sample_name}_quant Both indexing the transcriptome and running quantification are integrated as rules in the elvers workflow, so the whole process happens in an automated fashion. Modifying Params for Salmon \u00b6 Be sure to set up your sample info and build a configfile first (see Understanding and Configuring Workflows ). To see the available parameters for the salmon rule, run elvers config salmon --print_params This will print the following: #################### salmon #################### salmon: input_trimmomatic_trimmed: True index_params: extra: '' quant_params: libtype: A extra: '' ##################################################### If you set input_trimmomatic_trimmed: False in the salmon parameters, then salmon will use your raw input data instead of trimming first. Using trimmed data as input is recommended, this is just if you're pre-trimmed with another program! In addition to changing parameters we've specifically enabled, you can modify the extra param to pass any extra parameters.In salmon, both index and quantification steps can accept an extra param. See the Salmon documentation to learn more about the parameters you can pass into salmon . Be sure the modified lines go into the config file you're using to run elvers (see Understanding and Configuring Workflows ). Output files: \u00b6 Your main output directory will be determined by your config file: by default it is BASENAME_out (you specify BASENAME). Salmon will output files in the quant subdirectory of this output directory. Each sample will have its own directory, and the two most interesting files will be the salmon_quant.log and quant.sf files. The former contains the log information from running salmon, and the latter contains the transcript count data. A quant.sf file will look something like this. Name Length EffectiveLength TPM NumReads TRINITY_DN2202_c0_g1_i1 210 39.818 2.683835 2.000000 TRINITY_DN2270_c0_g1_i1 213 41.064 0.000000 0.000000 TRINITY_DN2201_c0_g1_i1 266 69.681 0.766816 1.000000 TRINITY_DN2222_c0_g1_i1 243 55.794 2.873014 3.000000 TRINITY_DN2291_c0_g1_i1 245 56.916 0.000000 0.000000 TRINITY_DN2269_c0_g1_i1 294 89.251 0.000000 0.000000 TRINITY_DN2269_c1_g1_i1 246 57.479 0.000000 0.000000 TRINITY_DN2279_c0_g1_i1 426 207.443 0.000000 0.000000 TRINITY_DN2262_c0_g1_i1 500 280.803 0.190459 1.000912 TRINITY_DN2253_c0_g1_i1 1523 1303.116 0.164015 4.000000 TRINITY_DN2287_c0_g1_i1 467 247.962 0.000000 0.000000 TRINITY_DN2287_c1_g1_i1 325 113.826 0.469425 1.000000 TRINITY_DN2237_c0_g1_i1 306 98.441 0.542788 1.000000 TRINITY_DN2237_c0_g2_i1 307 99.229 0.000000 0.000000 TRINITY_DN2250_c0_g1_i1 368 151.832 0.000000 0.000000 TRINITY_DN2250_c1_g1_i1 271 72.988 0.000000 0.000000 TRINITY_DN2208_c0_g1_i1 379 162.080 1.978014 6.000000 TRINITY_DN2277_c0_g1_i1 269 71.657 0.745677 1.000000 TRINITY_DN2231_c0_g1_i1 209 39.409 0.000000 0.000000 TRINITY_DN2231_c1_g1_i1 334 121.411 0.000000 0.000000 TRINITY_DN2204_c0_g1_i1 287 84.121 0.000000 0.000000 More on Salmon \u00b6 For further reading, on salmon see Intro blog post: http://robpatro.com/blog/?p=248 A 2016 blog post evaluating and comparing methods here Salmon github repo here https://github.com/ngs-docs/2015-nov-adv-rna/blob/master/salmon.rst http://angus.readthedocs.io/en/2016/rob_quant/tut.html https://2016-aug-nonmodel-rnaseq.readthedocs.io/en/latest/quantification.html Advanced Usage: Running Salmon as a standalone rule \u00b6 You can run salmon as a standalone rule, instead of withing a larger elvers workflow. However, to do this, you need to make sure the input files are available. For salmon, you need both 1) an assembly, and 2) trimmed input files. The assembly can be generated via another workflow, or passed to elvers via the configfile. Specifying an assembly: 1) If you've alread run read trimming and want to use a Trinity assembly generated via elvers , you can run: elvers my_config assemble salmon If you've already run the assembly, elvers will just use this info to locate that assembly. 2) Alternatively, you can input an assembly via the [get_reference](get_reference.md) utility rule: ``` elvers get_reference salmon ``` with an assembly in your `yaml` configfile, e.g.: ``` get_reference: reference: examples/nema.assembly.fasta gene_trans_map: examples/nema.assembly.fasta.gene_trans_map #optional reference_extension: '_input' ``` This is commented out in the test data yaml, but go ahead and uncomment (remove leading `#`) in order to use this option. If you have a gene to transcript map, please specify it as well. If not, delete this line from your `config`. The `assembly_extension` parameter is important: this is what allows us to build assemblies from several different assemblers on the same dataset. Feel free to use `_input`, as specified above, or pick something equally simple yet more informative. **Note: Please don't use additional underscores (`_`) in this extension!**. For more details, see the [get_reference documentation](get_reference.md). Specifying input reads: If you haven't yet run read trimming, you'll also need to run those steps: ``` elvers myconfig get_data trimmomatic salmon ``` Or if you have set `input_trimmomatic_trimmed: False`: ``` elvers myconfig get_data salmon ``` Snakemake Rule \u00b6 We wrote snakemake wrappers to run salmon index and salmon quant . For snakemake afficionados, see the Salmon rule on github .","title":"salmon"},{"location":"salmon/#quantification-with-salmon","text":"We can use Salmon to quantify expression. Salmon is a (relatively) new breed of software for quantifying RNAseq reads that is both really fast and takes transcript length into consideration ( Patro et al. 2015 ).","title":"Quantification with Salmon"},{"location":"salmon/#quickstart","text":"We recommend that you run salmon quantification via the \"default\" Eel Pond workflow or the quantify subworkflow . See \"Advanced Usage\" below for running salmon as a standalone rule.","title":"Quickstart"},{"location":"salmon/#salmon-commands","text":"There are two commands for salmon, salmon index and salmon quant . The first command, salmon index will index the transcriptome: salmon index --index nema --transcripts nema_trinity.fasta --type quasi And the second command, salmon quant will quantify the trimmed reads (not diginormed) using the transcriptome. For each pair of reads for a sample, we run: salmon quant -i nema -l A -1 <(gunzip -c $R1) -2 <(gunzip -c $R2) -o ${sample_name}_quant Both indexing the transcriptome and running quantification are integrated as rules in the elvers workflow, so the whole process happens in an automated fashion.","title":"Salmon Commands"},{"location":"salmon/#modifying-params-for-salmon","text":"Be sure to set up your sample info and build a configfile first (see Understanding and Configuring Workflows ). To see the available parameters for the salmon rule, run elvers config salmon --print_params This will print the following: #################### salmon #################### salmon: input_trimmomatic_trimmed: True index_params: extra: '' quant_params: libtype: A extra: '' ##################################################### If you set input_trimmomatic_trimmed: False in the salmon parameters, then salmon will use your raw input data instead of trimming first. Using trimmed data as input is recommended, this is just if you're pre-trimmed with another program! In addition to changing parameters we've specifically enabled, you can modify the extra param to pass any extra parameters.In salmon, both index and quantification steps can accept an extra param. See the Salmon documentation to learn more about the parameters you can pass into salmon . Be sure the modified lines go into the config file you're using to run elvers (see Understanding and Configuring Workflows ).","title":"Modifying Params for Salmon"},{"location":"salmon/#output-files","text":"Your main output directory will be determined by your config file: by default it is BASENAME_out (you specify BASENAME). Salmon will output files in the quant subdirectory of this output directory. Each sample will have its own directory, and the two most interesting files will be the salmon_quant.log and quant.sf files. The former contains the log information from running salmon, and the latter contains the transcript count data. A quant.sf file will look something like this. Name Length EffectiveLength TPM NumReads TRINITY_DN2202_c0_g1_i1 210 39.818 2.683835 2.000000 TRINITY_DN2270_c0_g1_i1 213 41.064 0.000000 0.000000 TRINITY_DN2201_c0_g1_i1 266 69.681 0.766816 1.000000 TRINITY_DN2222_c0_g1_i1 243 55.794 2.873014 3.000000 TRINITY_DN2291_c0_g1_i1 245 56.916 0.000000 0.000000 TRINITY_DN2269_c0_g1_i1 294 89.251 0.000000 0.000000 TRINITY_DN2269_c1_g1_i1 246 57.479 0.000000 0.000000 TRINITY_DN2279_c0_g1_i1 426 207.443 0.000000 0.000000 TRINITY_DN2262_c0_g1_i1 500 280.803 0.190459 1.000912 TRINITY_DN2253_c0_g1_i1 1523 1303.116 0.164015 4.000000 TRINITY_DN2287_c0_g1_i1 467 247.962 0.000000 0.000000 TRINITY_DN2287_c1_g1_i1 325 113.826 0.469425 1.000000 TRINITY_DN2237_c0_g1_i1 306 98.441 0.542788 1.000000 TRINITY_DN2237_c0_g2_i1 307 99.229 0.000000 0.000000 TRINITY_DN2250_c0_g1_i1 368 151.832 0.000000 0.000000 TRINITY_DN2250_c1_g1_i1 271 72.988 0.000000 0.000000 TRINITY_DN2208_c0_g1_i1 379 162.080 1.978014 6.000000 TRINITY_DN2277_c0_g1_i1 269 71.657 0.745677 1.000000 TRINITY_DN2231_c0_g1_i1 209 39.409 0.000000 0.000000 TRINITY_DN2231_c1_g1_i1 334 121.411 0.000000 0.000000 TRINITY_DN2204_c0_g1_i1 287 84.121 0.000000 0.000000","title":"Output files:"},{"location":"salmon/#more-on-salmon","text":"For further reading, on salmon see Intro blog post: http://robpatro.com/blog/?p=248 A 2016 blog post evaluating and comparing methods here Salmon github repo here https://github.com/ngs-docs/2015-nov-adv-rna/blob/master/salmon.rst http://angus.readthedocs.io/en/2016/rob_quant/tut.html https://2016-aug-nonmodel-rnaseq.readthedocs.io/en/latest/quantification.html","title":"More on Salmon"},{"location":"salmon/#advanced-usage-running-salmon-as-a-standalone-rule","text":"You can run salmon as a standalone rule, instead of withing a larger elvers workflow. However, to do this, you need to make sure the input files are available. For salmon, you need both 1) an assembly, and 2) trimmed input files. The assembly can be generated via another workflow, or passed to elvers via the configfile. Specifying an assembly: 1) If you've alread run read trimming and want to use a Trinity assembly generated via elvers , you can run: elvers my_config assemble salmon If you've already run the assembly, elvers will just use this info to locate that assembly. 2) Alternatively, you can input an assembly via the [get_reference](get_reference.md) utility rule: ``` elvers get_reference salmon ``` with an assembly in your `yaml` configfile, e.g.: ``` get_reference: reference: examples/nema.assembly.fasta gene_trans_map: examples/nema.assembly.fasta.gene_trans_map #optional reference_extension: '_input' ``` This is commented out in the test data yaml, but go ahead and uncomment (remove leading `#`) in order to use this option. If you have a gene to transcript map, please specify it as well. If not, delete this line from your `config`. The `assembly_extension` parameter is important: this is what allows us to build assemblies from several different assemblers on the same dataset. Feel free to use `_input`, as specified above, or pick something equally simple yet more informative. **Note: Please don't use additional underscores (`_`) in this extension!**. For more details, see the [get_reference documentation](get_reference.md). Specifying input reads: If you haven't yet run read trimming, you'll also need to run those steps: ``` elvers myconfig get_data trimmomatic salmon ``` Or if you have set `input_trimmomatic_trimmed: False`: ``` elvers myconfig get_data salmon ```","title":"Advanced Usage: Running Salmon as a standalone rule"},{"location":"salmon/#snakemake-rule","text":"We wrote snakemake wrappers to run salmon index and salmon quant . For snakemake afficionados, see the Salmon rule on github .","title":"Snakemake Rule"},{"location":"sourmash/","text":"Sourmash \u00b6 sourmash is a command-line tool and Python library for computing MinHash sketches from DNA sequences, comparing them to each other, and plotting the results. This allows you to estimate sequence similarity between even very large data sets quickly and accurately. Please see the mash software and the mash paper (Ondov et al., 2016) for background information on how and why MinHash sketches work. Sourmash is dib-lab software! Please see the sourmash documentation for more on sourmash. Sourmash 2.0 is coming soon. In the meantime, please cite Brown and Irber, 2016 At the moment we have only enabled sourmash compute functionality. Quickstart \u00b6 Run Sourmash as part of the \"default\" Eel Pond workflow or via the sourmash_compute subworkflow . At the moment, sourmash compute requires both an assembly and a set of reads as input. Please see the sourmash_compute subworkflow for how to run sourmash compute properly. Sourmash Command \u00b6 On the command line, the command elvers runs for each file is approximately: sourmash compute --scaled 1000 \\ -k 31 input_file -o output.sig Output files: \u00b6 Your main output directory will be determined by your config file: by default it is BASENAME_out (you specify BASENAME). Sourmash will output files in the sourmash subdirectory of this output directory. Sourmash signatures will have the same name as the file they're generated from, but end with .sig instead of .fasta or .fq.gz . Modifying Params for Sourmash: \u00b6 Be sure to set up your sample info and build a configfile first (see Understanding and Configuring Workflows ). To see the available parameters for the sourmash rule, run elvers config sourmash --print_params This will print the following: #################### sourmash #################### sourmash: k_size: 31 scaled: 1000 extra: '' ##################################################### In addition to changing parameters we've specifically enabled, you can modify the extra param to pass any extra sourmash parameters, e.g.: extra: ' --track-abundance ' Be sure the modified lines go into the config file you're using to run elvers (see Understanding and Configuring Workflows ). See the sourmash documentation to learn more about the parameters you can use with sourmash compute. Sourmash elvers rule \u00b6 We use a slightly modified version of the sourmash snakemake wrapper to run Sourmash compute via snakemake. For snakemake afficionados, see our sourmash rules on github .","title":"sourmash"},{"location":"sourmash/#sourmash","text":"sourmash is a command-line tool and Python library for computing MinHash sketches from DNA sequences, comparing them to each other, and plotting the results. This allows you to estimate sequence similarity between even very large data sets quickly and accurately. Please see the mash software and the mash paper (Ondov et al., 2016) for background information on how and why MinHash sketches work. Sourmash is dib-lab software! Please see the sourmash documentation for more on sourmash. Sourmash 2.0 is coming soon. In the meantime, please cite Brown and Irber, 2016 At the moment we have only enabled sourmash compute functionality.","title":"Sourmash"},{"location":"sourmash/#quickstart","text":"Run Sourmash as part of the \"default\" Eel Pond workflow or via the sourmash_compute subworkflow . At the moment, sourmash compute requires both an assembly and a set of reads as input. Please see the sourmash_compute subworkflow for how to run sourmash compute properly.","title":"Quickstart"},{"location":"sourmash/#sourmash-command","text":"On the command line, the command elvers runs for each file is approximately: sourmash compute --scaled 1000 \\ -k 31 input_file -o output.sig","title":"Sourmash Command"},{"location":"sourmash/#output-files","text":"Your main output directory will be determined by your config file: by default it is BASENAME_out (you specify BASENAME). Sourmash will output files in the sourmash subdirectory of this output directory. Sourmash signatures will have the same name as the file they're generated from, but end with .sig instead of .fasta or .fq.gz .","title":"Output files:"},{"location":"sourmash/#modifying-params-for-sourmash","text":"Be sure to set up your sample info and build a configfile first (see Understanding and Configuring Workflows ). To see the available parameters for the sourmash rule, run elvers config sourmash --print_params This will print the following: #################### sourmash #################### sourmash: k_size: 31 scaled: 1000 extra: '' ##################################################### In addition to changing parameters we've specifically enabled, you can modify the extra param to pass any extra sourmash parameters, e.g.: extra: ' --track-abundance ' Be sure the modified lines go into the config file you're using to run elvers (see Understanding and Configuring Workflows ). See the sourmash documentation to learn more about the parameters you can use with sourmash compute.","title":"Modifying Params for Sourmash:"},{"location":"sourmash/#sourmash-elvers-rule","text":"We use a slightly modified version of the sourmash snakemake wrapper to run Sourmash compute via snakemake. For snakemake afficionados, see our sourmash rules on github .","title":"Sourmash elvers rule"},{"location":"sourmash_compute/","text":"Sourmash_Compute Subworkflow \u00b6 Subworkflows combine tools in the right order to facilitate file targeting withing elvers . The \"sourmash_compute\" subworkflow conducts read quality trimming and kmer trimming prior to sourmash compute of the kmer-trimmed files. It currently also computes sourmash signatures for an assembly as well, which needs to be provided, either by running an assembly or providing one in your configfile. At the moment, this workflow consists of: get_data - an elvers utility trimmomatic sourmash Quickstart \u00b6 If you've generated an assembly, even if you've already run elvers examples/nema.yaml assemble : 1) \"Run\" trinity assembly at the same time. If you've already run the assembly, elvers will just locate your assembly file for sourmash_compute . elvers examples/nema.yaml assemble sourmash_compute 2) OR, Pass an assembly in via get_reference , with an assembly specified in your config file. elvers get_reference sourmash_compute In the configfile: get_reference: reference: examples/nema.assembly.fasta gene_trans_map: examples/nema.assembly.fasta.gene_trans_map #optional reference_extension: '_input' This is commented out in the test data yaml, but go ahead and uncomment (remove leading # ) in order to use this option. If you have a gene to transcript map, please specify it as well. If not, delete this line from your config . The assembly_extension parameter is important: this is what allows us to build assemblies from several different assemblers on the same dataset. Feel free to use _input , as specified above, or pick something equally simple yet more informative. Note: Please don't use additional underscores ( _ ) in this extension! . For more details, see the get_reference documentation . Configuring the sourmash_compute subworkflow \u00b6 To set up your sample info and build a configfile, see Understanding and Configuring Workflows . If you want to add the sourmash_compute program parameters to a previously built configfile, run: elvers config.yaml sourmash_compute --print_params A small set of parameters should print to your console: #################### sourmash_compute #################### get_data: download_data: false use_ftp: false trimmomatic: adapter_file: pe_path: ep_utils/TruSeq3-PE-2.fa se_path: ep_utils/TruSeq3-SE.fa extra: '' trim_cmd: ILLUMINACLIP:{}:2:40:15 LEADING:2 TRAILING:2 SLIDINGWINDOW:4:15 MINLEN:25 sourmash: k_size: 31 scaled: 1000 extra: '' ####################################################### Override default params for any program by placing these lines in your yaml config file, and modifying values as desired. For more details, see Understanding and Configuring Workflows .For more on what parameters are available, see the docs for each specific program or utility rule: get_data trimmomatic sourmash","title":"sourmash_compute"},{"location":"sourmash_compute/#sourmash_compute-subworkflow","text":"Subworkflows combine tools in the right order to facilitate file targeting withing elvers . The \"sourmash_compute\" subworkflow conducts read quality trimming and kmer trimming prior to sourmash compute of the kmer-trimmed files. It currently also computes sourmash signatures for an assembly as well, which needs to be provided, either by running an assembly or providing one in your configfile. At the moment, this workflow consists of: get_data - an elvers utility trimmomatic sourmash","title":"Sourmash_Compute Subworkflow"},{"location":"sourmash_compute/#quickstart","text":"If you've generated an assembly, even if you've already run elvers examples/nema.yaml assemble : 1) \"Run\" trinity assembly at the same time. If you've already run the assembly, elvers will just locate your assembly file for sourmash_compute . elvers examples/nema.yaml assemble sourmash_compute 2) OR, Pass an assembly in via get_reference , with an assembly specified in your config file. elvers get_reference sourmash_compute In the configfile: get_reference: reference: examples/nema.assembly.fasta gene_trans_map: examples/nema.assembly.fasta.gene_trans_map #optional reference_extension: '_input' This is commented out in the test data yaml, but go ahead and uncomment (remove leading # ) in order to use this option. If you have a gene to transcript map, please specify it as well. If not, delete this line from your config . The assembly_extension parameter is important: this is what allows us to build assemblies from several different assemblers on the same dataset. Feel free to use _input , as specified above, or pick something equally simple yet more informative. Note: Please don't use additional underscores ( _ ) in this extension! . For more details, see the get_reference documentation .","title":"Quickstart"},{"location":"sourmash_compute/#configuring-the-sourmash_compute-subworkflow","text":"To set up your sample info and build a configfile, see Understanding and Configuring Workflows . If you want to add the sourmash_compute program parameters to a previously built configfile, run: elvers config.yaml sourmash_compute --print_params A small set of parameters should print to your console: #################### sourmash_compute #################### get_data: download_data: false use_ftp: false trimmomatic: adapter_file: pe_path: ep_utils/TruSeq3-PE-2.fa se_path: ep_utils/TruSeq3-SE.fa extra: '' trim_cmd: ILLUMINACLIP:{}:2:40:15 LEADING:2 TRAILING:2 SLIDINGWINDOW:4:15 MINLEN:25 sourmash: k_size: 31 scaled: 1000 extra: '' ####################################################### Override default params for any program by placing these lines in your yaml config file, and modifying values as desired. For more details, see Understanding and Configuring Workflows .For more on what parameters are available, see the docs for each specific program or utility rule: get_data trimmomatic sourmash","title":"Configuring the sourmash_compute subworkflow"},{"location":"trimmomatic/","text":"Trimmomatic \u00b6 We use Trimmomatic (version 0.36) to trim off residual Illumina adapters that were left behind after demultiplexing. Here we use a set TruSeq Illumina adapters. However, if running this on your own data and you know you have different adapters, you'll want to input them in the configfile (see params section, below). If you're using the right adapters, you should see that some of the reads are trimmed; if they\u2019re not, you won\u2019t see anything get trimmed. See this excellent paper on trimming parameters by MacManes 2014 . Trimmomatic Command \u00b6 Based on recommendations from MacManes 2014 , we use this command by default: TrimmomaticPE ${base}.fastq.gz ${baseR2}.fastq.gz \\ ${base}.qc.fq.gz s1_se \\ ${baseR2}.qc.fq.gz s2_se \\ ILLUMINACLIP:TruSeq3-PE.fa:2:40:15 \\ LEADING:2 TRAILING:2 \\ SLIDINGWINDOW:4:15 \\ MINLEN:25 However, the trimming command can be extensively modified via the configfile. Here's how the parameters for the command above look in our config: trimmomatic: trim_cmd: \"ILLUMINACLIP:{}:2:40:15 LEADING:2 TRAILING:2 SLIDINGWINDOW:4:15 MINLEN:25\" extra: '' Quickstart \u00b6 Run Trimmomatic via the \"default\" Eel Pond workflow or via the preprocess subworkflow . To run Trimmomatic as a standalone program, see \"Advanced Usage\" section below. Output files: \u00b6 Your main output directory will be determined by your config file: by default it is BASENAME_out (you specify BASENAME). Trimmomatic will output files in the preprocess subdirectory of this output directory. All outputs will contain *.trim.fq.gz . Modifying Params for Trimmomatic: \u00b6 Be sure to set up your sample info and build a configfile first (see Understanding and Configuring Workflows ). To see the available parameters for the trimmomatic rule, run elvers config trimmomatic --print_params In here, you'll see a section for \"trimmomatic\" parameters that looks like this: #################### trimmomatic #################### trimmomatic: adapter_file: pe_path: ep_utils/TruSeq3-PE-2.fa se_path: ep_utils/TruSeq3-SE.fa trim_cmd: ILLUMINACLIP:{}:2:40:15 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35 extra: '' ##################################################### Override default params by modifying these lines. In addition to changing parameters we've specifically enabled, you can modify the \"extra\" param to pass any extra trimmomatic parameters, e.g.: extra: 'HEADCROP:5' # to remove the first 5 bases at the front of the read. See the Trimmomatic documentation for parameters and options you can pass to Trimmomatic. Be sure the modified lines go into the config file you're using to run elvers (see Understanding and Configuring Workflows ). Advanced Usage: Running Trimmomatic as a standalone rule \u00b6 You can run trimmomatic as a standalone rule, instead of withing a larger elvers workflow. However, to do this, you need to make sure the input files are available. For Trimmomatic, the input files are your input data - either downloaded or linked into the input_data directory via get_data . If you've already done this, you can run: elvers my_config trimmomatic If not, you can run both at once to make sure trimmomatic can find its input files: elvers my_config get_data trimmomatic Snakemake Rule \u00b6 We use a local copies of the trimmomatic snakemake wrappers to run Trimmomatic. For snakemake afficionados, see the trimmomatic rules on github .","title":"trimmomatic"},{"location":"trimmomatic/#trimmomatic","text":"We use Trimmomatic (version 0.36) to trim off residual Illumina adapters that were left behind after demultiplexing. Here we use a set TruSeq Illumina adapters. However, if running this on your own data and you know you have different adapters, you'll want to input them in the configfile (see params section, below). If you're using the right adapters, you should see that some of the reads are trimmed; if they\u2019re not, you won\u2019t see anything get trimmed. See this excellent paper on trimming parameters by MacManes 2014 .","title":"Trimmomatic"},{"location":"trimmomatic/#trimmomatic-command","text":"Based on recommendations from MacManes 2014 , we use this command by default: TrimmomaticPE ${base}.fastq.gz ${baseR2}.fastq.gz \\ ${base}.qc.fq.gz s1_se \\ ${baseR2}.qc.fq.gz s2_se \\ ILLUMINACLIP:TruSeq3-PE.fa:2:40:15 \\ LEADING:2 TRAILING:2 \\ SLIDINGWINDOW:4:15 \\ MINLEN:25 However, the trimming command can be extensively modified via the configfile. Here's how the parameters for the command above look in our config: trimmomatic: trim_cmd: \"ILLUMINACLIP:{}:2:40:15 LEADING:2 TRAILING:2 SLIDINGWINDOW:4:15 MINLEN:25\" extra: ''","title":"Trimmomatic Command"},{"location":"trimmomatic/#quickstart","text":"Run Trimmomatic via the \"default\" Eel Pond workflow or via the preprocess subworkflow . To run Trimmomatic as a standalone program, see \"Advanced Usage\" section below.","title":"Quickstart"},{"location":"trimmomatic/#output-files","text":"Your main output directory will be determined by your config file: by default it is BASENAME_out (you specify BASENAME). Trimmomatic will output files in the preprocess subdirectory of this output directory. All outputs will contain *.trim.fq.gz .","title":"Output files:"},{"location":"trimmomatic/#modifying-params-for-trimmomatic","text":"Be sure to set up your sample info and build a configfile first (see Understanding and Configuring Workflows ). To see the available parameters for the trimmomatic rule, run elvers config trimmomatic --print_params In here, you'll see a section for \"trimmomatic\" parameters that looks like this: #################### trimmomatic #################### trimmomatic: adapter_file: pe_path: ep_utils/TruSeq3-PE-2.fa se_path: ep_utils/TruSeq3-SE.fa trim_cmd: ILLUMINACLIP:{}:2:40:15 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35 extra: '' ##################################################### Override default params by modifying these lines. In addition to changing parameters we've specifically enabled, you can modify the \"extra\" param to pass any extra trimmomatic parameters, e.g.: extra: 'HEADCROP:5' # to remove the first 5 bases at the front of the read. See the Trimmomatic documentation for parameters and options you can pass to Trimmomatic. Be sure the modified lines go into the config file you're using to run elvers (see Understanding and Configuring Workflows ).","title":"Modifying Params for Trimmomatic:"},{"location":"trimmomatic/#advanced-usage-running-trimmomatic-as-a-standalone-rule","text":"You can run trimmomatic as a standalone rule, instead of withing a larger elvers workflow. However, to do this, you need to make sure the input files are available. For Trimmomatic, the input files are your input data - either downloaded or linked into the input_data directory via get_data . If you've already done this, you can run: elvers my_config trimmomatic If not, you can run both at once to make sure trimmomatic can find its input files: elvers my_config get_data trimmomatic","title":"Advanced Usage: Running Trimmomatic as a standalone rule"},{"location":"trimmomatic/#snakemake-rule","text":"We use a local copies of the trimmomatic snakemake wrappers to run Trimmomatic. For snakemake afficionados, see the trimmomatic rules on github .","title":"Snakemake Rule"},{"location":"trinity/","text":"Trinity \u00b6 The Eel Pond protocol uses the Trinity de novo transcriptome assembler to take short, trimmed/diginorm Illumina reads data and assemble (predict) full-length transcripts into a single fasta file output. Each contig in the fasta assembly file represents one unique transcript. Trinity is a single-ksize assembler, with a default of k = 25. We recommend using kmer-trimmed reads (output of khmer) as input into Triniity to reduce dataset complexity without losing valuable kmers. The resulting output assembly fasta file can then be used to align the trimmed (not diginorm) short Illumina reads and quantify expression per transcript. Note, the current version of Trininty (after 2.3.2) is configured to diginorm the input reads before assembly begins. Since we have already applied diginorm to our reads, the result will be a negligible decrease in read counts prior to the assembly. We provide options to disable this digital normalization via the config file, but applying diginorm twice is not really a problem. For data sets with large numbers of reads, applying diginorm as a separate step as we have via khmer may decrease the memory requirements needed by the Trinity pipeline. The ID for each transcript is output (version 2.2.0 to current) as follows, where the TRINITY is constant, the DN2202 is an example of a variable contig/transcript ID, c stands for component, g gene and i isoform: TRINITY_DN2202_c0_g1_i1 Trinity Command \u00b6 On the command line, the command elvers runs is approximately: Trinity --left left.fq \\ --right right.fq --seqType fq --max_memory 10G \\ --CPU 4 But we highly recommend you modify max_memory and CPU to fit your data and compute resources. Quickstart \u00b6 Run Trinity via the \"default\" Eel Pond workflow or via the assemble subworkflow . To run Trinity as a standalone program, see \"Advanced Usage\" section below. Output files: \u00b6 Your main output directory will be determined by your config file: by default it is BASENAME_out (you specify BASENAME). Trinity will output files in the assembly subdirectory of this output directory. The fasta file will be BASENAME_trinity.fasta and the gene-trans map will be BASENAME_trinity.fasta.gene_trans_map . Modifying Params for Trinity: \u00b6 Be sure to set up your sample info and build a configfile first (see Understanding and Configuring Workflows ). To see the available parameters for the trinity rule, run elvers config trinity --print_params This will print the following: #################### trinity #################### trinity: input_kmer_trimmed: true input_trimmomatic_trimmed: false add_single_to_paired: false # would you like to add the orphaned reads to the trinity assembly? max_memory: 30G seqtype: fq extra: '' ##################################################### In addition to changing parameters we've specifically enabled, you can modify the extra param to pass any extra trinity parameters, e.g.: extra: '--no_normalize_reads' # to turn off Trinity's digital normalization steps Within the \"default\" Eel Pond workflow or the assemble subworkflow , these options enable you to choose kmer-trimmed, quality-trimmed, or raw sequencing data as input. We recommend using kmer-trimmed reads as input. If both input_kmer_trimmed and input_trimmomatic_trimmed are False , we will just use raw reads from the samples.tsv file. See the Trinity documentation to learn more about these parameters. Be sure the modified lines go into the config file you're using to run elvers (see Understanding and Configuring Workflows ). Advanced Usage: Running Trinity as a standalone rule \u00b6 You can run trinity as a standalone rule, instead of withing a larger elvers workflow. However, to do this, you need to make sure the input files are available. For trinity, the default input files are kmer-trimmed input data (e.g. output of khmer). If you've already done this, you can run: elvers my_config trinity If not, you can run the prior steps at the same time to make sure khmer can find these input files: elvers my_config get_data trimmomatic khmer trinity Snakemake rule \u00b6 We wrote a Trinity snakemake wrapper to run Trinity. For snakemake afficionados, see the Trinity rule on github .","title":"trinity"},{"location":"trinity/#trinity","text":"The Eel Pond protocol uses the Trinity de novo transcriptome assembler to take short, trimmed/diginorm Illumina reads data and assemble (predict) full-length transcripts into a single fasta file output. Each contig in the fasta assembly file represents one unique transcript. Trinity is a single-ksize assembler, with a default of k = 25. We recommend using kmer-trimmed reads (output of khmer) as input into Triniity to reduce dataset complexity without losing valuable kmers. The resulting output assembly fasta file can then be used to align the trimmed (not diginorm) short Illumina reads and quantify expression per transcript. Note, the current version of Trininty (after 2.3.2) is configured to diginorm the input reads before assembly begins. Since we have already applied diginorm to our reads, the result will be a negligible decrease in read counts prior to the assembly. We provide options to disable this digital normalization via the config file, but applying diginorm twice is not really a problem. For data sets with large numbers of reads, applying diginorm as a separate step as we have via khmer may decrease the memory requirements needed by the Trinity pipeline. The ID for each transcript is output (version 2.2.0 to current) as follows, where the TRINITY is constant, the DN2202 is an example of a variable contig/transcript ID, c stands for component, g gene and i isoform: TRINITY_DN2202_c0_g1_i1","title":"Trinity"},{"location":"trinity/#trinity-command","text":"On the command line, the command elvers runs is approximately: Trinity --left left.fq \\ --right right.fq --seqType fq --max_memory 10G \\ --CPU 4 But we highly recommend you modify max_memory and CPU to fit your data and compute resources.","title":"Trinity Command"},{"location":"trinity/#quickstart","text":"Run Trinity via the \"default\" Eel Pond workflow or via the assemble subworkflow . To run Trinity as a standalone program, see \"Advanced Usage\" section below.","title":"Quickstart"},{"location":"trinity/#output-files","text":"Your main output directory will be determined by your config file: by default it is BASENAME_out (you specify BASENAME). Trinity will output files in the assembly subdirectory of this output directory. The fasta file will be BASENAME_trinity.fasta and the gene-trans map will be BASENAME_trinity.fasta.gene_trans_map .","title":"Output files:"},{"location":"trinity/#modifying-params-for-trinity","text":"Be sure to set up your sample info and build a configfile first (see Understanding and Configuring Workflows ). To see the available parameters for the trinity rule, run elvers config trinity --print_params This will print the following: #################### trinity #################### trinity: input_kmer_trimmed: true input_trimmomatic_trimmed: false add_single_to_paired: false # would you like to add the orphaned reads to the trinity assembly? max_memory: 30G seqtype: fq extra: '' ##################################################### In addition to changing parameters we've specifically enabled, you can modify the extra param to pass any extra trinity parameters, e.g.: extra: '--no_normalize_reads' # to turn off Trinity's digital normalization steps Within the \"default\" Eel Pond workflow or the assemble subworkflow , these options enable you to choose kmer-trimmed, quality-trimmed, or raw sequencing data as input. We recommend using kmer-trimmed reads as input. If both input_kmer_trimmed and input_trimmomatic_trimmed are False , we will just use raw reads from the samples.tsv file. See the Trinity documentation to learn more about these parameters. Be sure the modified lines go into the config file you're using to run elvers (see Understanding and Configuring Workflows ).","title":"Modifying Params for Trinity:"},{"location":"trinity/#advanced-usage-running-trinity-as-a-standalone-rule","text":"You can run trinity as a standalone rule, instead of withing a larger elvers workflow. However, to do this, you need to make sure the input files are available. For trinity, the default input files are kmer-trimmed input data (e.g. output of khmer). If you've already done this, you can run: elvers my_config trinity If not, you can run the prior steps at the same time to make sure khmer can find these input files: elvers my_config get_data trimmomatic khmer trinity","title":"Advanced Usage: Running Trinity as a standalone rule"},{"location":"trinity/#snakemake-rule","text":"We wrote a Trinity snakemake wrapper to run Trinity. For snakemake afficionados, see the Trinity rule on github .","title":"Snakemake rule"},{"location":"under_the_hood/","text":"Under the Hood \u00b6 elvers runs snakemake rules in workflows that are highly customizeable. The user chooses a tool or workflow, and elvers aggregates all of the default parameters for that tool or workflow, build end-stage target files, and passes this information into the snakefile. Snakemake then builds a DAG of the workflow and runs the workflow in an automated manner. Program Rules \u00b6 The unit at the heart of all workflows is a standalone snakemake rule for a program function. Rules are all placed within subdirectories of the rules directory. For example, the trinity rule is found at rules/trinity/trinity.rule . Utility rules are instead found at, for example: rules/utils/get_data.rule . Each rule needs several files: rulename.rule : the snakemake rule(s) for this program. rulename-env.yaml : a conda environment file for this program. This is passed into the conda snakemake directive in the rule. (optional) rulename-wrapper.py or rulename-script.R : a script that is used to more easily and reproducibly run the program. rulename_params.yaml : a parameter file to direct program output and set default program parameters The last file ( _params ) provides program defaults that can be overridden by user input from the main config file (via a nested dictionary update). The parameters do need to be exact, however. The --print_params and --build_config options in elvers are designed to facilitate this. Rule Params \u00b6 The rule params files (above) have two components: elvers_params and program_params . program_params are exposed to the user when displaying parameters or building configfiles. These are relatively safe to modify, and most get passed into the program rule itself. On the other hand, elvers_params are used internally to build the names of output files when they need to be passed to the Snakefile as \"targets\". These are never exposed to the user and should be set just once when a rule is built. For example, let's look at the trinity.yaml file: trinity: elvers_params: outdir: assembly extensions: assembly_extensions: # use this extension only for all output of this assembler - _trinity base: - .fasta - .fasta.gene_trans_map program_params: # input kmer-trimmed reads input_kmer_trimmed: True # input trimmed-reads input_trimmomatic_trimmed: False # do we want to assemble the single reads with pe reads? add_single_to_paired: False max_memory : 30G seqtype: fq extra: '' The program_params allow the user to choose quality-trimmed or kmer-trimmed reads as input to trinity, and pass paramters for memory and sequence type. Whereever possible, program params also include an extra parameter that enable the user to pass any number of parameters directly to the command line of that program. The elvers_params specify that trinity produces two files, with the assembly_extension '_trinity'. These will be BASENAME_trinity.fasta and BASENAME_trinity.fasta.gene_trans_map . The assembly_extension allows us to have downstream steps that work on different assemblies, but trinity is an assembler, and should only ever produce an assembly with the extension _trinity . Assembly extensions are used downstream as well. For example, paladin only works on protein assemblies, and thus will only map to assemblies with the extension _plass . Assembly extensions are the most important of this type of parameter, but the differential expression steps also have a weird paramter they pass in: contrast . Target files are generated in ep_utils/generate_all_targets.py - have a look if you'd like to see how we build targets to pass into the Snakefile, and look for handling assembly_extensions and contrasts . Workflows \u00b6 Rules are aggregated into Subworkflows and Workflows that include all the rules necessary to run a larger analysis. For example, the default eel pond protocol conducts de novo transcriptome assembly, annotation, and quick differential expression analysis. For now, workflows are specified in the ep_utils/pipeline_defaults.yaml . Each workflow (or subworkflow, which are smaller subsets of the workflows) needs two pieces of information: include : rules to include targets : the endpoints of workflows that are not used to generate additional files. These are the \"targets\" we build and pass to the Snakefile.","title":"Under the Hood"},{"location":"under_the_hood/#under-the-hood","text":"elvers runs snakemake rules in workflows that are highly customizeable. The user chooses a tool or workflow, and elvers aggregates all of the default parameters for that tool or workflow, build end-stage target files, and passes this information into the snakefile. Snakemake then builds a DAG of the workflow and runs the workflow in an automated manner.","title":"Under the Hood"},{"location":"under_the_hood/#program-rules","text":"The unit at the heart of all workflows is a standalone snakemake rule for a program function. Rules are all placed within subdirectories of the rules directory. For example, the trinity rule is found at rules/trinity/trinity.rule . Utility rules are instead found at, for example: rules/utils/get_data.rule . Each rule needs several files: rulename.rule : the snakemake rule(s) for this program. rulename-env.yaml : a conda environment file for this program. This is passed into the conda snakemake directive in the rule. (optional) rulename-wrapper.py or rulename-script.R : a script that is used to more easily and reproducibly run the program. rulename_params.yaml : a parameter file to direct program output and set default program parameters The last file ( _params ) provides program defaults that can be overridden by user input from the main config file (via a nested dictionary update). The parameters do need to be exact, however. The --print_params and --build_config options in elvers are designed to facilitate this.","title":"Program Rules"},{"location":"under_the_hood/#rule-params","text":"The rule params files (above) have two components: elvers_params and program_params . program_params are exposed to the user when displaying parameters or building configfiles. These are relatively safe to modify, and most get passed into the program rule itself. On the other hand, elvers_params are used internally to build the names of output files when they need to be passed to the Snakefile as \"targets\". These are never exposed to the user and should be set just once when a rule is built. For example, let's look at the trinity.yaml file: trinity: elvers_params: outdir: assembly extensions: assembly_extensions: # use this extension only for all output of this assembler - _trinity base: - .fasta - .fasta.gene_trans_map program_params: # input kmer-trimmed reads input_kmer_trimmed: True # input trimmed-reads input_trimmomatic_trimmed: False # do we want to assemble the single reads with pe reads? add_single_to_paired: False max_memory : 30G seqtype: fq extra: '' The program_params allow the user to choose quality-trimmed or kmer-trimmed reads as input to trinity, and pass paramters for memory and sequence type. Whereever possible, program params also include an extra parameter that enable the user to pass any number of parameters directly to the command line of that program. The elvers_params specify that trinity produces two files, with the assembly_extension '_trinity'. These will be BASENAME_trinity.fasta and BASENAME_trinity.fasta.gene_trans_map . The assembly_extension allows us to have downstream steps that work on different assemblies, but trinity is an assembler, and should only ever produce an assembly with the extension _trinity . Assembly extensions are used downstream as well. For example, paladin only works on protein assemblies, and thus will only map to assemblies with the extension _plass . Assembly extensions are the most important of this type of parameter, but the differential expression steps also have a weird paramter they pass in: contrast . Target files are generated in ep_utils/generate_all_targets.py - have a look if you'd like to see how we build targets to pass into the Snakefile, and look for handling assembly_extensions and contrasts .","title":"Rule Params"},{"location":"under_the_hood/#workflows","text":"Rules are aggregated into Subworkflows and Workflows that include all the rules necessary to run a larger analysis. For example, the default eel pond protocol conducts de novo transcriptome assembly, annotation, and quick differential expression analysis. For now, workflows are specified in the ep_utils/pipeline_defaults.yaml . Each workflow (or subworkflow, which are smaller subsets of the workflows) needs two pieces of information: include : rules to include targets : the endpoints of workflows that are not used to generate additional files. These are the \"targets\" we build and pass to the Snakefile.","title":"Workflows"},{"location":"workflows/","text":"Workflows and Tools \u00b6 Available Workflows \u00b6 To see the available workflows, run: elvers examples/nema.yaml --print_workflows You should see something like this: #################### Available Eelpond Workflows #################### default: get_data trimmomatic khmer trinity fastqc dammit salmon sourmash protein_assembly: get_data trimmomatic khmer plass pear fastqc paladin sourmash preprocess: get_data fastqc trimmomatic kmer_trim: get_data trimmomatic khmer assemble: get_data trimmomatic khmer trinity annotate: dammit quantify: get_data trimmomatic salmon diffexp: get_data trimmomatic salmon deseq2 sourmash_compute: get_data trimmomatic khmer sourmash plass_assemble: get_data trimmomatic khmer plass paladin_map: get_data trimmomatic khmer plass pear paladin correct_reads: get_data trimmomatic rcorrector Available Tools \u00b6 To see the available integrated programs, run: elvers examples/nema.yaml --print_rules You should see something like this: #################### Advanced usage: all available ruless #################### get_data trimmomatic fastqc rcorrector khmer trinity salmon deseq2 dammit sourmash get_reference plass pear paladin","title":"Workflows and Tools"},{"location":"workflows/#workflows-and-tools","text":"","title":"Workflows and Tools"},{"location":"workflows/#available-workflows","text":"To see the available workflows, run: elvers examples/nema.yaml --print_workflows You should see something like this: #################### Available Eelpond Workflows #################### default: get_data trimmomatic khmer trinity fastqc dammit salmon sourmash protein_assembly: get_data trimmomatic khmer plass pear fastqc paladin sourmash preprocess: get_data fastqc trimmomatic kmer_trim: get_data trimmomatic khmer assemble: get_data trimmomatic khmer trinity annotate: dammit quantify: get_data trimmomatic salmon diffexp: get_data trimmomatic salmon deseq2 sourmash_compute: get_data trimmomatic khmer sourmash plass_assemble: get_data trimmomatic khmer plass paladin_map: get_data trimmomatic khmer plass pear paladin correct_reads: get_data trimmomatic rcorrector","title":"Available Workflows"},{"location":"workflows/#available-tools","text":"To see the available integrated programs, run: elvers examples/nema.yaml --print_rules You should see something like this: #################### Advanced usage: all available ruless #################### get_data trimmomatic fastqc rcorrector khmer trinity salmon deseq2 dammit sourmash get_reference plass pear paladin","title":"Available Tools"},{"location":"old_docs/Annotation/","text":"Annotating de novo transcriptomes with dammit \u00b6 dammit! dammit is an annotation pipeline written by Camille Scott . dammit runs a relatively standard annotation protocol for transcriptomes: it begins by building gene models with Transdecoder , and then uses the following protein databases as evidence for annotation: Pfam-A , Rfam , OrthoDB , uniref90 (uniref is optional with --full ). If a protein dataset is available, this can also be supplied to the dammit pipeline with --user-databases as optional evidence for annotation. In addition, BUSCO v3 is run, which will compare the gene content in your transcriptome with a lineage-specific data set. The output is a proportion of your transcriptome that matches with the data set, which can be used as an estimate of the completeness of your transcriptome based on evolutionary expectation ( Simho et al. 2015 ). There are several lineage-specific datasets available from the authors of BUSCO. We will use the metazoa dataset for this transcriptome. Database Preparation \u00b6 dammit has two major subcommands: dammit databases and dammit annotate . databases checks that the databases are installed and prepared, and if run with the --install flag, will perform that installation and preparation. If you just run dammit databases on its own, you should get a notification that some database tasks are not up-to-date -- we need to install them! Unless we're running short on time, we're going to do a full run. If you want to run a quick version of the pipeline, add a parameter, --quick , to omit OrthoDB, Uniref, Pfam, and Rfam. A \"full\" run will take longer to install and run, but you'll have access to the full annotation pipeline. dammit databases --install --busco-group metazoa # --quick We used the \"metazoa\" BUSCO group. We can use any of the BUSCO databases, so long as we install them with the dammit databases subcommand. You can see the whole list by running dammit databases -h . You should try to match your species as closely as possible for the best results. If we want to install another, for example: dammit databases --install --busco-group fungi # --quick Note: if you have limited space on your instance, you can also install these databases in a different location (e.g. on an external volume). You would want to run this command before running the database installs we just ran. #Run ONLY if you want to install databases in different location. #To run, remove the `#` from the front of the following command: # dammit databases --database-dir /path/to/databases Annotation \u00b6 Now we'll download a custom Nematostella vectensis protein database available from JGI. Here, somebody has already created a proper database for us [1] (it has a reference proteome available through uniprot). If your critter is a non-model organism, you will likely need to create your own with proteins from closely-related species. This will rely on your knowledge of your system! Run the command: dammit annotate trinity.nema.fasta --busco-group metazoa --user-databases nema.reference.prot.faa --n_threads 6 # --quick While dammit runs, it will print out which tasks its running to the terminal. dammit is written with a library called pydoit , which is a python workflow library similar to GNU Make. This not only helps organize the underlying workflow, but also means that if we interrupt it, it will properly resume! After a successful run, you'll have a new directory called trinity.nema.fasta.dammit . If you look inside, you'll see a lot of files: ls trinity.nema.fasta.dammit/ annotate.doit.db trinity.nema.fasta.dammit.namemap.csv trinity.nema.fasta.transdecoder.pep dammit.log trinity.nema.fasta.dammit.stats.json trinity.nema.fasta.x.nema.reference.prot.faa.crbl.csv run_trinity.nema.fasta.metazoa.busco.results trinity.nema.fasta.transdecoder.bed trinity.nema.fasta.x.nema.reference.prot.faa.crbl.gff3 tmp trinity.nema.fasta.transdecoder.cds trinity.nema.fasta.x.nema.reference.prot.faa.crbl.model.csv trinity.nema.fasta trinity.nema.fasta.transdecoder_dir trinity.nema.fasta.x.nema.reference.prot.faa.crbl.model.plot.pdf trinity.nema.fasta.dammit.fasta trinity.nema.fasta.transdecoder.gff3 trinity.nema.fasta.dammit.gff3 trinity.nema.fasta.transdecoder.mRNA The most important files for you are trinity.nema.fasta.dammit.fasta , trinity.nema.fasta.dammit.gff3 , and trinity.nema.fasta.dammit.stats.json . If the above dammit command is run again, there will be a message: **Pipeline is already completed!** Parse dammit output \u00b6 Cammille wrote dammit in Python, which includes a library to parse gff3 dammit output. To send this output to a useful table, we will need to open the Python environemnt. cd trinity.nema.fasta.dammit python Then, using this script, will output a list of gene ID: import pandas as pd from dammit.fileio.gff3 import GFF3Parser gff_file = \"trinity.nema.fasta.dammit.gff3\" annotations = GFF3Parser(filename=gff_file).read() names = annotations.sort_values(by=['seqid', 'score'], ascending=True).query('score < 1e-05').drop_duplicates(subset='seqid')[['seqid', 'Name']] new_file = names.dropna(axis=0,how='all') new_file.head() new_file.to_csv(\"nema_gene_name_id.csv\") exit() This will output a table of genes with 'seqid' and 'Name' in a .csv file: nema_gene_name_id.csv . Let's take a look at that file: less nema_gene_name_id.csv Notice there are multiple transcripts per gene model prediction. This .csv file can be used in tximport in downstream DE analysis. References \u00b6 Putnam NH, Srivastava M, Hellsten U, Dirks B, Chapman J, Salamov A, Terry A, Shapiro H, Lindquist E, Kapitonov VV, Jurka J, Genikhovich G, Grigoriev IV, Lucas SM, Steele RE, Finnerty JR, Technau U, Martindale MQ, Rokhsar DS. (2007) Sea anemone genome reveals ancestral eumetazoan gene repertoire and genomic organization. Science. 317, 86-94.","title":"Annotating de novo transcriptomes with dammit"},{"location":"old_docs/Annotation/#annotating-de-novo-transcriptomes-with-dammit","text":"dammit! dammit is an annotation pipeline written by Camille Scott . dammit runs a relatively standard annotation protocol for transcriptomes: it begins by building gene models with Transdecoder , and then uses the following protein databases as evidence for annotation: Pfam-A , Rfam , OrthoDB , uniref90 (uniref is optional with --full ). If a protein dataset is available, this can also be supplied to the dammit pipeline with --user-databases as optional evidence for annotation. In addition, BUSCO v3 is run, which will compare the gene content in your transcriptome with a lineage-specific data set. The output is a proportion of your transcriptome that matches with the data set, which can be used as an estimate of the completeness of your transcriptome based on evolutionary expectation ( Simho et al. 2015 ). There are several lineage-specific datasets available from the authors of BUSCO. We will use the metazoa dataset for this transcriptome.","title":"Annotating de novo transcriptomes with dammit"},{"location":"old_docs/Annotation/#database-preparation","text":"dammit has two major subcommands: dammit databases and dammit annotate . databases checks that the databases are installed and prepared, and if run with the --install flag, will perform that installation and preparation. If you just run dammit databases on its own, you should get a notification that some database tasks are not up-to-date -- we need to install them! Unless we're running short on time, we're going to do a full run. If you want to run a quick version of the pipeline, add a parameter, --quick , to omit OrthoDB, Uniref, Pfam, and Rfam. A \"full\" run will take longer to install and run, but you'll have access to the full annotation pipeline. dammit databases --install --busco-group metazoa # --quick We used the \"metazoa\" BUSCO group. We can use any of the BUSCO databases, so long as we install them with the dammit databases subcommand. You can see the whole list by running dammit databases -h . You should try to match your species as closely as possible for the best results. If we want to install another, for example: dammit databases --install --busco-group fungi # --quick Note: if you have limited space on your instance, you can also install these databases in a different location (e.g. on an external volume). You would want to run this command before running the database installs we just ran. #Run ONLY if you want to install databases in different location. #To run, remove the `#` from the front of the following command: # dammit databases --database-dir /path/to/databases","title":"Database Preparation"},{"location":"old_docs/Annotation/#annotation","text":"Now we'll download a custom Nematostella vectensis protein database available from JGI. Here, somebody has already created a proper database for us [1] (it has a reference proteome available through uniprot). If your critter is a non-model organism, you will likely need to create your own with proteins from closely-related species. This will rely on your knowledge of your system! Run the command: dammit annotate trinity.nema.fasta --busco-group metazoa --user-databases nema.reference.prot.faa --n_threads 6 # --quick While dammit runs, it will print out which tasks its running to the terminal. dammit is written with a library called pydoit , which is a python workflow library similar to GNU Make. This not only helps organize the underlying workflow, but also means that if we interrupt it, it will properly resume! After a successful run, you'll have a new directory called trinity.nema.fasta.dammit . If you look inside, you'll see a lot of files: ls trinity.nema.fasta.dammit/ annotate.doit.db trinity.nema.fasta.dammit.namemap.csv trinity.nema.fasta.transdecoder.pep dammit.log trinity.nema.fasta.dammit.stats.json trinity.nema.fasta.x.nema.reference.prot.faa.crbl.csv run_trinity.nema.fasta.metazoa.busco.results trinity.nema.fasta.transdecoder.bed trinity.nema.fasta.x.nema.reference.prot.faa.crbl.gff3 tmp trinity.nema.fasta.transdecoder.cds trinity.nema.fasta.x.nema.reference.prot.faa.crbl.model.csv trinity.nema.fasta trinity.nema.fasta.transdecoder_dir trinity.nema.fasta.x.nema.reference.prot.faa.crbl.model.plot.pdf trinity.nema.fasta.dammit.fasta trinity.nema.fasta.transdecoder.gff3 trinity.nema.fasta.dammit.gff3 trinity.nema.fasta.transdecoder.mRNA The most important files for you are trinity.nema.fasta.dammit.fasta , trinity.nema.fasta.dammit.gff3 , and trinity.nema.fasta.dammit.stats.json . If the above dammit command is run again, there will be a message: **Pipeline is already completed!**","title":"Annotation"},{"location":"old_docs/Annotation/#parse-dammit-output","text":"Cammille wrote dammit in Python, which includes a library to parse gff3 dammit output. To send this output to a useful table, we will need to open the Python environemnt. cd trinity.nema.fasta.dammit python Then, using this script, will output a list of gene ID: import pandas as pd from dammit.fileio.gff3 import GFF3Parser gff_file = \"trinity.nema.fasta.dammit.gff3\" annotations = GFF3Parser(filename=gff_file).read() names = annotations.sort_values(by=['seqid', 'score'], ascending=True).query('score < 1e-05').drop_duplicates(subset='seqid')[['seqid', 'Name']] new_file = names.dropna(axis=0,how='all') new_file.head() new_file.to_csv(\"nema_gene_name_id.csv\") exit() This will output a table of genes with 'seqid' and 'Name' in a .csv file: nema_gene_name_id.csv . Let's take a look at that file: less nema_gene_name_id.csv Notice there are multiple transcripts per gene model prediction. This .csv file can be used in tximport in downstream DE analysis.","title":"Parse dammit output"},{"location":"old_docs/Annotation/#references","text":"Putnam NH, Srivastava M, Hellsten U, Dirks B, Chapman J, Salamov A, Terry A, Shapiro H, Lindquist E, Kapitonov VV, Jurka J, Genikhovich G, Grigoriev IV, Lucas SM, Steele RE, Finnerty JR, Technau U, Martindale MQ, Rokhsar DS. (2007) Sea anemone genome reveals ancestral eumetazoan gene repertoire and genomic organization. Science. 317, 86-94.","title":"References"},{"location":"old_docs/Assembly/","text":"Transcriptome Assembly \u00b6 Kmer Trimming \u00b6 Before running transcriptome assembly, we recommend doing some kmer spectral error trimming on your dataset, and if you have lots of reads, also performing digital normalization. We use khmer for both of these tasks. You can choose whether or not to use khmer diginal normalization with --no-diginorm . Note that you can also conduct diginorm with the Trinity assembler. The commands are as follows: With digital normalizition: \" (interleave-reads.py {input.r1} {input.r2} && zcat {input.r1_orphan} {input.r2_orphan}) | \" \" (trim-low-abund.py -V -k {params.k} -Z {params.Z} -C {params.C} - -o - -M {params.memory} \" \" --diginorm --diginorm-coverage={params.cov}) | (extract-paired-reads.py --gzip \" \" -p {output.paired} -s {output.single}) > {log}; split-paired-reads.py {output.paired} \" \" -1 {output.r1_out} -2 {output.r2_out} >> {log}\" Without digital normalization: \" (interleave-reads.py {input.r1} {input.r2} && zcat {input.r1_orphan} {input.r2_orphan}) | \" \" (trim-low-abund.py -V -k {params.k} -Z {params.Z} -C {params.C} - -o - -M {params.memory} \" \"| (extract-paired-reads.py --gzip -p {output.paired} -s {output.single}) > {log}; \" split-paired-reads.py {output.paired} -1 {output.r1_out} -2 {output.r2_out} >> {log}\" Assembling with Trinity \u00b6 We use the Trinity de novo transcriptome assembler to take short, trimmed/diginorm Illumina reads data and assemble (predict) full-length transcripts into a single fasta file output. Each contig in the fasta assembly file represents one unique transcript. The default k -mer size for the Trinity assembler is k = 25. The resulting output assembly fasta file can then be used to align the original, trimmed (not diginorm) short Illumina reads and quantify expression per transcript. The ID for each transcript is output (version 2.2.0 to current) as follows, where the TRINITY is constant, the DN2202 is an example of a variable contig/transcript ID, c stands for component, g gene and i isoform: TRINITY_DN2202_c0_g1_i1 This snakemake pipeline will run the following command: Trinity --left left.fq \\ --right right.fq --seqType fq --max_memory 10G \\ --CPU 4 Note, the current version of Trininty (after 2.3.2) is configured to diginorm the input reads before assembly begins. Since we have already applied diginorm to our reads, the result will be a negligible decrease in read counts prior to the assembly. Applying diginorm twice is fine. For data sets with large numbers of reads, applying diginorm as a separate step as we have here may decrease the memory requirements needed by the Trinity pipeline.","title":"Transcriptome Assembly"},{"location":"old_docs/Assembly/#transcriptome-assembly","text":"","title":"Transcriptome Assembly"},{"location":"old_docs/Assembly/#kmer-trimming","text":"Before running transcriptome assembly, we recommend doing some kmer spectral error trimming on your dataset, and if you have lots of reads, also performing digital normalization. We use khmer for both of these tasks. You can choose whether or not to use khmer diginal normalization with --no-diginorm . Note that you can also conduct diginorm with the Trinity assembler. The commands are as follows: With digital normalizition: \" (interleave-reads.py {input.r1} {input.r2} && zcat {input.r1_orphan} {input.r2_orphan}) | \" \" (trim-low-abund.py -V -k {params.k} -Z {params.Z} -C {params.C} - -o - -M {params.memory} \" \" --diginorm --diginorm-coverage={params.cov}) | (extract-paired-reads.py --gzip \" \" -p {output.paired} -s {output.single}) > {log}; split-paired-reads.py {output.paired} \" \" -1 {output.r1_out} -2 {output.r2_out} >> {log}\" Without digital normalization: \" (interleave-reads.py {input.r1} {input.r2} && zcat {input.r1_orphan} {input.r2_orphan}) | \" \" (trim-low-abund.py -V -k {params.k} -Z {params.Z} -C {params.C} - -o - -M {params.memory} \" \"| (extract-paired-reads.py --gzip -p {output.paired} -s {output.single}) > {log}; \" split-paired-reads.py {output.paired} -1 {output.r1_out} -2 {output.r2_out} >> {log}\"","title":"Kmer Trimming"},{"location":"old_docs/Assembly/#assembling-with-trinity","text":"We use the Trinity de novo transcriptome assembler to take short, trimmed/diginorm Illumina reads data and assemble (predict) full-length transcripts into a single fasta file output. Each contig in the fasta assembly file represents one unique transcript. The default k -mer size for the Trinity assembler is k = 25. The resulting output assembly fasta file can then be used to align the original, trimmed (not diginorm) short Illumina reads and quantify expression per transcript. The ID for each transcript is output (version 2.2.0 to current) as follows, where the TRINITY is constant, the DN2202 is an example of a variable contig/transcript ID, c stands for component, g gene and i isoform: TRINITY_DN2202_c0_g1_i1 This snakemake pipeline will run the following command: Trinity --left left.fq \\ --right right.fq --seqType fq --max_memory 10G \\ --CPU 4 Note, the current version of Trininty (after 2.3.2) is configured to diginorm the input reads before assembly begins. Since we have already applied diginorm to our reads, the result will be a negligible decrease in read counts prior to the assembly. Applying diginorm twice is fine. For data sets with large numbers of reads, applying diginorm as a separate step as we have here may decrease the memory requirements needed by the Trinity pipeline.","title":"Assembling with Trinity"},{"location":"old_docs/DE/","text":"Differential expression analysis with DESeq2 \u00b6 Comparing gene expression differences in samples between experimental conditions. We will be using DESeq2 . References: Documentation for DESeq2 with example analysis Love et al. 2014 * Love et al. 2016 Additional links: DE lecture by Jane Khudyakov, July 2017 Example DE analysis from two populations of killifish! (Fundulus heteroclitus MDPL vs. MDPL) * A Review of Differential Gene Expression Software for mRNA sequencing RStudio! \u00b6 The pipeline will be running these commands in an R script. You could run them in R Studio: Load libraries library(DESeq2) library(\"lattice\") library(tximport) library(readr) library(gplots) library(RColorBrewer) source('~/plotPCAWithSampleNames.R') Tell RStudio where your files are and ask whether they exist: setwd(\"/mnt/work/quant/salmon_out/\") dir<-\"/mnt/work/quant/\" files_list = list.files() files <- file.path(dir, \"salmon_out\",files_list, \"quant.sf\") names(files) <- c(\"0Hour_1\",\"0Hour_2\",\"0Hour_3\",\"0Hour_4\",\"0Hour_5\",\"6Hour_1\",\"6Hour_2\",\"6Hour_3\",\"6Hour_4\",\"6Hour_5\") files print(file.exists(files)) Grab the gene names and transcript ID file to summarize expression at the gene level . tx2gene <- read.table(\"~/nema_transcript_gene_id.txt\",sep=\"\\t\") cols<-c(\"transcript_id\",\"gene_id\") colnames(tx2gene)<-cols head(tx2gene) txi.salmon <- tximport(files, type = \"salmon\", tx2gene = tx2gene,importer=read.delim) head(txi.salmon$counts) dim(txi.salmon$counts) Assign experimental variables: condition = factor(c(\"0Hour\",\"0Hour\",\"0Hour\",\"0Hour\",\"0Hour\",\"6Hour\",\"6Hour\",\"6Hour\",\"6Hour\",\"6Hour\")) ExpDesign <- data.frame(row.names=colnames(txi.salmon$counts), condition = condition) ExpDesign Run DESeq2: dds <- DESeqDataSetFromTximport(txi.salmon, ExpDesign, ~condition) dds <- DESeq(dds, betaPrior=FALSE) Get counts: counts_table = counts( dds, normalized=TRUE ) Filtering out low expression transcripts: See plot from Lisa Komoroske generated with RNAseq123 filtered_norm_counts<-counts_table[!rowSums(counts_table==0)>=1, ] filtered_norm_counts<-as.data.frame(filtered_norm_counts) GeneID<-rownames(filtered_norm_counts) filtered_norm_counts<-cbind(filtered_norm_counts,GeneID) dim(filtered_norm_counts) head(filtered_norm_counts) Estimate dispersion: plotDispEsts(dds) PCA: log_dds<-rlog(dds) plotPCAWithSampleNames(log_dds, intgroup=\"condition\", ntop=40000) Get DE results: res<-results(dds,contrast=c(\"condition\",\"6Hour\",\"0Hour\")) head(res) res_ordered<-res[order(res$padj),] GeneID<-rownames(res_ordered) res_ordered<-as.data.frame(res_ordered) res_genes<-cbind(res_ordered,GeneID) dim(res_genes) head(res_genes) dim(res_genes) res_genes_merged <- merge(res_genes,filtered_norm_counts,by=unique(\"GeneID\")) dim(res_genes_merged) head(res_genes_merged) res_ordered<-res_genes_merged[order(res_genes_merged$padj),] write.csv(res_ordered, file=\"nema_DESeq_all.csv\" ) Set a threshold cutoff of padj<0.05 and \u00b1 log2FC 1: resSig = res_ordered[res_ordered$padj < 0.05, ] resSig = resSig[resSig$log2FoldChange > 1 | resSig$log2FoldChange < -1,] write.csv(resSig,file=\"nema_DESeq_padj0.05_log2FC1.csv\") MA plot with gene names: plot(log2(res_ordered$baseMean), res_ordered$log2FoldChange, col=ifelse(res_ordered$padj < 0.05, \"red\",\"gray67\"),main=\"nema (padj<0.05, log2FC = \u00b11)\",xlim=c(1,20),pch=20,cex=1,ylim=c(-12,12)) abline(h=c(-1,1), col=\"blue\") genes<-resSig$GeneID mygenes <- resSig[,] baseMean_mygenes <- mygenes[,\"baseMean\"] log2FoldChange_mygenes <- mygenes[,\"log2FoldChange\"] text(log2(baseMean_mygenes),log2FoldChange_mygenes,labels=genes,pos=2,cex=0.60) Heatmap d<-resSig dim(d) head(d) colnames(d) d<-d[,c(8:17)] d<-as.matrix(d) d<-as.data.frame(d) d<-as.matrix(d) rownames(d) <- resSig[,1] head(d) hr <- hclust(as.dist(1-cor(t(d), method=\"pearson\")), method=\"complete\") mycl <- cutree(hr, h=max(hr$height/1.5)) clusterCols <- rainbow(length(unique(mycl))) myClusterSideBar <- clusterCols[mycl] myheatcol <- greenred(75) heatmap.2(d, main=\"nema (padj<0.05, log2FC = \u00b11)\", Rowv=as.dendrogram(hr), cexRow=0.75,cexCol=0.8,srtCol= 90, adjCol = c(NA,0),offsetCol=2.5, Colv=NA, dendrogram=\"row\", scale=\"row\", col=myheatcol, density.info=\"none\", trace=\"none\", RowSideColors= myClusterSideBar)","title":"Differential expression analysis with DESeq2"},{"location":"old_docs/DE/#differential-expression-analysis-with-deseq2","text":"Comparing gene expression differences in samples between experimental conditions. We will be using DESeq2 . References: Documentation for DESeq2 with example analysis Love et al. 2014 * Love et al. 2016 Additional links: DE lecture by Jane Khudyakov, July 2017 Example DE analysis from two populations of killifish! (Fundulus heteroclitus MDPL vs. MDPL) * A Review of Differential Gene Expression Software for mRNA sequencing","title":"Differential expression analysis with DESeq2"},{"location":"old_docs/DE/#rstudio","text":"The pipeline will be running these commands in an R script. You could run them in R Studio: Load libraries library(DESeq2) library(\"lattice\") library(tximport) library(readr) library(gplots) library(RColorBrewer) source('~/plotPCAWithSampleNames.R') Tell RStudio where your files are and ask whether they exist: setwd(\"/mnt/work/quant/salmon_out/\") dir<-\"/mnt/work/quant/\" files_list = list.files() files <- file.path(dir, \"salmon_out\",files_list, \"quant.sf\") names(files) <- c(\"0Hour_1\",\"0Hour_2\",\"0Hour_3\",\"0Hour_4\",\"0Hour_5\",\"6Hour_1\",\"6Hour_2\",\"6Hour_3\",\"6Hour_4\",\"6Hour_5\") files print(file.exists(files)) Grab the gene names and transcript ID file to summarize expression at the gene level . tx2gene <- read.table(\"~/nema_transcript_gene_id.txt\",sep=\"\\t\") cols<-c(\"transcript_id\",\"gene_id\") colnames(tx2gene)<-cols head(tx2gene) txi.salmon <- tximport(files, type = \"salmon\", tx2gene = tx2gene,importer=read.delim) head(txi.salmon$counts) dim(txi.salmon$counts) Assign experimental variables: condition = factor(c(\"0Hour\",\"0Hour\",\"0Hour\",\"0Hour\",\"0Hour\",\"6Hour\",\"6Hour\",\"6Hour\",\"6Hour\",\"6Hour\")) ExpDesign <- data.frame(row.names=colnames(txi.salmon$counts), condition = condition) ExpDesign Run DESeq2: dds <- DESeqDataSetFromTximport(txi.salmon, ExpDesign, ~condition) dds <- DESeq(dds, betaPrior=FALSE) Get counts: counts_table = counts( dds, normalized=TRUE ) Filtering out low expression transcripts: See plot from Lisa Komoroske generated with RNAseq123 filtered_norm_counts<-counts_table[!rowSums(counts_table==0)>=1, ] filtered_norm_counts<-as.data.frame(filtered_norm_counts) GeneID<-rownames(filtered_norm_counts) filtered_norm_counts<-cbind(filtered_norm_counts,GeneID) dim(filtered_norm_counts) head(filtered_norm_counts) Estimate dispersion: plotDispEsts(dds) PCA: log_dds<-rlog(dds) plotPCAWithSampleNames(log_dds, intgroup=\"condition\", ntop=40000) Get DE results: res<-results(dds,contrast=c(\"condition\",\"6Hour\",\"0Hour\")) head(res) res_ordered<-res[order(res$padj),] GeneID<-rownames(res_ordered) res_ordered<-as.data.frame(res_ordered) res_genes<-cbind(res_ordered,GeneID) dim(res_genes) head(res_genes) dim(res_genes) res_genes_merged <- merge(res_genes,filtered_norm_counts,by=unique(\"GeneID\")) dim(res_genes_merged) head(res_genes_merged) res_ordered<-res_genes_merged[order(res_genes_merged$padj),] write.csv(res_ordered, file=\"nema_DESeq_all.csv\" ) Set a threshold cutoff of padj<0.05 and \u00b1 log2FC 1: resSig = res_ordered[res_ordered$padj < 0.05, ] resSig = resSig[resSig$log2FoldChange > 1 | resSig$log2FoldChange < -1,] write.csv(resSig,file=\"nema_DESeq_padj0.05_log2FC1.csv\") MA plot with gene names: plot(log2(res_ordered$baseMean), res_ordered$log2FoldChange, col=ifelse(res_ordered$padj < 0.05, \"red\",\"gray67\"),main=\"nema (padj<0.05, log2FC = \u00b11)\",xlim=c(1,20),pch=20,cex=1,ylim=c(-12,12)) abline(h=c(-1,1), col=\"blue\") genes<-resSig$GeneID mygenes <- resSig[,] baseMean_mygenes <- mygenes[,\"baseMean\"] log2FoldChange_mygenes <- mygenes[,\"log2FoldChange\"] text(log2(baseMean_mygenes),log2FoldChange_mygenes,labels=genes,pos=2,cex=0.60) Heatmap d<-resSig dim(d) head(d) colnames(d) d<-d[,c(8:17)] d<-as.matrix(d) d<-as.data.frame(d) d<-as.matrix(d) rownames(d) <- resSig[,1] head(d) hr <- hclust(as.dist(1-cor(t(d), method=\"pearson\")), method=\"complete\") mycl <- cutree(hr, h=max(hr$height/1.5)) clusterCols <- rainbow(length(unique(mycl))) myClusterSideBar <- clusterCols[mycl] myheatcol <- greenred(75) heatmap.2(d, main=\"nema (padj<0.05, log2FC = \u00b11)\", Rowv=as.dendrogram(hr), cexRow=0.75,cexCol=0.8,srtCol= 90, adjCol = c(NA,0),offsetCol=2.5, Colv=NA, dendrogram=\"row\", scale=\"row\", col=myheatcol, density.info=\"none\", trace=\"none\", RowSideColors= myClusterSideBar)","title":"RStudio!"},{"location":"old_docs/Diginorm/","text":"Digital Normalization \u00b6 In this section, we\u2019ll apply digital normalization and variable-coverage k-mer abundance trimming to the reads prior to assembly using the khmer software package (version 2.1). This has the effect of reducing the computational cost of assembly without negatively affecting the quality of the assembly. This is all run in one command, taking the trimmed reads as input, uses the orphaned reads that survived while their mated pair did not during adapter and quality trimming. Then, low-abundance reads are trimmed to a coverage of 18 and normalized to a k -mer ( k = 20) coverage of 20. (interleave-reads.py {}{}.trim_1P.fq {}{}.trim_2P.fq && zcat {}orphans.fq.gz)| \\\\ (trim-low-abund.py -V -k 20 -Z 18 -C 2 - -o - -M 4e9 --diginorm --diginorm-coverage=20) | \\\\ (extract-paired-reads.py --gzip -p {}{}.paired.gz -s {}{}.single.gz) > /dev/null The output files are the remaining reads, grouped as pairs and singles (orphans). Since the Trinity de novo assembly software expects paired reads, we will split them into left left.fq and right.fq read pair files, including the single orphans in the left.fq file. for file in *.paired.gz do split-paired-reads.py ${file} done cat *.1 > left.fq cat *.2 > right.fq gunzip -c ../diginorm/single.gz >> left.fq","title":"Digital Normalization"},{"location":"old_docs/Diginorm/#digital-normalization","text":"In this section, we\u2019ll apply digital normalization and variable-coverage k-mer abundance trimming to the reads prior to assembly using the khmer software package (version 2.1). This has the effect of reducing the computational cost of assembly without negatively affecting the quality of the assembly. This is all run in one command, taking the trimmed reads as input, uses the orphaned reads that survived while their mated pair did not during adapter and quality trimming. Then, low-abundance reads are trimmed to a coverage of 18 and normalized to a k -mer ( k = 20) coverage of 20. (interleave-reads.py {}{}.trim_1P.fq {}{}.trim_2P.fq && zcat {}orphans.fq.gz)| \\\\ (trim-low-abund.py -V -k 20 -Z 18 -C 2 - -o - -M 4e9 --diginorm --diginorm-coverage=20) | \\\\ (extract-paired-reads.py --gzip -p {}{}.paired.gz -s {}{}.single.gz) > /dev/null The output files are the remaining reads, grouped as pairs and singles (orphans). Since the Trinity de novo assembly software expects paired reads, we will split them into left left.fq and right.fq read pair files, including the single orphans in the left.fq file. for file in *.paired.gz do split-paired-reads.py ${file} done cat *.1 > left.fq cat *.2 > right.fq gunzip -c ../diginorm/single.gz >> left.fq","title":"Digital Normalization"},{"location":"old_docs/QC/","text":"Short read quality and trimming \u00b6 The first step of any sequencing pipeline is to assess read quality and perform quality control as necessary. We natively enable quality assessment with FastQC and multiqc , and do quality trimming with Trimmomatic . Quality Assessment \u00b6 We\u2019re going to use FastQC (version 0.11.5) and multiqc (version 1.2) to summarize the data before and after adapter trimming. There are several caveats about FastQC - the main one is that it only calculates certain statistics (like duplicated sequences) for subsets of the data (e.g. duplicate sequences are only analyzed for the first 100,000 sequences in each file. Multiqc will summarize individual fastqc output into one output so that you can see all quality information for all files simultaenously. Adapter trim each pair of files \u00b6 We use Trimmomatic (version 0.36) to trim off residual Illumina adapters that were left behind after demultiplexing. This pipeline assumes TruSeq3-PE.fa adapters. However, if running this on your own data, you\u2019ll need to know which Illumina sequencing adapters were used for your library prep in order to trim them off. If they are the right adapters, you should see that some of the reads are trimmed; if they\u2019re not, you won\u2019t see anything get trimmed. See excellent paper on trimming parameters by MacManes 2014 . Based on these recommendations by MacManes 2014, we use this command in this pipeline: TrimmomaticPE ${base}.fastq.gz ${baseR2}.fastq.gz \\ ${base}.qc.fq.gz s1_se \\ ${baseR2}.qc.fq.gz s2_se \\ ILLUMINACLIP:TruSeq3-PE.fa:2:40:15 \\ LEADING:2 TRAILING:2 \\ SLIDINGWINDOW:4:2 \\ MINLEN:25 Customizing the trimming pipeline \u00b6 The default trimming paramters can be overridden by providing the following in the .yaml configuration file: trimmomatic: trim_cmd: \"ILLUMINACLIP:{}:2:40:15 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35\" Be sure to modify the trimming commands as desired","title":"Short read quality and trimming"},{"location":"old_docs/QC/#short-read-quality-and-trimming","text":"The first step of any sequencing pipeline is to assess read quality and perform quality control as necessary. We natively enable quality assessment with FastQC and multiqc , and do quality trimming with Trimmomatic .","title":"Short read quality and trimming"},{"location":"old_docs/QC/#quality-assessment","text":"We\u2019re going to use FastQC (version 0.11.5) and multiqc (version 1.2) to summarize the data before and after adapter trimming. There are several caveats about FastQC - the main one is that it only calculates certain statistics (like duplicated sequences) for subsets of the data (e.g. duplicate sequences are only analyzed for the first 100,000 sequences in each file. Multiqc will summarize individual fastqc output into one output so that you can see all quality information for all files simultaenously.","title":"Quality Assessment"},{"location":"old_docs/QC/#adapter-trim-each-pair-of-files","text":"We use Trimmomatic (version 0.36) to trim off residual Illumina adapters that were left behind after demultiplexing. This pipeline assumes TruSeq3-PE.fa adapters. However, if running this on your own data, you\u2019ll need to know which Illumina sequencing adapters were used for your library prep in order to trim them off. If they are the right adapters, you should see that some of the reads are trimmed; if they\u2019re not, you won\u2019t see anything get trimmed. See excellent paper on trimming parameters by MacManes 2014 . Based on these recommendations by MacManes 2014, we use this command in this pipeline: TrimmomaticPE ${base}.fastq.gz ${baseR2}.fastq.gz \\ ${base}.qc.fq.gz s1_se \\ ${baseR2}.qc.fq.gz s2_se \\ ILLUMINACLIP:TruSeq3-PE.fa:2:40:15 \\ LEADING:2 TRAILING:2 \\ SLIDINGWINDOW:4:2 \\ MINLEN:25","title":"Adapter trim each pair of files"},{"location":"old_docs/QC/#customizing-the-trimming-pipeline","text":"The default trimming paramters can be overridden by providing the following in the .yaml configuration file: trimmomatic: trim_cmd: \"ILLUMINACLIP:{}:2:40:15 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35\" Be sure to modify the trimming commands as desired","title":"Customizing the trimming pipeline"},{"location":"old_docs/Quality/","text":"Evaluating your transcriptome assembly \u00b6 We will be using Transrate and BUSCO! BUSCO \u00b6 B enchmarking U niversal S ingle C opy O rthologs (BUSCO) Eukaryota database has 303 genes Metazoa database has 978 genes \"Complete\" lengths are within two standard deviations of the BUSCO group mean length Genes that make up the BUSCO sets for each major lineage are selected from orthologous groups with genes present as single-copy orthologs in at least 90% of the species. Useful links: Website with additional busco databases: http://busco.ezlab.org/ Paper: Simao et al. 2015 User Guide Command: run_BUSCO.py \\ -i Trinity.fixed.fasta \\ -o nema_busco_metazoa -l ~/busco/metazoa_odb9 \\ -m transcriptome --cpu 2 Transrate \u00b6 Transrate serves two main purposes. It can compare two assemblies to see how similar they are. Or, it can give you a score which represents proportion of input reads that provide positive support for the assembly. We will use transrate to get a score for the assembly. Use the trimmed reads. For a further explanation of metrics and how to run the reference-based transrate, see the documentation and the paper by Smith-Unna et al. 2016 . How do two transcriptomes compare with each other? transrate --reference=Trinity.fixed.fasta --assembly=trinity-nematostella-raw.fa --output=full_v_subset transrate --reference=trinity-nematostella-raw.fa --assembly=Trinity.fixed.fasta --output=subset_v_full","title":"Evaluating your transcriptome assembly"},{"location":"old_docs/Quality/#evaluating-your-transcriptome-assembly","text":"We will be using Transrate and BUSCO!","title":"Evaluating your transcriptome assembly"},{"location":"old_docs/Quality/#busco","text":"B enchmarking U niversal S ingle C opy O rthologs (BUSCO) Eukaryota database has 303 genes Metazoa database has 978 genes \"Complete\" lengths are within two standard deviations of the BUSCO group mean length Genes that make up the BUSCO sets for each major lineage are selected from orthologous groups with genes present as single-copy orthologs in at least 90% of the species. Useful links: Website with additional busco databases: http://busco.ezlab.org/ Paper: Simao et al. 2015 User Guide Command: run_BUSCO.py \\ -i Trinity.fixed.fasta \\ -o nema_busco_metazoa -l ~/busco/metazoa_odb9 \\ -m transcriptome --cpu 2","title":"BUSCO"},{"location":"old_docs/Quality/#transrate","text":"Transrate serves two main purposes. It can compare two assemblies to see how similar they are. Or, it can give you a score which represents proportion of input reads that provide positive support for the assembly. We will use transrate to get a score for the assembly. Use the trimmed reads. For a further explanation of metrics and how to run the reference-based transrate, see the documentation and the paper by Smith-Unna et al. 2016 . How do two transcriptomes compare with each other? transrate --reference=Trinity.fixed.fasta --assembly=trinity-nematostella-raw.fa --output=full_v_subset transrate --reference=trinity-nematostella-raw.fa --assembly=Trinity.fixed.fasta --output=subset_v_full","title":"Transrate"},{"location":"old_docs/Quant/","text":"Quantification with Salmon \u00b6 We will use Salmon to quantify expression. Salmon is a new breed of software for quantifying RNAseq reads that is both really fast and takes transcript length into consideration ( Patro et al. 2015 ). For further reading, see Intro blog post: http://robpatro.com/blog/?p=248 A 2016 blog post evaluating and comparing methods here Salmon github repo here https://github.com/ngs-docs/2015-nov-adv-rna/blob/master/salmon.rst http://angus.readthedocs.io/en/2016/rob_quant/tut.html https://2016-aug-nonmodel-rnaseq.readthedocs.io/en/latest/quantification.html The two most interesting files are salmon_quant.log and quant.sf . The latter contains the counts; the former contains the log information from running things. We recommend quantifying using the Trinity transcriptome assembly fasta file, which will give expression values for each contig, like this in quant.sf : Name Length EffectiveLength TPM NumReads TRINITY_DN2202_c0_g1_i1 210 39.818 2.683835 2.000000 TRINITY_DN2270_c0_g1_i1 213 41.064 0.000000 0.000000 TRINITY_DN2201_c0_g1_i1 266 69.681 0.766816 1.000000 TRINITY_DN2222_c0_g1_i1 243 55.794 2.873014 3.000000 TRINITY_DN2291_c0_g1_i1 245 56.916 0.000000 0.000000 TRINITY_DN2269_c0_g1_i1 294 89.251 0.000000 0.000000 TRINITY_DN2269_c1_g1_i1 246 57.479 0.000000 0.000000 TRINITY_DN2279_c0_g1_i1 426 207.443 0.000000 0.000000 TRINITY_DN2262_c0_g1_i1 500 280.803 0.190459 1.000912 TRINITY_DN2253_c0_g1_i1 1523 1303.116 0.164015 4.000000 TRINITY_DN2287_c0_g1_i1 467 247.962 0.000000 0.000000 TRINITY_DN2287_c1_g1_i1 325 113.826 0.469425 1.000000 TRINITY_DN2237_c0_g1_i1 306 98.441 0.542788 1.000000 TRINITY_DN2237_c0_g2_i1 307 99.229 0.000000 0.000000 TRINITY_DN2250_c0_g1_i1 368 151.832 0.000000 0.000000 TRINITY_DN2250_c1_g1_i1 271 72.988 0.000000 0.000000 TRINITY_DN2208_c0_g1_i1 379 162.080 1.978014 6.000000 TRINITY_DN2277_c0_g1_i1 269 71.657 0.745677 1.000000 TRINITY_DN2231_c0_g1_i1 209 39.409 0.000000 0.000000 TRINITY_DN2231_c1_g1_i1 334 121.411 0.000000 0.000000 TRINITY_DN2204_c0_g1_i1 287 84.121 0.000000 0.000000 There are two commands for salmon, salmon index and salmon quant . The first command, salmon index will index the transcriptome: salmon index --index nema --transcripts trinity.nema.full.fasta --type quasi And the second command, salmon quant will quantify the trimmed reads (not diginormed) using the transcriptome: for R1 in *R1*.fastq.gz do sample=$(basename $R1 extract.fastq.gz) echo sample is $sample, R1 is $R1 R2=${R1/R1/R2} echo R2 is $R2 salmon quant -i nema -p 2 -l IU -1 <(gunzip -c $R1) -2 <(gunzip -c $R2) -o ${sample}quant done","title":"Quantification with Salmon"},{"location":"old_docs/Quant/#quantification-with-salmon","text":"We will use Salmon to quantify expression. Salmon is a new breed of software for quantifying RNAseq reads that is both really fast and takes transcript length into consideration ( Patro et al. 2015 ). For further reading, see Intro blog post: http://robpatro.com/blog/?p=248 A 2016 blog post evaluating and comparing methods here Salmon github repo here https://github.com/ngs-docs/2015-nov-adv-rna/blob/master/salmon.rst http://angus.readthedocs.io/en/2016/rob_quant/tut.html https://2016-aug-nonmodel-rnaseq.readthedocs.io/en/latest/quantification.html The two most interesting files are salmon_quant.log and quant.sf . The latter contains the counts; the former contains the log information from running things. We recommend quantifying using the Trinity transcriptome assembly fasta file, which will give expression values for each contig, like this in quant.sf : Name Length EffectiveLength TPM NumReads TRINITY_DN2202_c0_g1_i1 210 39.818 2.683835 2.000000 TRINITY_DN2270_c0_g1_i1 213 41.064 0.000000 0.000000 TRINITY_DN2201_c0_g1_i1 266 69.681 0.766816 1.000000 TRINITY_DN2222_c0_g1_i1 243 55.794 2.873014 3.000000 TRINITY_DN2291_c0_g1_i1 245 56.916 0.000000 0.000000 TRINITY_DN2269_c0_g1_i1 294 89.251 0.000000 0.000000 TRINITY_DN2269_c1_g1_i1 246 57.479 0.000000 0.000000 TRINITY_DN2279_c0_g1_i1 426 207.443 0.000000 0.000000 TRINITY_DN2262_c0_g1_i1 500 280.803 0.190459 1.000912 TRINITY_DN2253_c0_g1_i1 1523 1303.116 0.164015 4.000000 TRINITY_DN2287_c0_g1_i1 467 247.962 0.000000 0.000000 TRINITY_DN2287_c1_g1_i1 325 113.826 0.469425 1.000000 TRINITY_DN2237_c0_g1_i1 306 98.441 0.542788 1.000000 TRINITY_DN2237_c0_g2_i1 307 99.229 0.000000 0.000000 TRINITY_DN2250_c0_g1_i1 368 151.832 0.000000 0.000000 TRINITY_DN2250_c1_g1_i1 271 72.988 0.000000 0.000000 TRINITY_DN2208_c0_g1_i1 379 162.080 1.978014 6.000000 TRINITY_DN2277_c0_g1_i1 269 71.657 0.745677 1.000000 TRINITY_DN2231_c0_g1_i1 209 39.409 0.000000 0.000000 TRINITY_DN2231_c1_g1_i1 334 121.411 0.000000 0.000000 TRINITY_DN2204_c0_g1_i1 287 84.121 0.000000 0.000000 There are two commands for salmon, salmon index and salmon quant . The first command, salmon index will index the transcriptome: salmon index --index nema --transcripts trinity.nema.full.fasta --type quasi And the second command, salmon quant will quantify the trimmed reads (not diginormed) using the transcriptome: for R1 in *R1*.fastq.gz do sample=$(basename $R1 extract.fastq.gz) echo sample is $sample, R1 is $R1 R2=${R1/R1/R2} echo R2 is $R2 salmon quant -i nema -p 2 -l IU -1 <(gunzip -c $R1) -2 <(gunzip -c $R2) -o ${sample}quant done","title":"Quantification with Salmon"}]}