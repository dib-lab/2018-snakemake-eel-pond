{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"EelPond \u00b6 ___ .-' `'. / \\ | ; | | ___.--, _.._ |O) ~ (O) | _.---'`__.-( (_. __.--'`_.. '.__.\\ '--. \\_.-' ,.--'` `\"\"` ( ,.--'` ',__ /./; ;, '.__.'` __ _`) ) .---.__.' / | |\\ \\__..--\"\" \"\"\"--.,_ `---' .'.''-._.-'`_./ /\\ '. \\_.-~~~````~~~-.__`-.__.' | | .' _.-' | | \\ \\ '. \\ \\/ .' \\ \\ '. '-._) \\/ / \\ \\ `=.__`-~-. / /\\ `) ) / / `\"\".`\\ , _.-'.'\\ \\ / / ( ( / / `--~` ) ) .-' .' '.'. | ( (/` ( (` ) ) `-; ` '--; (' eelpond started as a snakemake update of the Eel Pond Protocal for de novo RNAseq analysis. It has evolved slightly to enable a number of workflows for (mostly) RNA data, which can all be run via the eelpond workflow wrapper. eelpond uses snakemake for workflow management and conda for software installation. The code can be found here . Getting Started \u00b6 Linux is the recommended OS. Nearly everything also works on MacOSX, but some programs (fastqc, Trinity) are troublesome. If you don't have conda yet, install miniconda (for Ubuntu 16.04 Jetstream image ): wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh -b echo export PATH=\"$HOME/miniconda3/bin:$PATH\" >> ~/.bash_profile source ~/.bash_profile Now, get the eelpond code git clone https://github.com/dib-lab/eelpond.git cd eelpond Create a conda environment with all the dependencies for eelpond conda env create --file ep_utils/eelpond_environment.yaml -n eelpond Activate that environment. You'll need to do this anytime you want to run eelpond source activate eelpond # or # conda activate eelpond Now you can start running workflows! Default workflow: Eel Pond Protocol for de novo RNAseq analysis \u00b6 The Eel Pond protocol (which inspired the eelpond name) included line-by-line commands that the user could follow along with using a test dataset provided in the instructions. We have re-implemented the protocol here to enable automated de novo transcriptome assembly, annotation, and quick differential expression analysis on a set of short-read Illumina data using a single command. See more about this protocol here . To test the default workflow: ./run_eelpond nema-test full This will run a small set of Nematostella vectensis test data (from Tulin et al., 2013 ) Running Your Own Data \u00b6 To run your own data, you'll need to create two files, a tsv file containing your sample info, and a yaml file containing basic configuration info. To start, copy the test data files so you can modify them. cp nema_samples.tsv <my-tsv-samples.tsv> Modify this tab-separated file with your sample names and locations for each file. Notes: - eelpond is a bit finicky here: all columns must be separated by tabs, not spaces. If you get an immediate error about your samples file, this is likely the cause. - the unit column allows you to enter multiple files or file pairs for each samples, such as files that are from different lanes. If you don't have a unit bit of information, just add something short as placeholder (e.g. a ). All units for a single sample are combined prior to the differential expression steps. At some point, we may enable a no_units version of the eelpond , but it's not in our immediate plans :). Next, build a configfile to edit: ./run_eelpond config_name --build_config This configfile will contain all the default paramters for each step of the workflow you target. If you don't specify any targets, it will default to the \"full\" pipeline, which executes read preprocessing, assembly, annotation, and quantification. Then, modify this configfile as necessary. The configfile must contain at least: samples: path/to/my-tsv-samples.tsv which directs eelpond to your sample files. Additional Info \u00b6 Each independent step is split into a smaller workflow that can be run independently, if desired, e.g. ./run_eelpond nema-test preprocess . Individual tools can also be run independently, see Advanced Usage . See the help, here: ./run_eelpond -h available workflows: preprocess: Read Quality Trimming and Filtering (fastqc, trimmomatic) kmer_trim: Kmer Trimming and/or Digital Normalization (khmer) assemble: Transcriptome Assembly (trinity) assemblyinput: Specify assembly for downstream steps annotate : Annotate the transcriptome (dammit, sourmash) quantify: Quantify transcripts (salmon) full: preprocess, kmer_trim, assemble, annotate, quantify You can see the available workflows (and which programs they run) by using the --print_workflows flag: ./run_eelpond nema-test --print_workflows References: original eel-pond protocol docs, last updated 2015 eel-pond protocol docs, last updated 2016 DIBSI, nonmodel RNAseq workshop, July 2017 SIO-BUG, nonmodel RNAseq workshop, October 2017","title":"About"},{"location":"#eelpond","text":"___ .-' `'. / \\ | ; | | ___.--, _.._ |O) ~ (O) | _.---'`__.-( (_. __.--'`_.. '.__.\\ '--. \\_.-' ,.--'` `\"\"` ( ,.--'` ',__ /./; ;, '.__.'` __ _`) ) .---.__.' / | |\\ \\__..--\"\" \"\"\"--.,_ `---' .'.''-._.-'`_./ /\\ '. \\_.-~~~````~~~-.__`-.__.' | | .' _.-' | | \\ \\ '. \\ \\/ .' \\ \\ '. '-._) \\/ / \\ \\ `=.__`-~-. / /\\ `) ) / / `\"\".`\\ , _.-'.'\\ \\ / / ( ( / / `--~` ) ) .-' .' '.'. | ( (/` ( (` ) ) `-; ` '--; (' eelpond started as a snakemake update of the Eel Pond Protocal for de novo RNAseq analysis. It has evolved slightly to enable a number of workflows for (mostly) RNA data, which can all be run via the eelpond workflow wrapper. eelpond uses snakemake for workflow management and conda for software installation. The code can be found here .","title":"EelPond"},{"location":"#getting-started","text":"Linux is the recommended OS. Nearly everything also works on MacOSX, but some programs (fastqc, Trinity) are troublesome. If you don't have conda yet, install miniconda (for Ubuntu 16.04 Jetstream image ): wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh -b echo export PATH=\"$HOME/miniconda3/bin:$PATH\" >> ~/.bash_profile source ~/.bash_profile Now, get the eelpond code git clone https://github.com/dib-lab/eelpond.git cd eelpond Create a conda environment with all the dependencies for eelpond conda env create --file ep_utils/eelpond_environment.yaml -n eelpond Activate that environment. You'll need to do this anytime you want to run eelpond source activate eelpond # or # conda activate eelpond Now you can start running workflows!","title":"Getting Started"},{"location":"#default-workflow-eel-pond-protocol-for-de-novo-rnaseq-analysis","text":"The Eel Pond protocol (which inspired the eelpond name) included line-by-line commands that the user could follow along with using a test dataset provided in the instructions. We have re-implemented the protocol here to enable automated de novo transcriptome assembly, annotation, and quick differential expression analysis on a set of short-read Illumina data using a single command. See more about this protocol here . To test the default workflow: ./run_eelpond nema-test full This will run a small set of Nematostella vectensis test data (from Tulin et al., 2013 )","title":"Default workflow: Eel Pond Protocol for de novo RNAseq analysis"},{"location":"#running-your-own-data","text":"To run your own data, you'll need to create two files, a tsv file containing your sample info, and a yaml file containing basic configuration info. To start, copy the test data files so you can modify them. cp nema_samples.tsv <my-tsv-samples.tsv> Modify this tab-separated file with your sample names and locations for each file. Notes: - eelpond is a bit finicky here: all columns must be separated by tabs, not spaces. If you get an immediate error about your samples file, this is likely the cause. - the unit column allows you to enter multiple files or file pairs for each samples, such as files that are from different lanes. If you don't have a unit bit of information, just add something short as placeholder (e.g. a ). All units for a single sample are combined prior to the differential expression steps. At some point, we may enable a no_units version of the eelpond , but it's not in our immediate plans :). Next, build a configfile to edit: ./run_eelpond config_name --build_config This configfile will contain all the default paramters for each step of the workflow you target. If you don't specify any targets, it will default to the \"full\" pipeline, which executes read preprocessing, assembly, annotation, and quantification. Then, modify this configfile as necessary. The configfile must contain at least: samples: path/to/my-tsv-samples.tsv which directs eelpond to your sample files.","title":"Running Your Own Data"},{"location":"#additional-info","text":"Each independent step is split into a smaller workflow that can be run independently, if desired, e.g. ./run_eelpond nema-test preprocess . Individual tools can also be run independently, see Advanced Usage . See the help, here: ./run_eelpond -h available workflows: preprocess: Read Quality Trimming and Filtering (fastqc, trimmomatic) kmer_trim: Kmer Trimming and/or Digital Normalization (khmer) assemble: Transcriptome Assembly (trinity) assemblyinput: Specify assembly for downstream steps annotate : Annotate the transcriptome (dammit, sourmash) quantify: Quantify transcripts (salmon) full: preprocess, kmer_trim, assemble, annotate, quantify You can see the available workflows (and which programs they run) by using the --print_workflows flag: ./run_eelpond nema-test --print_workflows References: original eel-pond protocol docs, last updated 2015 eel-pond protocol docs, last updated 2016 DIBSI, nonmodel RNAseq workshop, July 2017 SIO-BUG, nonmodel RNAseq workshop, October 2017","title":"Additional Info"},{"location":"Configuration/","text":"Configuring the pipeline \u00b6 Each step of this pipeline is highly configurable via the yaml configuration file. More info coming soon.","title":"Configuring the pipeline"},{"location":"Configuration/#configuring-the-pipeline","text":"Each step of this pipeline is highly configurable via the yaml configuration file. More info coming soon.","title":"Configuring the pipeline"},{"location":"Eel_Pond_workflow/","text":"Eel Pond Protocol Workflow \u00b6 The Eel Pond protocol (which inspired the eelpond name) included line-by-line commands that the user could follow along with using a test dataset provided in the instructions. We have re-implemented the protocol here to enable automated de novo transcriptome assembly, annotation, and quick differential expression analysis on a set of short-read Illumina data using a single command. See more about this protocol here . The \"Eel Pond\" Protocol for RNAseq consists of: trimmomatic adapter and read quality trimming fastqc read qc evaluation khmer k-mer trimming and (optional) digital normalization trinity de novo assembly dammit annotation salmon read quantification to the trinity assembly deseq2 differential expression analysis Running Test Data \u00b6 This is the default workflow for eelpond . To run: ./run_eelpond nema-test.yaml # or ./run_eelpond nema-test.yaml full This will run a small set of Nematostella vectensis test data (from Tulin et al., 2013 ). Running Your Own Data \u00b6 To run your own data, you'll need to create two files, a tsv file containing your sample info, and a yaml file containing basic configuration info. IMPORTANT: The sample info must be in a properly formatted tsv file. The easiest way to do this is to copy the test data tsv and modify: cp nema_samles.tsv my-samples.tsv Now modify my-samples.tsv with your sample information. Next, build a configfile to edit: ./run_eelpond my-config.yaml --build_config This configfile will contain all the default parameters for each step of the pipeline you target. If you don't specify any targets, it will default to the \"full\" Eel Pond Protocol pipeline, which executes read preprocessing, assembly, annotation, and quantification. Please see the documentation file for each individual program (linked above) for what parameters to modify. The configfile should look something like this: #################### Eelpond Pipeline Configfile #################### basename: eelpond experiment: _experiment1 samples: samples.tsv ### PATH TO YOUR SAMPLE FILE GOES HERE #################### assemble #################### get_data: download_data: false khmer: C: 3 Z: 18 coverage: 20 diginorm: true extra: '' ksize: 20 memory: 4e9 trimmomatic: adapter_file: pe_name: TruSeq3-PE.fa pe_url: https://raw.githubusercontent.com/timflutre/trimmomatic/master/adapters/TruSeq3-PE-2.fa se_name: TruSeq3-SE.fa se_url: https://raw.githubusercontent.com/timflutre/trimmomatic/master/adapters/TruSeq3-SE.fa extra: '' trim_cmd: ILLUMINACLIP:{}:2:40:15 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35 trinity: add_single_to_paired: false extra: '' input_kmer_trimmed: true input_trimmomatic_trimmed: false max_memory: 30G seqtype: fq #################### annotate #################### dammit: busco_group: - metazoa - eukaryota db_dir: databases db_extra: '' sourmash: extra: '' #################### quantify #################### salmon: index_params: extra: '' quant_params: extra: '' libtype: A #################### diffexp #################### deseq2: contrasts: time0-vs-time6: - time0 - time6 gene_trans_map: true pca: labels: - condition","title":"Eel Pond Protocol"},{"location":"Eel_Pond_workflow/#eel-pond-protocol-workflow","text":"The Eel Pond protocol (which inspired the eelpond name) included line-by-line commands that the user could follow along with using a test dataset provided in the instructions. We have re-implemented the protocol here to enable automated de novo transcriptome assembly, annotation, and quick differential expression analysis on a set of short-read Illumina data using a single command. See more about this protocol here . The \"Eel Pond\" Protocol for RNAseq consists of: trimmomatic adapter and read quality trimming fastqc read qc evaluation khmer k-mer trimming and (optional) digital normalization trinity de novo assembly dammit annotation salmon read quantification to the trinity assembly deseq2 differential expression analysis","title":"Eel Pond Protocol Workflow"},{"location":"Eel_Pond_workflow/#running-test-data","text":"This is the default workflow for eelpond . To run: ./run_eelpond nema-test.yaml # or ./run_eelpond nema-test.yaml full This will run a small set of Nematostella vectensis test data (from Tulin et al., 2013 ).","title":"Running Test Data"},{"location":"Eel_Pond_workflow/#running-your-own-data","text":"To run your own data, you'll need to create two files, a tsv file containing your sample info, and a yaml file containing basic configuration info. IMPORTANT: The sample info must be in a properly formatted tsv file. The easiest way to do this is to copy the test data tsv and modify: cp nema_samles.tsv my-samples.tsv Now modify my-samples.tsv with your sample information. Next, build a configfile to edit: ./run_eelpond my-config.yaml --build_config This configfile will contain all the default parameters for each step of the pipeline you target. If you don't specify any targets, it will default to the \"full\" Eel Pond Protocol pipeline, which executes read preprocessing, assembly, annotation, and quantification. Please see the documentation file for each individual program (linked above) for what parameters to modify. The configfile should look something like this: #################### Eelpond Pipeline Configfile #################### basename: eelpond experiment: _experiment1 samples: samples.tsv ### PATH TO YOUR SAMPLE FILE GOES HERE #################### assemble #################### get_data: download_data: false khmer: C: 3 Z: 18 coverage: 20 diginorm: true extra: '' ksize: 20 memory: 4e9 trimmomatic: adapter_file: pe_name: TruSeq3-PE.fa pe_url: https://raw.githubusercontent.com/timflutre/trimmomatic/master/adapters/TruSeq3-PE-2.fa se_name: TruSeq3-SE.fa se_url: https://raw.githubusercontent.com/timflutre/trimmomatic/master/adapters/TruSeq3-SE.fa extra: '' trim_cmd: ILLUMINACLIP:{}:2:40:15 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35 trinity: add_single_to_paired: false extra: '' input_kmer_trimmed: true input_trimmomatic_trimmed: false max_memory: 30G seqtype: fq #################### annotate #################### dammit: busco_group: - metazoa - eukaryota db_dir: databases db_extra: '' sourmash: extra: '' #################### quantify #################### salmon: index_params: extra: '' quant_params: extra: '' libtype: A #################### diffexp #################### deseq2: contrasts: time0-vs-time6: - time0 - time6 gene_trans_map: true pca: labels: - condition","title":"Running Your Own Data"},{"location":"Protein_assembly_workflow/","text":"Protein Assembly Workflow \u00b6 in progress The protein assembly workflow relies on the PLASS assembler and some downstream protein mapping tools. It consists of: PLASS pear paladin Running Test Data \u00b6 ./run_eelpond nema-test.yaml plass_assemble paladin_map This will run a small set of Nematostella vectensis test data (from Tulin et al., 2013 ). Running Your Own Data \u00b6 To run your own data, you'll need to create two files, a tsv file containing your sample info, and a yaml file containing basic configuration info. IMPORTANT: The sample info must be in a properly formatted tsv file. The easiest way to do this is to copy the test data tsv and modify: cp nema_samles.tsv my-samples.tsv Now modify my-samples.tsv with your sample information. Next, build a configfile to edit: ./run_eelpond my-config.yaml plass_assemble paladin_map --build_config This configfile will contain all the default paramters for each step of the targeted workflows. Please see the documentation file for each individual program (linked above) for what paramters to modify.","title":"Protein-level Assembly Workflow"},{"location":"Protein_assembly_workflow/#protein-assembly-workflow","text":"in progress The protein assembly workflow relies on the PLASS assembler and some downstream protein mapping tools. It consists of: PLASS pear paladin","title":"Protein Assembly Workflow"},{"location":"Protein_assembly_workflow/#running-test-data","text":"./run_eelpond nema-test.yaml plass_assemble paladin_map This will run a small set of Nematostella vectensis test data (from Tulin et al., 2013 ).","title":"Running Test Data"},{"location":"Protein_assembly_workflow/#running-your-own-data","text":"To run your own data, you'll need to create two files, a tsv file containing your sample info, and a yaml file containing basic configuration info. IMPORTANT: The sample info must be in a properly formatted tsv file. The easiest way to do this is to copy the test data tsv and modify: cp nema_samles.tsv my-samples.tsv Now modify my-samples.tsv with your sample information. Next, build a configfile to edit: ./run_eelpond my-config.yaml plass_assemble paladin_map --build_config This configfile will contain all the default paramters for each step of the targeted workflows. Please see the documentation file for each individual program (linked above) for what paramters to modify.","title":"Running Your Own Data"},{"location":"advanced_usage/","text":"Advanced Usage \u00b6 Each independent step is split into a smaller workflow that can be run independently, if desired, e.g. ./run_eelpond nema-test preprocess . Individual tools can also be run independently, see Advanced Usage . See the help, here: ./run_eelpond -h available workflows: preprocess: Read Quality Trimming and Filtering (fastqc, trimmomatic) kmer_trim: Kmer Trimming and/or Digital Normalization (khmer) assemble: Transcriptome Assembly (trinity) assemblyinput: Specify assembly for downstream steps annotate : Annotate the transcriptome (dammit, sourmash) quantify: Quantify transcripts (salmon) full: preprocess, kmer_trim, assemble, annotate, quantify You can see the available workflows (and which programs they run) by using the --print_workflows flag: ./run_eelpond nema-test --print_workflows more info coming soon","title":"Advanced Usage"},{"location":"advanced_usage/#advanced-usage","text":"Each independent step is split into a smaller workflow that can be run independently, if desired, e.g. ./run_eelpond nema-test preprocess . Individual tools can also be run independently, see Advanced Usage . See the help, here: ./run_eelpond -h available workflows: preprocess: Read Quality Trimming and Filtering (fastqc, trimmomatic) kmer_trim: Kmer Trimming and/or Digital Normalization (khmer) assemble: Transcriptome Assembly (trinity) assemblyinput: Specify assembly for downstream steps annotate : Annotate the transcriptome (dammit, sourmash) quantify: Quantify transcripts (salmon) full: preprocess, kmer_trim, assemble, annotate, quantify You can see the available workflows (and which programs they run) by using the --print_workflows flag: ./run_eelpond nema-test --print_workflows more info coming soon","title":"Advanced Usage"},{"location":"dammit/","text":"Annotating de novo transcriptomes with dammit \u00b6 Quickstart: Running dammit via eelpond \u00b6 Test data: ./run_eelpond nema-test annotate Note that you either need to 1) have already run an assembly, such that a fasta file is sitting in the eelpond/assembly directory, 2) Run an assembly at the same time, or 3) pass an assembly in via assemblyinput If you have not already run ./run_eelpond nema-test assemble : 2) Run trinity assembly at the same time: ./run_eelpond nema-test assemble annotate 3) OR, Pass an assembly in via assemblyinput ./run_eelpond assemblyinput annotate with an assembly in your yaml configfile, e.g.: assemblyinput: assembly: rna_testdata/nema.fasta gene_trans_map: rna_testdata/nema.fasta.gene_trans_map assembly_extension: '_input' This is commented out in the test data yaml, but go ahead and uncomment (remove leading # ) in order to use this option dammit! \u00b6 dammit is an annotation pipeline written by Camille Scott . dammit runs a relatively standard annotation protocol for transcriptomes: it begins by building gene models with Transdecoder , and then uses the following protein databases as evidence for annotation: - Swiss-Prot (manually reviewed and curated) - Pfam-A - Rfam - OrthoDB - uniref90 (uniref is optional with --full ). - nr (nr is optional with --nr ). If a protein dataset is available, this can also be supplied to the dammit pipeline with --user-databases as optional evidence for annotation. In addition, BUSCO v3 is run, which will compare the gene content in your transcriptome with a lineage-specific data set. The output is a proportion of your transcriptome that matches with the data set, which can be used as an estimate of the completeness of your transcriptome based on evolutionary expectation ( Simho et al.2015 ). There are several lineage-specific datasets available from the authors of BUSCO. We will use the metazoa dataset for this transcriptome. Computational Requirements \u00b6 For the standard pipeline, dammit needs ~18GB of storage space to store its prepared databases, plus a few hundred MB per BUSCO database. For the standard annotation pipeline, we recommend at least 16GB of RAM. This can be reduced by editing LAST parameters via a custom configuration file. The full pipeline, which uses uniref90, needs several hundred GB of space and considerable RAM to prepare the databases. You'll also want either a fat internet connection or a big cup of patience to download uniref. For some species, we have found that the amount of RAM required can be proportional to the size of the transcriptome being annotated. While dammit runs, it will print out which tasks its running to the terminal. dammit is written with a library called pydoit , which is a python workflow library similar to GNU Make and Snakemake. This not only helps organize the underlying workflow, but also means that if we interrupt it, it should properly resume! Caveat: if your job dies, without properly shutting down, snakemake will leave your directory \"locked\" (a safety feature to prevent two runs/programs from editing the same file simultaneously). If this happens, you'll need to run eelpond with the --unlock flag. Dammit Commands \u00b6 dammit has two major subcommands: dammit databases and dammit annotate . databases checks that the databases are installed and prepared, and if run with the --install flag, will perform that installation and preparation. annotate then performs the annotation using installed tools and databases. These commands are automated and integrated into a single rule in eelpond . At some point, these may be split into databases and annotate if that functionality is desired, but it's not in our immediate plans. By default, databases are placed at eelpond/databases . Both databases and annotate have a --quick option, that only installs or runs a \"quick\" version of the pipeline: transdecoder On the command line, the commands we would run are as follows: Databases \u00b6 #install databases dammit databases --install --busco-group metazoa --busco-group eukaryota Note: if you have limited space on your instance, you can also install these databases in a different location (e.g. on an external volume) by adding --database-dir /path/to/databases . You can also use a custom protein database for your species. If your critter is a non-model organism, you will likely need to create your own with proteins from closely-related species. This will rely on your knowledge of your system! Annotation \u00b6 dammit annotate trinity.nema.fasta --busco-group metazoa --user-databases <your-database-here> --n_threads 6 If you want to run a quick version of the pipeline, add a parameter, --quick , to omit OrthoDB, Uniref, Pfam, and Rfam. A \"full\" run will take longer to install and run, but you'll have access to the full annotation pipeline. Customizing Dammit parameters \u00b6 To modify any program params, you need to add a couple lines to the config file you provide to eelpond . To get a dammit configfile you can modify, run: ./run_eelpond dammit.yaml dammit --build_config The output should be a small yaml configfile that will look something like this: #################### dammit #################### dammit: busco_group: - metazoa - eukaryota db_dir: databases db_extra: annot_extra: ' --quick ' In addition to changing parameters we've specifically enabled, you can modify the extra param to pass any extra parameters. In dammit, both databases and annotation take an extra param: db_extra: '--someflag someparam --someotherflag thatotherparam' annot_extra: '--someflag someparam --someotherflag thatotherparam' Override default params by modifying any of these lines, and placing them in the config file you're using to run eelpond . Dammit Output \u00b6 After a successful run, you'll have a new directory called BASENAME.fasta.dammit . If you look inside, you'll see a lot of files. For example, for a transcriptome with basename trinity.nema , the folder trinity.nema.fasta.dammit should contain the following files after a standard (not --quick ) run: ls trinity.nema.fasta.dammit/ annotate.doit.db trinity.nema.fasta.dammit.namemap.csv trinity.nema.fasta.transdecoder.pep dammit.log trinity.nema.fasta.dammit.stats.json trinity.nema.fasta.x.nema.reference.prot.faa.crbl.csv run_trinity.nema.fasta.metazoa.busco.results trinity.nema.fasta.transdecoder.bed trinity.nema.fasta.x.nema.reference.prot.faa.crbl.gff3 tmp trinity.nema.fasta.transdecoder.cds trinity.nema.fasta.x.nema.reference.prot.faa.crbl.model.csv trinity.nema.fasta trinity.nema.fasta.transdecoder_dir trinity.nema.fasta.x.nema.reference.prot.faa.crbl.model.plot.pdf trinity.nema.fasta.dammit.fasta trinity.nema.fasta.transdecoder.gff3 trinity.nema.fasta.dammit.gff3 trinity.nema.fasta.transdecoder.mRNA As part of eelpond, we copy the two most important files, trinity.nema.fasta.dammit.fasta and trinity.nema.fasta.dammit.gff3 into the main annotation directory. trinity.nema.fasta.dammit.stats.json also gives summary stats that are quite useful. If the above dammit command is run again, there will be a message: **Pipeline is already completed!** If you'd like to rerun the dammit pipeline, you'll need to use the --forceall flag, like so: ./run_eelpond nema-test annotation --forceall Additional Notes (non-eelpond): Parsing Dammit GFF3 files \u00b6 Camille wrote dammit in Python, which includes a library to parse gff3 dammit output. If you want to work with this gff3 downstream, use htis parsing library: First, enter in a dammit environment. you can find the one eelpond uses, but it might also be easier to just make a new one using the dammit environment file: # from within the eelpond directory conda env create -n dammit --file rules/dammit/dammit-env.yaml source activate dammit Remember you can exit your conda environments with source deactivate Then: cd trinity.nema.fasta.dammit python Use gff3 librarys to output a list of gene IDs: import pandas as pd from dammit.fileio.gff3 import GFF3Parser gff_file = \"trinity.nema.fasta.dammit.gff3\" annotations = GFF3Parser(filename=gff_file).read() names = annotations.sort_values(by=['seqid', 'score'], ascending=True).query('score < 1e-05').drop_duplicates(subset='seqid')[['seqid', 'Name']] new_file = names.dropna(axis=0,how='all') new_file.head() new_file.to_csv(\"nema_gene_name_id.csv\") exit() This will output a table of genes with 'seqid' and 'Name' in a .csv file: nema_gene_name_id.csv . Let's take a look at that file: less nema_gene_name_id.csv Notice there are multiple transcripts per gene model prediction. This .csv file can be used in tximport in downstream DE analysis.","title":"dammit"},{"location":"dammit/#annotating-de-novo-transcriptomes-with-dammit","text":"","title":"Annotating de novo transcriptomes with dammit"},{"location":"dammit/#quickstart-running-dammit-via-eelpond","text":"Test data: ./run_eelpond nema-test annotate Note that you either need to 1) have already run an assembly, such that a fasta file is sitting in the eelpond/assembly directory, 2) Run an assembly at the same time, or 3) pass an assembly in via assemblyinput If you have not already run ./run_eelpond nema-test assemble : 2) Run trinity assembly at the same time: ./run_eelpond nema-test assemble annotate 3) OR, Pass an assembly in via assemblyinput ./run_eelpond assemblyinput annotate with an assembly in your yaml configfile, e.g.: assemblyinput: assembly: rna_testdata/nema.fasta gene_trans_map: rna_testdata/nema.fasta.gene_trans_map assembly_extension: '_input' This is commented out in the test data yaml, but go ahead and uncomment (remove leading # ) in order to use this option","title":"Quickstart: Running dammit via eelpond"},{"location":"dammit/#dammit","text":"dammit is an annotation pipeline written by Camille Scott . dammit runs a relatively standard annotation protocol for transcriptomes: it begins by building gene models with Transdecoder , and then uses the following protein databases as evidence for annotation: - Swiss-Prot (manually reviewed and curated) - Pfam-A - Rfam - OrthoDB - uniref90 (uniref is optional with --full ). - nr (nr is optional with --nr ). If a protein dataset is available, this can also be supplied to the dammit pipeline with --user-databases as optional evidence for annotation. In addition, BUSCO v3 is run, which will compare the gene content in your transcriptome with a lineage-specific data set. The output is a proportion of your transcriptome that matches with the data set, which can be used as an estimate of the completeness of your transcriptome based on evolutionary expectation ( Simho et al.2015 ). There are several lineage-specific datasets available from the authors of BUSCO. We will use the metazoa dataset for this transcriptome.","title":"dammit!"},{"location":"dammit/#computational-requirements","text":"For the standard pipeline, dammit needs ~18GB of storage space to store its prepared databases, plus a few hundred MB per BUSCO database. For the standard annotation pipeline, we recommend at least 16GB of RAM. This can be reduced by editing LAST parameters via a custom configuration file. The full pipeline, which uses uniref90, needs several hundred GB of space and considerable RAM to prepare the databases. You'll also want either a fat internet connection or a big cup of patience to download uniref. For some species, we have found that the amount of RAM required can be proportional to the size of the transcriptome being annotated. While dammit runs, it will print out which tasks its running to the terminal. dammit is written with a library called pydoit , which is a python workflow library similar to GNU Make and Snakemake. This not only helps organize the underlying workflow, but also means that if we interrupt it, it should properly resume! Caveat: if your job dies, without properly shutting down, snakemake will leave your directory \"locked\" (a safety feature to prevent two runs/programs from editing the same file simultaneously). If this happens, you'll need to run eelpond with the --unlock flag.","title":"Computational Requirements"},{"location":"dammit/#dammit-commands","text":"dammit has two major subcommands: dammit databases and dammit annotate . databases checks that the databases are installed and prepared, and if run with the --install flag, will perform that installation and preparation. annotate then performs the annotation using installed tools and databases. These commands are automated and integrated into a single rule in eelpond . At some point, these may be split into databases and annotate if that functionality is desired, but it's not in our immediate plans. By default, databases are placed at eelpond/databases . Both databases and annotate have a --quick option, that only installs or runs a \"quick\" version of the pipeline: transdecoder On the command line, the commands we would run are as follows:","title":"Dammit Commands"},{"location":"dammit/#databases","text":"#install databases dammit databases --install --busco-group metazoa --busco-group eukaryota Note: if you have limited space on your instance, you can also install these databases in a different location (e.g. on an external volume) by adding --database-dir /path/to/databases . You can also use a custom protein database for your species. If your critter is a non-model organism, you will likely need to create your own with proteins from closely-related species. This will rely on your knowledge of your system!","title":"Databases"},{"location":"dammit/#annotation","text":"dammit annotate trinity.nema.fasta --busco-group metazoa --user-databases <your-database-here> --n_threads 6 If you want to run a quick version of the pipeline, add a parameter, --quick , to omit OrthoDB, Uniref, Pfam, and Rfam. A \"full\" run will take longer to install and run, but you'll have access to the full annotation pipeline.","title":"Annotation"},{"location":"dammit/#customizing-dammit-parameters","text":"To modify any program params, you need to add a couple lines to the config file you provide to eelpond . To get a dammit configfile you can modify, run: ./run_eelpond dammit.yaml dammit --build_config The output should be a small yaml configfile that will look something like this: #################### dammit #################### dammit: busco_group: - metazoa - eukaryota db_dir: databases db_extra: annot_extra: ' --quick ' In addition to changing parameters we've specifically enabled, you can modify the extra param to pass any extra parameters. In dammit, both databases and annotation take an extra param: db_extra: '--someflag someparam --someotherflag thatotherparam' annot_extra: '--someflag someparam --someotherflag thatotherparam' Override default params by modifying any of these lines, and placing them in the config file you're using to run eelpond .","title":"Customizing Dammit parameters"},{"location":"dammit/#dammit-output","text":"After a successful run, you'll have a new directory called BASENAME.fasta.dammit . If you look inside, you'll see a lot of files. For example, for a transcriptome with basename trinity.nema , the folder trinity.nema.fasta.dammit should contain the following files after a standard (not --quick ) run: ls trinity.nema.fasta.dammit/ annotate.doit.db trinity.nema.fasta.dammit.namemap.csv trinity.nema.fasta.transdecoder.pep dammit.log trinity.nema.fasta.dammit.stats.json trinity.nema.fasta.x.nema.reference.prot.faa.crbl.csv run_trinity.nema.fasta.metazoa.busco.results trinity.nema.fasta.transdecoder.bed trinity.nema.fasta.x.nema.reference.prot.faa.crbl.gff3 tmp trinity.nema.fasta.transdecoder.cds trinity.nema.fasta.x.nema.reference.prot.faa.crbl.model.csv trinity.nema.fasta trinity.nema.fasta.transdecoder_dir trinity.nema.fasta.x.nema.reference.prot.faa.crbl.model.plot.pdf trinity.nema.fasta.dammit.fasta trinity.nema.fasta.transdecoder.gff3 trinity.nema.fasta.dammit.gff3 trinity.nema.fasta.transdecoder.mRNA As part of eelpond, we copy the two most important files, trinity.nema.fasta.dammit.fasta and trinity.nema.fasta.dammit.gff3 into the main annotation directory. trinity.nema.fasta.dammit.stats.json also gives summary stats that are quite useful. If the above dammit command is run again, there will be a message: **Pipeline is already completed!** If you'd like to rerun the dammit pipeline, you'll need to use the --forceall flag, like so: ./run_eelpond nema-test annotation --forceall","title":"Dammit Output"},{"location":"dammit/#additional-notes-non-eelpond-parsing-dammit-gff3-files","text":"Camille wrote dammit in Python, which includes a library to parse gff3 dammit output. If you want to work with this gff3 downstream, use htis parsing library: First, enter in a dammit environment. you can find the one eelpond uses, but it might also be easier to just make a new one using the dammit environment file: # from within the eelpond directory conda env create -n dammit --file rules/dammit/dammit-env.yaml source activate dammit Remember you can exit your conda environments with source deactivate Then: cd trinity.nema.fasta.dammit python Use gff3 librarys to output a list of gene IDs: import pandas as pd from dammit.fileio.gff3 import GFF3Parser gff_file = \"trinity.nema.fasta.dammit.gff3\" annotations = GFF3Parser(filename=gff_file).read() names = annotations.sort_values(by=['seqid', 'score'], ascending=True).query('score < 1e-05').drop_duplicates(subset='seqid')[['seqid', 'Name']] new_file = names.dropna(axis=0,how='all') new_file.head() new_file.to_csv(\"nema_gene_name_id.csv\") exit() This will output a table of genes with 'seqid' and 'Name' in a .csv file: nema_gene_name_id.csv . Let's take a look at that file: less nema_gene_name_id.csv Notice there are multiple transcripts per gene model prediction. This .csv file can be used in tximport in downstream DE analysis.","title":"Additional Notes (non-eelpond): Parsing Dammit GFF3 files"},{"location":"deseq2/","text":"Differential expression analysis with DESeq2 \u00b6 coming soon","title":"deseq2"},{"location":"deseq2/#differential-expression-analysis-with-deseq2","text":"coming soon","title":"Differential expression analysis with DESeq2"},{"location":"dev_tips/","text":"Notes for Developers \u00b6 Admin: updating mkdocs documentation update the docs and commit your changes mkdocs build to update the docs note: if you don't already have mkdocs, install with: conda install -c conda-forge mkdocs if you haven't already, install ghp-import: conda install -c conda-forge ghp-import use ghp-import to push the updated to docs to the gh-pages branch ghp-import site -p Some useful conda, snakemake, workflow hints: optional: to make conda installs simpler, set up conda configuration \u00b6 conda config --set always_yes yes --set changeps1 no conda config --add channels conda-forge conda config --add channels defaults conda config --add channels bioconda If you need to modify a conda package: \u00b6 you'll need to work with a local install of that package. Here's how to use conda to install the dependenciesfrom the conda recipe. see also https://conda.io/docs/user-guide/tutorials/build-pkgs.html#building-and-installing install conda-build ``` conda install conda-build ``` clone the repo of interest and cd into it ``` git clone dammit-repo cd dammit-repo ``` There should be a folder called recipe. Use conda-build to build it. ``` conda build recipe ``` Install the local code ``` ## not working conda install dammit --use-local # or, you can use pip: # pip install -e . \u2014no-deps # NOW, use: conda develop . pip install -e . ```","title":"Notes for Developers"},{"location":"dev_tips/#notes-for-developers","text":"Admin: updating mkdocs documentation update the docs and commit your changes mkdocs build to update the docs note: if you don't already have mkdocs, install with: conda install -c conda-forge mkdocs if you haven't already, install ghp-import: conda install -c conda-forge ghp-import use ghp-import to push the updated to docs to the gh-pages branch ghp-import site -p Some useful conda, snakemake, workflow hints:","title":"Notes for Developers"},{"location":"dev_tips/#optional-to-make-conda-installs-simpler-set-up-conda-configuration","text":"conda config --set always_yes yes --set changeps1 no conda config --add channels conda-forge conda config --add channels defaults conda config --add channels bioconda","title":"optional: to make conda installs simpler, set up conda configuration"},{"location":"dev_tips/#if-you-need-to-modify-a-conda-package","text":"you'll need to work with a local install of that package. Here's how to use conda to install the dependenciesfrom the conda recipe. see also https://conda.io/docs/user-guide/tutorials/build-pkgs.html#building-and-installing install conda-build ``` conda install conda-build ``` clone the repo of interest and cd into it ``` git clone dammit-repo cd dammit-repo ``` There should be a folder called recipe. Use conda-build to build it. ``` conda build recipe ``` Install the local code ``` ## not working conda install dammit --use-local # or, you can use pip: # pip install -e . \u2014no-deps # NOW, use: conda develop . pip install -e . ```","title":"If you need to modify a conda package:"},{"location":"fastqc/","text":"FastQC \u00b6 Quickstart: Running FastQC with eelpond \u00b6 ./run_eelpond nema-test preprocess This will run trimmomatic trimming and fastqc on pre-trim and post-trim data. FastQC \u00b6 We use FastQC to assess quality of sequencing data before and after adapter trimming. From the FastQC documentation : What is FastQC Modern high throughput sequencers can generate hundreds of millions of sequences in a single run. Before analysing this sequence to draw biological conclusions you should always perform some simple quality control checks to ensure that the raw data looks good and there are no problems or biases in your data which may affect how you can usefully use it. Most sequencers will generate a QC report as part of their analysis pipeline, but this is usually only focused on identifying problems which were generated by the sequencer itself. FastQC aims to provide a QC report which can spot problems which originate either in the sequencer or in the starting library material. FastQC can be run in one of two modes. It can either run as a stand alone interactive application for the immediate analysis of small numbers of FastQ files, or it can be run in a non-interactive mode where it would be suitable for integrating into a larger analysis pipeline for the systematic processing of large numbers of files. There are several caveats about FastQC - the main one is that it only calculates certain statistics (like duplicated sequences) for subsets of the data (e.g. duplicate sequences are only analyzed for the first 100,000 sequences in each file). Check out these examples of good and bad Illumina data. FastQC Params \u00b6 To modify any program params, you need to add a couple lines to the config file you provide to eelpond . To get a fastqc config you can modify, run: ./run_eelpond fastqc.yaml fastqc --build_config The output should be a small yaml configfile that contains: #################### fastqc #################### fastqc: extra: '' There's almost nothing in here because we use default params. However, you can modify the extra param to pass any extra trimmomatic parameters, e.g.: extra: '--someflag someparam --someotherflag thatotherparam' Be sure the modified lines go into the config file you're using to run eelpond . FastQC rule \u00b6 We use a local copies of the fastqc snakemake wrapper to run Fastqc. For snakemake afficionados, Here's the basic structure of the fastqc rules. Directories and parameters are specified via the configfile (see the rule on github ). def get_trimmed(wildcards): if not is_single_end(**wildcards): return expand(join(trim_dir, '{sample}_{unit}_{end}.fq.gz'), **wildcards) return expand(join(trim_dir, '{sample}_{unit}_{end}.fq.gz'), **wildcards) def get_pretrim(wildcards): if not is_single_end(**wildcards): return expand(join(data_dir, '{sample}_{unit}_{end}.fq.gz'), **wildcards) return expand(join(data_dir, '{sample}_{unit}_{end}.fq.gz'), **wildcards) rule fastqc_trimmed: input: get_trimmed output: html=join(qc_dir,'{sample}_{unit}_{end}.fastqc.html'), zip=join(qc_dir,'{sample}_{unit}_{end}.fastqc.zip') params: fastqc_params.get('extra', \"\") log: join(LOGS_DIR, 'fastqc/{sample}_{unit}_{end}.log') conda: \"fastqc-env.yaml\" script: \"wrapper.py\" rule fastqc_pretrim: input: get_pretrim output: html=join(qc_dir,'{sample}_{unit}_{end}.fastqc.html'), zip=join(qc_dir,'{sample}_{unit}_{end}.fastqc.zip') params: fastqc_params.get('extra', \"\") log: join(LOGS_DIR, 'fastqc/{sample}_{unit}_{end}.log') conda: \"fastqc-env.yaml\" script: \"wrapper.py\"","title":"fastqc"},{"location":"fastqc/#fastqc","text":"","title":"FastQC"},{"location":"fastqc/#quickstart-running-fastqc-with-eelpond","text":"./run_eelpond nema-test preprocess This will run trimmomatic trimming and fastqc on pre-trim and post-trim data.","title":"Quickstart: Running FastQC with eelpond"},{"location":"fastqc/#fastqc_1","text":"We use FastQC to assess quality of sequencing data before and after adapter trimming. From the FastQC documentation : What is FastQC Modern high throughput sequencers can generate hundreds of millions of sequences in a single run. Before analysing this sequence to draw biological conclusions you should always perform some simple quality control checks to ensure that the raw data looks good and there are no problems or biases in your data which may affect how you can usefully use it. Most sequencers will generate a QC report as part of their analysis pipeline, but this is usually only focused on identifying problems which were generated by the sequencer itself. FastQC aims to provide a QC report which can spot problems which originate either in the sequencer or in the starting library material. FastQC can be run in one of two modes. It can either run as a stand alone interactive application for the immediate analysis of small numbers of FastQ files, or it can be run in a non-interactive mode where it would be suitable for integrating into a larger analysis pipeline for the systematic processing of large numbers of files. There are several caveats about FastQC - the main one is that it only calculates certain statistics (like duplicated sequences) for subsets of the data (e.g. duplicate sequences are only analyzed for the first 100,000 sequences in each file). Check out these examples of good and bad Illumina data.","title":"FastQC"},{"location":"fastqc/#fastqc-params","text":"To modify any program params, you need to add a couple lines to the config file you provide to eelpond . To get a fastqc config you can modify, run: ./run_eelpond fastqc.yaml fastqc --build_config The output should be a small yaml configfile that contains: #################### fastqc #################### fastqc: extra: '' There's almost nothing in here because we use default params. However, you can modify the extra param to pass any extra trimmomatic parameters, e.g.: extra: '--someflag someparam --someotherflag thatotherparam' Be sure the modified lines go into the config file you're using to run eelpond .","title":"FastQC Params"},{"location":"fastqc/#fastqc-rule","text":"We use a local copies of the fastqc snakemake wrapper to run Fastqc. For snakemake afficionados, Here's the basic structure of the fastqc rules. Directories and parameters are specified via the configfile (see the rule on github ). def get_trimmed(wildcards): if not is_single_end(**wildcards): return expand(join(trim_dir, '{sample}_{unit}_{end}.fq.gz'), **wildcards) return expand(join(trim_dir, '{sample}_{unit}_{end}.fq.gz'), **wildcards) def get_pretrim(wildcards): if not is_single_end(**wildcards): return expand(join(data_dir, '{sample}_{unit}_{end}.fq.gz'), **wildcards) return expand(join(data_dir, '{sample}_{unit}_{end}.fq.gz'), **wildcards) rule fastqc_trimmed: input: get_trimmed output: html=join(qc_dir,'{sample}_{unit}_{end}.fastqc.html'), zip=join(qc_dir,'{sample}_{unit}_{end}.fastqc.zip') params: fastqc_params.get('extra', \"\") log: join(LOGS_DIR, 'fastqc/{sample}_{unit}_{end}.log') conda: \"fastqc-env.yaml\" script: \"wrapper.py\" rule fastqc_pretrim: input: get_pretrim output: html=join(qc_dir,'{sample}_{unit}_{end}.fastqc.html'), zip=join(qc_dir,'{sample}_{unit}_{end}.fastqc.zip') params: fastqc_params.get('extra', \"\") log: join(LOGS_DIR, 'fastqc/{sample}_{unit}_{end}.log') conda: \"fastqc-env.yaml\" script: \"wrapper.py\"","title":"FastQC rule"},{"location":"khmer/","text":"Khmer k-mer trimming and (optional) diginorm \u00b6 Before running transcriptome assembly, we recommend doing some kmer spectral error trimming on your dataset, and if you have lots of reads, also performing digital normalization. This has the effect of reducing the computational cost of assembly without negatively affecting the quality of the assembly. We use khmer for both of these tasks. Khmer Command \u00b6 Here's the command as it would look on the command line: (interleave-reads.py sample_1.trim.fq sample_2.trim.fq )| \\\\ (trim-low-abund.py -V -k 20 -Z 18 -C 2 - -o - -M 4e9 --diginorm --diginorm-coverage=20) | \\\\ (extract-paired-reads.py --gzip -p sample.paired.gz -s sample.single.gz) > /dev/null Trimmed reads are used as input. trim-low-abund.py trims low-abundance reads to a coverage of 18. Here, we also perform digital normalization to a k -mer ( k = 20) coverage of 20.The output files are the remaining reads, grouped as pairs and singles (orphans). For more on trim-low-abund , see this recipe , Finally, since Trinity expects separate left and right files, we use split-paired-reads.py to split the interleaved pairs into two files. split-paired-reads.py sample.paired.gz Customizing Khmer parameters \u00b6 To modify any program params, you need to add a couple lines to the config file you provide to eelpond . To get a khmer configfile you can modify, run: ./run_eelpond khmer.yaml khmer --build_config The output should be a small yaml configfile that contains: #################### khmer #################### khmer: C: 3 Z: 18 coverage: 20 diginorm: true extra: '' ksize: 20 memory: 4e9 Override default params by modifying any of these lines, and placing them in the config file you're using to run eelpond . eelpond rule \u00b6 For snakemake afficionados, here's the basic structure of the khmer eelpond rules. Directories and parameters are specified via the configfile, for more, see the rule on github . rule khmer_pe_diginorm: \"\"\" kmer trim and diginorm with khmer \"\"\" input: unpack(get_trimmed) output: paired=join(khmer_dir,'{sample}_{unit}.paired.khmer.fq.gz'), single=join(khmer_dir,'{sample}_{unit}.single.khmer.fq.gz'), message: \"\"\"--- khmer trimming of low-abundance kmers and digital normalization ---\"\"\" params: k = khmer_params.get('ksize', 20), Z = khmer_params.get('Z', 18), C = khmer_params.get('C', 3), memory = khmer_params.get('memory', 4e9), cov = khmer_params.get('coverage', 20), extra = khmer_params.get('extra', '') threads: 10 log: join(LOGS_DIR, 'khmer/{sample}_{unit}.pe.diginorm.log') conda: 'khmer-env.yaml' shell: \" (interleave-reads.py {input.r1} {input.r2} ) | \" \" (trim-low-abund.py -V -k {params.k} -Z {params.Z} -C {params.C} - -o - -M {params.memory} \" \" --diginorm --diginorm-coverage={params.cov}) | (extract-paired-reads.py --gzip \" \" -p {output.paired} -s {output.single}) > {log}\" rule khmer_split_paired: input: join(khmer_dir,'{sample}_{unit}.paired.khmer.fq.gz'), output: r1=join(khmer_dir, '{sample}_{unit}_1.khmer.fq.gz'), r2=join(khmer_dir, '{sample}_{unit}_2.khmer.fq.gz'), threads: 2 log: join(LOGS_DIR, 'khmer/{sample}_{unit}_split_paired' + BASE + '.log') conda: \"khmer-env.yaml\" shell: \"\"\" split-paired-reads.py {input} --gzip -1 {output.r1} -2 {output.r2} >> {log} \"\"\"","title":"khmer"},{"location":"khmer/#khmer-k-mer-trimming-and-optional-diginorm","text":"Before running transcriptome assembly, we recommend doing some kmer spectral error trimming on your dataset, and if you have lots of reads, also performing digital normalization. This has the effect of reducing the computational cost of assembly without negatively affecting the quality of the assembly. We use khmer for both of these tasks.","title":"Khmer k-mer trimming and (optional) diginorm"},{"location":"khmer/#khmer-command","text":"Here's the command as it would look on the command line: (interleave-reads.py sample_1.trim.fq sample_2.trim.fq )| \\\\ (trim-low-abund.py -V -k 20 -Z 18 -C 2 - -o - -M 4e9 --diginorm --diginorm-coverage=20) | \\\\ (extract-paired-reads.py --gzip -p sample.paired.gz -s sample.single.gz) > /dev/null Trimmed reads are used as input. trim-low-abund.py trims low-abundance reads to a coverage of 18. Here, we also perform digital normalization to a k -mer ( k = 20) coverage of 20.The output files are the remaining reads, grouped as pairs and singles (orphans). For more on trim-low-abund , see this recipe , Finally, since Trinity expects separate left and right files, we use split-paired-reads.py to split the interleaved pairs into two files. split-paired-reads.py sample.paired.gz","title":"Khmer Command"},{"location":"khmer/#customizing-khmer-parameters","text":"To modify any program params, you need to add a couple lines to the config file you provide to eelpond . To get a khmer configfile you can modify, run: ./run_eelpond khmer.yaml khmer --build_config The output should be a small yaml configfile that contains: #################### khmer #################### khmer: C: 3 Z: 18 coverage: 20 diginorm: true extra: '' ksize: 20 memory: 4e9 Override default params by modifying any of these lines, and placing them in the config file you're using to run eelpond .","title":"Customizing Khmer parameters"},{"location":"khmer/#eelpond-rule","text":"For snakemake afficionados, here's the basic structure of the khmer eelpond rules. Directories and parameters are specified via the configfile, for more, see the rule on github . rule khmer_pe_diginorm: \"\"\" kmer trim and diginorm with khmer \"\"\" input: unpack(get_trimmed) output: paired=join(khmer_dir,'{sample}_{unit}.paired.khmer.fq.gz'), single=join(khmer_dir,'{sample}_{unit}.single.khmer.fq.gz'), message: \"\"\"--- khmer trimming of low-abundance kmers and digital normalization ---\"\"\" params: k = khmer_params.get('ksize', 20), Z = khmer_params.get('Z', 18), C = khmer_params.get('C', 3), memory = khmer_params.get('memory', 4e9), cov = khmer_params.get('coverage', 20), extra = khmer_params.get('extra', '') threads: 10 log: join(LOGS_DIR, 'khmer/{sample}_{unit}.pe.diginorm.log') conda: 'khmer-env.yaml' shell: \" (interleave-reads.py {input.r1} {input.r2} ) | \" \" (trim-low-abund.py -V -k {params.k} -Z {params.Z} -C {params.C} - -o - -M {params.memory} \" \" --diginorm --diginorm-coverage={params.cov}) | (extract-paired-reads.py --gzip \" \" -p {output.paired} -s {output.single}) > {log}\" rule khmer_split_paired: input: join(khmer_dir,'{sample}_{unit}.paired.khmer.fq.gz'), output: r1=join(khmer_dir, '{sample}_{unit}_1.khmer.fq.gz'), r2=join(khmer_dir, '{sample}_{unit}_2.khmer.fq.gz'), threads: 2 log: join(LOGS_DIR, 'khmer/{sample}_{unit}_split_paired' + BASE + '.log') conda: \"khmer-env.yaml\" shell: \"\"\" split-paired-reads.py {input} --gzip -1 {output.r1} -2 {output.r2} >> {log} \"\"\"","title":"eelpond rule"},{"location":"paladin/","text":"Mapping with Paladin \u00b6 coming soon","title":"paladin"},{"location":"paladin/#mapping-with-paladin","text":"coming soon","title":"Mapping with Paladin"},{"location":"pear/","text":"Merging Read Pairs with PEAR \u00b6 coming soon","title":"pear"},{"location":"pear/#merging-read-pairs-with-pear","text":"coming soon","title":"Merging Read Pairs with PEAR"},{"location":"plass/","text":"Assembling with PLASS \u00b6 coming soon!","title":"plass"},{"location":"plass/#assembling-with-plass","text":"coming soon!","title":"Assembling with PLASS"},{"location":"rcorrector/","text":"Correcting reads with Rcorrector \u00b6 coming soon*","title":"rcorrector"},{"location":"rcorrector/#correcting-reads-with-rcorrector","text":"coming soon*","title":"Correcting reads with Rcorrector"},{"location":"salmon/","text":"Quantification with Salmon \u00b6 Quickstart: Running salmon quantificaton with eelpond \u00b6 ./run_eelpond nema-test quantify If you haven't already trimmed reads with trimmomatic, that will happen automatically for you. However, you do need to either 1) have already run an assembly, such that a fasta file is sitting in the eelpond/assembly directory, 2) Run an assembly at the same time, or 3) pass an assembly in via assemblyinput If you have not already run ./run_eelpond nema-test assemble : 2) Run trinity assembly at the same time: ./run_eelpond nema-test assemble annotate 3) OR, Pass an assembly in via assemblyinput ./run_eelpond assemblyinput annotate with an assembly in your yaml configfile, e.g.: assemblyinput: assembly: rna_testdata/nema.fasta gene_trans_map: rna_testdata/nema.fasta.gene_trans_map assembly_extension: '_input' This is commented out in the test data yaml, but go ahead and uncomment (remove leading # ) in order to use this option. If you have these in your configfile, eelpond will automatically assume you want to run the assemblyinput rules, but it's nice to specify them in the command anyway :). We will use Salmon to quantify expression. Salmon is a new breed of software for quantifying RNAseq reads that is both really fast and takes transcript length into consideration ( Patro et al. 2015 ). Salmon Commands \u00b6 There are two commands for salmon, salmon index and salmon quant . The first command, salmon index will index the transcriptome: salmon index --index nema --transcripts trinity.nema.full.fasta --type quasi And the second command, salmon quant will quantify the trimmed reads (not diginormed) using the transcriptome: for R1 in *R1*.fastq.gz do sample=$(basename $R1 extract.fastq.gz) echo sample is $sample, R1 is $R1 R2=${R1/R1/R2} echo R2 is $R2 salmon quant -i nema -p 2 -l IU -1 <(gunzip -c $R1) -2 <(gunzip -c $R2) -o ${sample}quant done These are both integrated as rules in the eelpond workflow, so the whole process happens in an automated fashion. Customizing Salmon Parameters \u00b6 To modify any program params, you need to add a couple lines to the config file you provide to eelpond . To get a salmon configfile you can modify, run: ./run_eelpond salmon.yaml salmon --build_config The output should be a small yaml configfile that contains: #################### salmon #################### salmon: index_params: extra: '' quant_params: libtype: A extra: '' In addition to changing parameters we've specifically enabled, you can modify the extra param to pass any extra parameters.In salmon, both index and quantification steps can accept an extra param. Override default params by modifying any of these lines, and placing them in the config file you're using to run eelpond . Salmon Output \u00b6 The two most interesting files are salmon_quant.log and quant.sf . The latter contains the counts; the former contains the log information from running things. We recommend quantifying using the Trinity transcriptome assembly fasta file, which will give expression values for each contig, like this in quant.sf : Name Length EffectiveLength TPM NumReads TRINITY_DN2202_c0_g1_i1 210 39.818 2.683835 2.000000 TRINITY_DN2270_c0_g1_i1 213 41.064 0.000000 0.000000 TRINITY_DN2201_c0_g1_i1 266 69.681 0.766816 1.000000 TRINITY_DN2222_c0_g1_i1 243 55.794 2.873014 3.000000 TRINITY_DN2291_c0_g1_i1 245 56.916 0.000000 0.000000 TRINITY_DN2269_c0_g1_i1 294 89.251 0.000000 0.000000 TRINITY_DN2269_c1_g1_i1 246 57.479 0.000000 0.000000 TRINITY_DN2279_c0_g1_i1 426 207.443 0.000000 0.000000 TRINITY_DN2262_c0_g1_i1 500 280.803 0.190459 1.000912 TRINITY_DN2253_c0_g1_i1 1523 1303.116 0.164015 4.000000 TRINITY_DN2287_c0_g1_i1 467 247.962 0.000000 0.000000 TRINITY_DN2287_c1_g1_i1 325 113.826 0.469425 1.000000 TRINITY_DN2237_c0_g1_i1 306 98.441 0.542788 1.000000 TRINITY_DN2237_c0_g2_i1 307 99.229 0.000000 0.000000 TRINITY_DN2250_c0_g1_i1 368 151.832 0.000000 0.000000 TRINITY_DN2250_c1_g1_i1 271 72.988 0.000000 0.000000 TRINITY_DN2208_c0_g1_i1 379 162.080 1.978014 6.000000 TRINITY_DN2277_c0_g1_i1 269 71.657 0.745677 1.000000 TRINITY_DN2231_c0_g1_i1 209 39.409 0.000000 0.000000 TRINITY_DN2231_c1_g1_i1 334 121.411 0.000000 0.000000 TRINITY_DN2204_c0_g1_i1 287 84.121 0.000000 0.000000 Further Reading \u00b6 For further reading, on salmon see Intro blog post: http://robpatro.com/blog/?p=248 A 2016 blog post evaluating and comparing methods here Salmon github repo here https://github.com/ngs-docs/2015-nov-adv-rna/blob/master/salmon.rst http://angus.readthedocs.io/en/2016/rob_quant/tut.html https://2016-aug-nonmodel-rnaseq.readthedocs.io/en/latest/quantification.html","title":"salmon"},{"location":"salmon/#quantification-with-salmon","text":"","title":"Quantification with Salmon"},{"location":"salmon/#quickstart-running-salmon-quantificaton-with-eelpond","text":"./run_eelpond nema-test quantify If you haven't already trimmed reads with trimmomatic, that will happen automatically for you. However, you do need to either 1) have already run an assembly, such that a fasta file is sitting in the eelpond/assembly directory, 2) Run an assembly at the same time, or 3) pass an assembly in via assemblyinput If you have not already run ./run_eelpond nema-test assemble : 2) Run trinity assembly at the same time: ./run_eelpond nema-test assemble annotate 3) OR, Pass an assembly in via assemblyinput ./run_eelpond assemblyinput annotate with an assembly in your yaml configfile, e.g.: assemblyinput: assembly: rna_testdata/nema.fasta gene_trans_map: rna_testdata/nema.fasta.gene_trans_map assembly_extension: '_input' This is commented out in the test data yaml, but go ahead and uncomment (remove leading # ) in order to use this option. If you have these in your configfile, eelpond will automatically assume you want to run the assemblyinput rules, but it's nice to specify them in the command anyway :). We will use Salmon to quantify expression. Salmon is a new breed of software for quantifying RNAseq reads that is both really fast and takes transcript length into consideration ( Patro et al. 2015 ).","title":"Quickstart: Running salmon quantificaton with eelpond"},{"location":"salmon/#salmon-commands","text":"There are two commands for salmon, salmon index and salmon quant . The first command, salmon index will index the transcriptome: salmon index --index nema --transcripts trinity.nema.full.fasta --type quasi And the second command, salmon quant will quantify the trimmed reads (not diginormed) using the transcriptome: for R1 in *R1*.fastq.gz do sample=$(basename $R1 extract.fastq.gz) echo sample is $sample, R1 is $R1 R2=${R1/R1/R2} echo R2 is $R2 salmon quant -i nema -p 2 -l IU -1 <(gunzip -c $R1) -2 <(gunzip -c $R2) -o ${sample}quant done These are both integrated as rules in the eelpond workflow, so the whole process happens in an automated fashion.","title":"Salmon Commands"},{"location":"salmon/#customizing-salmon-parameters","text":"To modify any program params, you need to add a couple lines to the config file you provide to eelpond . To get a salmon configfile you can modify, run: ./run_eelpond salmon.yaml salmon --build_config The output should be a small yaml configfile that contains: #################### salmon #################### salmon: index_params: extra: '' quant_params: libtype: A extra: '' In addition to changing parameters we've specifically enabled, you can modify the extra param to pass any extra parameters.In salmon, both index and quantification steps can accept an extra param. Override default params by modifying any of these lines, and placing them in the config file you're using to run eelpond .","title":"Customizing Salmon Parameters"},{"location":"salmon/#salmon-output","text":"The two most interesting files are salmon_quant.log and quant.sf . The latter contains the counts; the former contains the log information from running things. We recommend quantifying using the Trinity transcriptome assembly fasta file, which will give expression values for each contig, like this in quant.sf : Name Length EffectiveLength TPM NumReads TRINITY_DN2202_c0_g1_i1 210 39.818 2.683835 2.000000 TRINITY_DN2270_c0_g1_i1 213 41.064 0.000000 0.000000 TRINITY_DN2201_c0_g1_i1 266 69.681 0.766816 1.000000 TRINITY_DN2222_c0_g1_i1 243 55.794 2.873014 3.000000 TRINITY_DN2291_c0_g1_i1 245 56.916 0.000000 0.000000 TRINITY_DN2269_c0_g1_i1 294 89.251 0.000000 0.000000 TRINITY_DN2269_c1_g1_i1 246 57.479 0.000000 0.000000 TRINITY_DN2279_c0_g1_i1 426 207.443 0.000000 0.000000 TRINITY_DN2262_c0_g1_i1 500 280.803 0.190459 1.000912 TRINITY_DN2253_c0_g1_i1 1523 1303.116 0.164015 4.000000 TRINITY_DN2287_c0_g1_i1 467 247.962 0.000000 0.000000 TRINITY_DN2287_c1_g1_i1 325 113.826 0.469425 1.000000 TRINITY_DN2237_c0_g1_i1 306 98.441 0.542788 1.000000 TRINITY_DN2237_c0_g2_i1 307 99.229 0.000000 0.000000 TRINITY_DN2250_c0_g1_i1 368 151.832 0.000000 0.000000 TRINITY_DN2250_c1_g1_i1 271 72.988 0.000000 0.000000 TRINITY_DN2208_c0_g1_i1 379 162.080 1.978014 6.000000 TRINITY_DN2277_c0_g1_i1 269 71.657 0.745677 1.000000 TRINITY_DN2231_c0_g1_i1 209 39.409 0.000000 0.000000 TRINITY_DN2231_c1_g1_i1 334 121.411 0.000000 0.000000 TRINITY_DN2204_c0_g1_i1 287 84.121 0.000000 0.000000","title":"Salmon Output"},{"location":"salmon/#further-reading","text":"For further reading, on salmon see Intro blog post: http://robpatro.com/blog/?p=248 A 2016 blog post evaluating and comparing methods here Salmon github repo here https://github.com/ngs-docs/2015-nov-adv-rna/blob/master/salmon.rst http://angus.readthedocs.io/en/2016/rob_quant/tut.html https://2016-aug-nonmodel-rnaseq.readthedocs.io/en/latest/quantification.html","title":"Further Reading"},{"location":"sourmash/","text":"Sourmash \u00b6 coming soon","title":"sourmash"},{"location":"sourmash/#sourmash","text":"coming soon","title":"Sourmash"},{"location":"trimmomatic/","text":"Trimmomatic \u00b6 Quickstart: Running Trimmomatic with eelpond \u00b6 ./run_eelpond nema-test preprocess This will run trimmomatic trimming and fastqc on pre-trim and post-trim data. Trimmomatic \u00b6 We use Trimmomatic (version 0.36) to trim off residual Illumina adapters that were left behind after demultiplexing. Here we use a set TruSeq Illumina adapters. However, if running this on your own data and you know you have different adapters, you'll want to input them in the configfile (see params section, below). If you're using the right adapters, you should see that some of the reads are trimmed; if they\u2019re not, you won\u2019t see anything get trimmed. See excellent paper on trimming parameters by MacManes 2014 . Trimmomatic Command \u00b6 Based on these recommendations by MacManes 2014, we use this command in this pipeline: TrimmomaticPE ${base}.fastq.gz ${baseR2}.fastq.gz \\ ${base}.qc.fq.gz s1_se \\ ${baseR2}.qc.fq.gz s2_se \\ ILLUMINACLIP:TruSeq3-PE.fa:2:40:15 \\ LEADING:2 TRAILING:2 \\ SLIDINGWINDOW:4:15 \\ MINLEN:25 However, the trimming command can be extensively modified via the configfile. Here's how the parameters for the command above look in our config: trimmomatic: trim_cmd: \"ILLUMINACLIP:{}:2:40:15 LEADING:2 TRAILING:2 SLIDINGWINDOW:4:15 MINLEN:25\" extra: '' Customizing the trimming parameters \u00b6 To modify any program params, you need to add a couple lines to the config file you provide to eelpond . To get a trimmomatic config you can modify, run: ./run_eelpond trimmomatic.yaml trimmomatic --build_config The output should be a small yaml configfile that contains: #################### trimmomatic #################### trimmomatic: adapter_file: pe_name: TruSeq3-PE.fa pe_url: https://raw.githubusercontent.com/timflutre/trimmomatic/master/adapters/TruSeq3-PE-2.fa se_name: TruSeq3-SE.fa se_url: https://raw.githubusercontent.com/timflutre/trimmomatic/master/adapters/TruSeq3-SE.fa trim_cmd: ILLUMINACLIP:{}:2:40:15 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35 extra: '' Override default params by modifying these lines. In addition to changing parameters we've specifically enabled, you can modify the extra param to pass any extra trimmomatic parameters, e.g.: extra: '--someflag someparam --someotherflag thatotherparam' Or in Trimmomatic params: extra: 'HEADCROP:5' # to remove the first 5 bases at the front of the read. `` Be sure the modified lines go into the config file you're using to run `eelpond`. For more on what parameters are available, see the [Trimmomatic documentation](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf). ## Trimmomatic Rule We use a local copies of the [trimmomatic snakemake wrappers](https://snakemake-wrappers.readthedocs.io/en/stable/wrappers/trimmomatic.html) to run Trimmomatic. For snakemake afficionados, here's the basic structure of the trimmomatic eelpond rules. Directories and parameters are specified via the configfile (see the rule on [github](https://github.com/dib-lab/eelpond/blob/master/rules/trimmomatic/trimmomatic.rule)). def get_pretrim(w): readsD = {} if not is_single_end( w): readsD['r1'] = expand(join(data_dir, '{sample}_{unit}_1.fq.gz'), w) readsD['r2'] = expand(join(data_dir, '{sample}_{unit}_2.fq.gz'), w) return readsD return expand(join(data_dir, '{sample}_{unit}_1.fq.gz'), w) rule trimmomatic_pe: \"\"\" Trim reads from the sequencer by trimming or dropping low-quality reads. \"\"\" input: unpack(get_pretrim) output: r1=join(trim_dir, \"{sample} {unit}_1.trim.fq.gz\"), r2=join(trim_dir, \"{sample} {unit} 2.trim.fq.gz\"), r1_unpaired=join(trim_dir, \"{sample} {unit} 1.se.trim.fq.gz\"), r2_unpaired=join(trim_dir, \"{sample} {unit} 2.se.trim.fq.gz\"), message: \"\"\"--- Quality trimming PE read data with Trimmomatic.\"\"\" threads: trim_params.get('cpu', 16) params: trimmer = (trim_params['trim_cmd'].format(trim_params['adapter_file']['pe_name'])).split(' '), extra = '' log: join(LOGS_DIR, 'trimmomatic/{sample} {unit}_pe.log') wrapper: '0.27.1/bio/trimmomatic/pe' rule trimmomatic_se: \"\"\" Trim reads from the sequencer by trimming or dropping low-quality reads. \"\"\" input: get_pretrim output: r1=join(trim_dir, \"{sample} {unit}.se.trim.fq.gz\"), message: \"\"\"--- Quality trimming SE read data with Trimmomatic.\"\"\" threads: trim_params.get('cpu', 16) params: trimmer = (trim_params['trim_cmd'].format(trim_params['adapter_file']['se_name'])).split(' '), extra = '' log: join(LOGS_DIR, 'trimmomatic/{sample} {unit}_se.log') wrapper: '0.27.1/bio/trimmomatic/se' ```","title":"trimmomatic"},{"location":"trimmomatic/#trimmomatic","text":"","title":"Trimmomatic"},{"location":"trimmomatic/#quickstart-running-trimmomatic-with-eelpond","text":"./run_eelpond nema-test preprocess This will run trimmomatic trimming and fastqc on pre-trim and post-trim data.","title":"Quickstart: Running Trimmomatic with eelpond"},{"location":"trimmomatic/#trimmomatic_1","text":"We use Trimmomatic (version 0.36) to trim off residual Illumina adapters that were left behind after demultiplexing. Here we use a set TruSeq Illumina adapters. However, if running this on your own data and you know you have different adapters, you'll want to input them in the configfile (see params section, below). If you're using the right adapters, you should see that some of the reads are trimmed; if they\u2019re not, you won\u2019t see anything get trimmed. See excellent paper on trimming parameters by MacManes 2014 .","title":"Trimmomatic"},{"location":"trimmomatic/#trimmomatic-command","text":"Based on these recommendations by MacManes 2014, we use this command in this pipeline: TrimmomaticPE ${base}.fastq.gz ${baseR2}.fastq.gz \\ ${base}.qc.fq.gz s1_se \\ ${baseR2}.qc.fq.gz s2_se \\ ILLUMINACLIP:TruSeq3-PE.fa:2:40:15 \\ LEADING:2 TRAILING:2 \\ SLIDINGWINDOW:4:15 \\ MINLEN:25 However, the trimming command can be extensively modified via the configfile. Here's how the parameters for the command above look in our config: trimmomatic: trim_cmd: \"ILLUMINACLIP:{}:2:40:15 LEADING:2 TRAILING:2 SLIDINGWINDOW:4:15 MINLEN:25\" extra: ''","title":"Trimmomatic Command"},{"location":"trimmomatic/#customizing-the-trimming-parameters","text":"To modify any program params, you need to add a couple lines to the config file you provide to eelpond . To get a trimmomatic config you can modify, run: ./run_eelpond trimmomatic.yaml trimmomatic --build_config The output should be a small yaml configfile that contains: #################### trimmomatic #################### trimmomatic: adapter_file: pe_name: TruSeq3-PE.fa pe_url: https://raw.githubusercontent.com/timflutre/trimmomatic/master/adapters/TruSeq3-PE-2.fa se_name: TruSeq3-SE.fa se_url: https://raw.githubusercontent.com/timflutre/trimmomatic/master/adapters/TruSeq3-SE.fa trim_cmd: ILLUMINACLIP:{}:2:40:15 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35 extra: '' Override default params by modifying these lines. In addition to changing parameters we've specifically enabled, you can modify the extra param to pass any extra trimmomatic parameters, e.g.: extra: '--someflag someparam --someotherflag thatotherparam' Or in Trimmomatic params: extra: 'HEADCROP:5' # to remove the first 5 bases at the front of the read. `` Be sure the modified lines go into the config file you're using to run `eelpond`. For more on what parameters are available, see the [Trimmomatic documentation](http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/TrimmomaticManual_V0.32.pdf). ## Trimmomatic Rule We use a local copies of the [trimmomatic snakemake wrappers](https://snakemake-wrappers.readthedocs.io/en/stable/wrappers/trimmomatic.html) to run Trimmomatic. For snakemake afficionados, here's the basic structure of the trimmomatic eelpond rules. Directories and parameters are specified via the configfile (see the rule on [github](https://github.com/dib-lab/eelpond/blob/master/rules/trimmomatic/trimmomatic.rule)). def get_pretrim(w): readsD = {} if not is_single_end( w): readsD['r1'] = expand(join(data_dir, '{sample}_{unit}_1.fq.gz'), w) readsD['r2'] = expand(join(data_dir, '{sample}_{unit}_2.fq.gz'), w) return readsD return expand(join(data_dir, '{sample}_{unit}_1.fq.gz'), w) rule trimmomatic_pe: \"\"\" Trim reads from the sequencer by trimming or dropping low-quality reads. \"\"\" input: unpack(get_pretrim) output: r1=join(trim_dir, \"{sample} {unit}_1.trim.fq.gz\"), r2=join(trim_dir, \"{sample} {unit} 2.trim.fq.gz\"), r1_unpaired=join(trim_dir, \"{sample} {unit} 1.se.trim.fq.gz\"), r2_unpaired=join(trim_dir, \"{sample} {unit} 2.se.trim.fq.gz\"), message: \"\"\"--- Quality trimming PE read data with Trimmomatic.\"\"\" threads: trim_params.get('cpu', 16) params: trimmer = (trim_params['trim_cmd'].format(trim_params['adapter_file']['pe_name'])).split(' '), extra = '' log: join(LOGS_DIR, 'trimmomatic/{sample} {unit}_pe.log') wrapper: '0.27.1/bio/trimmomatic/pe' rule trimmomatic_se: \"\"\" Trim reads from the sequencer by trimming or dropping low-quality reads. \"\"\" input: get_pretrim output: r1=join(trim_dir, \"{sample} {unit}.se.trim.fq.gz\"), message: \"\"\"--- Quality trimming SE read data with Trimmomatic.\"\"\" threads: trim_params.get('cpu', 16) params: trimmer = (trim_params['trim_cmd'].format(trim_params['adapter_file']['se_name'])).split(' '), extra = '' log: join(LOGS_DIR, 'trimmomatic/{sample} {unit}_se.log') wrapper: '0.27.1/bio/trimmomatic/se' ```","title":"Customizing the trimming parameters"},{"location":"trinity/","text":"Trinity \u00b6 Quickstart: running trinity via eelpond: \u00b6 ./run_eelpond nema-test assemble This will run preprocessing and kmer-trimming for you prior to assembly. For more options, read below! Trinity \u00b6 The Eel Pond protocol uses the Trinity de novo transcriptome assembler to take short, trimmed/diginorm Illumina reads data and assemble (predict) full-length transcripts into a single fasta file output. Each contig in the fasta assembly file represents one unique transcript. Trinity is a single-ksize assembler, with a default of k = 25. We recommend using kmer-trimmed reads (output of khmer) as input into Triniity to reduce dataset complexity without losing valuable kmers. The resulting output assembly fasta file can then be used to align the trimmed (not diginorm) short Illumina reads and quantify expression per transcript. Note, the current version of Trininty (after 2.3.2) is configured to diginorm the input reads before assembly begins. Since we have already applied diginorm to our reads, the result will be a negligible decrease in read counts prior to the assembly. We provide options to disable this digital normalization via the config file, but applying diginorm twice is not really a problem. For data sets with large numbers of reads, applying diginorm as a separate step as we have via khmer may decrease the memory requirements needed by the Trinity pipeline. The ID for each transcript is output (version 2.2.0 to current) as follows, where the TRINITY is constant, the DN2202 is an example of a variable contig/transcript ID, c stands for component, g gene and i isoform: TRINITY_DN2202_c0_g1_i1 Trinity Command \u00b6 On the command line, the command eelpond runs is approximately: Trinity --left left.fq \\ --right right.fq --seqType fq --max_memory 10G \\ --CPU 4 But we highly recommend you modify max_memory and CPU to fit your data and compute resources. Customizing Trinity parameters \u00b6 To modify any program params, you need to add a couple lines to the config file you provide to eelpond . To get a trinity configfile you can modify, run: ./run_eelpond trinity.yaml trinity --build_config The output should be a small yaml configfile that contains: #################### trinity #################### trinity: input_kmer_trimmed: true input_trimmomatic_trimmed: false add_single_to_paired: false # would you like to add the orphaned reads to the trinity assembly? max_memory: 30G seqtype: fq extra: '' In addition to changing parameters we've specifically enabled, you can modify the extra param to pass any extra trinity parameters, e.g.: extra: '--someflag someparam --someotherflag thatotherparam' Or in Trinity params: extra: '--no_normalize_reads' # to turn off Trinity's digital normalization steps Override default params by modifying any of these lines, and placing them in the config file you're using to run eelpond . eelpond rule \u00b6 We wrote a Trinity snakemake wrapper to run Trinity. For snakemake afficionados, here's the basic structure of the trinity eelpond rule. Directories and parameters are specified via the configfile, for more, see the rule on github . rule trinity: input: unpack(get_assembly_input) output: fasta = join(assembly_dir,\"trinity_out_dir/Trinity.fasta\"), gene_trans_map = join(assembly_dir,\"trinity_out_dir/Trinity.fasta.gene_trans_map\"), message: \"\"\"--- Assembling read data with Trinity --- \"\"\" params: # optional parameters seqtype=assembly_params.get('seqtype', 'fq'), extra=assembly_params.get('extra', '') threads: 4 log: join(LOGS_DIR, 'trinity/trinity.log') conda: \"trinity-env.yaml\" script: \"trinity-wrapper.py\" rule rename_trinity_fasta: input: rules.trinity.output.fasta output: join(assembly_dir, BASE + assembly_extension + '.fasta') log: join(LOGS_DIR, 'trinity/cp_assembly.log') shell: (\"cp {input} {output}\") rule rename_trinity_gene_trans_map: input: rules.trinity.output.gene_trans_map output: join(assembly_dir, BASE + assembly_extension + '.fasta.gene_trans_map') log: join(LOGS_DIR, 'trinity/cp_gt_map.log') shell: (\"cp {input} {output}\")","title":"trinity"},{"location":"trinity/#trinity","text":"","title":"Trinity"},{"location":"trinity/#quickstart-running-trinity-via-eelpond","text":"./run_eelpond nema-test assemble This will run preprocessing and kmer-trimming for you prior to assembly. For more options, read below!","title":"Quickstart: running trinity via eelpond:"},{"location":"trinity/#trinity_1","text":"The Eel Pond protocol uses the Trinity de novo transcriptome assembler to take short, trimmed/diginorm Illumina reads data and assemble (predict) full-length transcripts into a single fasta file output. Each contig in the fasta assembly file represents one unique transcript. Trinity is a single-ksize assembler, with a default of k = 25. We recommend using kmer-trimmed reads (output of khmer) as input into Triniity to reduce dataset complexity without losing valuable kmers. The resulting output assembly fasta file can then be used to align the trimmed (not diginorm) short Illumina reads and quantify expression per transcript. Note, the current version of Trininty (after 2.3.2) is configured to diginorm the input reads before assembly begins. Since we have already applied diginorm to our reads, the result will be a negligible decrease in read counts prior to the assembly. We provide options to disable this digital normalization via the config file, but applying diginorm twice is not really a problem. For data sets with large numbers of reads, applying diginorm as a separate step as we have via khmer may decrease the memory requirements needed by the Trinity pipeline. The ID for each transcript is output (version 2.2.0 to current) as follows, where the TRINITY is constant, the DN2202 is an example of a variable contig/transcript ID, c stands for component, g gene and i isoform: TRINITY_DN2202_c0_g1_i1","title":"Trinity"},{"location":"trinity/#trinity-command","text":"On the command line, the command eelpond runs is approximately: Trinity --left left.fq \\ --right right.fq --seqType fq --max_memory 10G \\ --CPU 4 But we highly recommend you modify max_memory and CPU to fit your data and compute resources.","title":"Trinity Command"},{"location":"trinity/#customizing-trinity-parameters","text":"To modify any program params, you need to add a couple lines to the config file you provide to eelpond . To get a trinity configfile you can modify, run: ./run_eelpond trinity.yaml trinity --build_config The output should be a small yaml configfile that contains: #################### trinity #################### trinity: input_kmer_trimmed: true input_trimmomatic_trimmed: false add_single_to_paired: false # would you like to add the orphaned reads to the trinity assembly? max_memory: 30G seqtype: fq extra: '' In addition to changing parameters we've specifically enabled, you can modify the extra param to pass any extra trinity parameters, e.g.: extra: '--someflag someparam --someotherflag thatotherparam' Or in Trinity params: extra: '--no_normalize_reads' # to turn off Trinity's digital normalization steps Override default params by modifying any of these lines, and placing them in the config file you're using to run eelpond .","title":"Customizing Trinity parameters"},{"location":"trinity/#eelpond-rule","text":"We wrote a Trinity snakemake wrapper to run Trinity. For snakemake afficionados, here's the basic structure of the trinity eelpond rule. Directories and parameters are specified via the configfile, for more, see the rule on github . rule trinity: input: unpack(get_assembly_input) output: fasta = join(assembly_dir,\"trinity_out_dir/Trinity.fasta\"), gene_trans_map = join(assembly_dir,\"trinity_out_dir/Trinity.fasta.gene_trans_map\"), message: \"\"\"--- Assembling read data with Trinity --- \"\"\" params: # optional parameters seqtype=assembly_params.get('seqtype', 'fq'), extra=assembly_params.get('extra', '') threads: 4 log: join(LOGS_DIR, 'trinity/trinity.log') conda: \"trinity-env.yaml\" script: \"trinity-wrapper.py\" rule rename_trinity_fasta: input: rules.trinity.output.fasta output: join(assembly_dir, BASE + assembly_extension + '.fasta') log: join(LOGS_DIR, 'trinity/cp_assembly.log') shell: (\"cp {input} {output}\") rule rename_trinity_gene_trans_map: input: rules.trinity.output.gene_trans_map output: join(assembly_dir, BASE + assembly_extension + '.fasta.gene_trans_map') log: join(LOGS_DIR, 'trinity/cp_gt_map.log') shell: (\"cp {input} {output}\")","title":"eelpond rule"},{"location":"old_docs/Annotation/","text":"Annotating de novo transcriptomes with dammit \u00b6 dammit! dammit is an annotation pipeline written by Camille Scott . dammit runs a relatively standard annotation protocol for transcriptomes: it begins by building gene models with Transdecoder , and then uses the following protein databases as evidence for annotation: Pfam-A , Rfam , OrthoDB , uniref90 (uniref is optional with --full ). If a protein dataset is available, this can also be supplied to the dammit pipeline with --user-databases as optional evidence for annotation. In addition, BUSCO v3 is run, which will compare the gene content in your transcriptome with a lineage-specific data set. The output is a proportion of your transcriptome that matches with the data set, which can be used as an estimate of the completeness of your transcriptome based on evolutionary expectation ( Simho et al. 2015 ). There are several lineage-specific datasets available from the authors of BUSCO. We will use the metazoa dataset for this transcriptome. Database Preparation \u00b6 dammit has two major subcommands: dammit databases and dammit annotate . databases checks that the databases are installed and prepared, and if run with the --install flag, will perform that installation and preparation. If you just run dammit databases on its own, you should get a notification that some database tasks are not up-to-date -- we need to install them! Unless we're running short on time, we're going to do a full run. If you want to run a quick version of the pipeline, add a parameter, --quick , to omit OrthoDB, Uniref, Pfam, and Rfam. A \"full\" run will take longer to install and run, but you'll have access to the full annotation pipeline. dammit databases --install --busco-group metazoa # --quick We used the \"metazoa\" BUSCO group. We can use any of the BUSCO databases, so long as we install them with the dammit databases subcommand. You can see the whole list by running dammit databases -h . You should try to match your species as closely as possible for the best results. If we want to install another, for example: dammit databases --install --busco-group fungi # --quick Note: if you have limited space on your instance, you can also install these databases in a different location (e.g. on an external volume). You would want to run this command before running the database installs we just ran. #Run ONLY if you want to install databases in different location. #To run, remove the `#` from the front of the following command: # dammit databases --database-dir /path/to/databases Annotation \u00b6 Now we'll download a custom Nematostella vectensis protein database available from JGI. Here, somebody has already created a proper database for us [1] (it has a reference proteome available through uniprot). If your critter is a non-model organism, you will likely need to create your own with proteins from closely-related species. This will rely on your knowledge of your system! Run the command: dammit annotate trinity.nema.fasta --busco-group metazoa --user-databases nema.reference.prot.faa --n_threads 6 # --quick While dammit runs, it will print out which tasks its running to the terminal. dammit is written with a library called pydoit , which is a python workflow library similar to GNU Make. This not only helps organize the underlying workflow, but also means that if we interrupt it, it will properly resume! After a successful run, you'll have a new directory called trinity.nema.fasta.dammit . If you look inside, you'll see a lot of files: ls trinity.nema.fasta.dammit/ annotate.doit.db trinity.nema.fasta.dammit.namemap.csv trinity.nema.fasta.transdecoder.pep dammit.log trinity.nema.fasta.dammit.stats.json trinity.nema.fasta.x.nema.reference.prot.faa.crbl.csv run_trinity.nema.fasta.metazoa.busco.results trinity.nema.fasta.transdecoder.bed trinity.nema.fasta.x.nema.reference.prot.faa.crbl.gff3 tmp trinity.nema.fasta.transdecoder.cds trinity.nema.fasta.x.nema.reference.prot.faa.crbl.model.csv trinity.nema.fasta trinity.nema.fasta.transdecoder_dir trinity.nema.fasta.x.nema.reference.prot.faa.crbl.model.plot.pdf trinity.nema.fasta.dammit.fasta trinity.nema.fasta.transdecoder.gff3 trinity.nema.fasta.dammit.gff3 trinity.nema.fasta.transdecoder.mRNA The most important files for you are trinity.nema.fasta.dammit.fasta , trinity.nema.fasta.dammit.gff3 , and trinity.nema.fasta.dammit.stats.json . If the above dammit command is run again, there will be a message: **Pipeline is already completed!** Parse dammit output \u00b6 Cammille wrote dammit in Python, which includes a library to parse gff3 dammit output. To send this output to a useful table, we will need to open the Python environemnt. cd trinity.nema.fasta.dammit python Then, using this script, will output a list of gene ID: import pandas as pd from dammit.fileio.gff3 import GFF3Parser gff_file = \"trinity.nema.fasta.dammit.gff3\" annotations = GFF3Parser(filename=gff_file).read() names = annotations.sort_values(by=['seqid', 'score'], ascending=True).query('score < 1e-05').drop_duplicates(subset='seqid')[['seqid', 'Name']] new_file = names.dropna(axis=0,how='all') new_file.head() new_file.to_csv(\"nema_gene_name_id.csv\") exit() This will output a table of genes with 'seqid' and 'Name' in a .csv file: nema_gene_name_id.csv . Let's take a look at that file: less nema_gene_name_id.csv Notice there are multiple transcripts per gene model prediction. This .csv file can be used in tximport in downstream DE analysis. References \u00b6 Putnam NH, Srivastava M, Hellsten U, Dirks B, Chapman J, Salamov A, Terry A, Shapiro H, Lindquist E, Kapitonov VV, Jurka J, Genikhovich G, Grigoriev IV, Lucas SM, Steele RE, Finnerty JR, Technau U, Martindale MQ, Rokhsar DS. (2007) Sea anemone genome reveals ancestral eumetazoan gene repertoire and genomic organization. Science. 317, 86-94.","title":"Annotating de novo transcriptomes with dammit"},{"location":"old_docs/Annotation/#annotating-de-novo-transcriptomes-with-dammit","text":"dammit! dammit is an annotation pipeline written by Camille Scott . dammit runs a relatively standard annotation protocol for transcriptomes: it begins by building gene models with Transdecoder , and then uses the following protein databases as evidence for annotation: Pfam-A , Rfam , OrthoDB , uniref90 (uniref is optional with --full ). If a protein dataset is available, this can also be supplied to the dammit pipeline with --user-databases as optional evidence for annotation. In addition, BUSCO v3 is run, which will compare the gene content in your transcriptome with a lineage-specific data set. The output is a proportion of your transcriptome that matches with the data set, which can be used as an estimate of the completeness of your transcriptome based on evolutionary expectation ( Simho et al. 2015 ). There are several lineage-specific datasets available from the authors of BUSCO. We will use the metazoa dataset for this transcriptome.","title":"Annotating de novo transcriptomes with dammit"},{"location":"old_docs/Annotation/#database-preparation","text":"dammit has two major subcommands: dammit databases and dammit annotate . databases checks that the databases are installed and prepared, and if run with the --install flag, will perform that installation and preparation. If you just run dammit databases on its own, you should get a notification that some database tasks are not up-to-date -- we need to install them! Unless we're running short on time, we're going to do a full run. If you want to run a quick version of the pipeline, add a parameter, --quick , to omit OrthoDB, Uniref, Pfam, and Rfam. A \"full\" run will take longer to install and run, but you'll have access to the full annotation pipeline. dammit databases --install --busco-group metazoa # --quick We used the \"metazoa\" BUSCO group. We can use any of the BUSCO databases, so long as we install them with the dammit databases subcommand. You can see the whole list by running dammit databases -h . You should try to match your species as closely as possible for the best results. If we want to install another, for example: dammit databases --install --busco-group fungi # --quick Note: if you have limited space on your instance, you can also install these databases in a different location (e.g. on an external volume). You would want to run this command before running the database installs we just ran. #Run ONLY if you want to install databases in different location. #To run, remove the `#` from the front of the following command: # dammit databases --database-dir /path/to/databases","title":"Database Preparation"},{"location":"old_docs/Annotation/#annotation","text":"Now we'll download a custom Nematostella vectensis protein database available from JGI. Here, somebody has already created a proper database for us [1] (it has a reference proteome available through uniprot). If your critter is a non-model organism, you will likely need to create your own with proteins from closely-related species. This will rely on your knowledge of your system! Run the command: dammit annotate trinity.nema.fasta --busco-group metazoa --user-databases nema.reference.prot.faa --n_threads 6 # --quick While dammit runs, it will print out which tasks its running to the terminal. dammit is written with a library called pydoit , which is a python workflow library similar to GNU Make. This not only helps organize the underlying workflow, but also means that if we interrupt it, it will properly resume! After a successful run, you'll have a new directory called trinity.nema.fasta.dammit . If you look inside, you'll see a lot of files: ls trinity.nema.fasta.dammit/ annotate.doit.db trinity.nema.fasta.dammit.namemap.csv trinity.nema.fasta.transdecoder.pep dammit.log trinity.nema.fasta.dammit.stats.json trinity.nema.fasta.x.nema.reference.prot.faa.crbl.csv run_trinity.nema.fasta.metazoa.busco.results trinity.nema.fasta.transdecoder.bed trinity.nema.fasta.x.nema.reference.prot.faa.crbl.gff3 tmp trinity.nema.fasta.transdecoder.cds trinity.nema.fasta.x.nema.reference.prot.faa.crbl.model.csv trinity.nema.fasta trinity.nema.fasta.transdecoder_dir trinity.nema.fasta.x.nema.reference.prot.faa.crbl.model.plot.pdf trinity.nema.fasta.dammit.fasta trinity.nema.fasta.transdecoder.gff3 trinity.nema.fasta.dammit.gff3 trinity.nema.fasta.transdecoder.mRNA The most important files for you are trinity.nema.fasta.dammit.fasta , trinity.nema.fasta.dammit.gff3 , and trinity.nema.fasta.dammit.stats.json . If the above dammit command is run again, there will be a message: **Pipeline is already completed!**","title":"Annotation"},{"location":"old_docs/Annotation/#parse-dammit-output","text":"Cammille wrote dammit in Python, which includes a library to parse gff3 dammit output. To send this output to a useful table, we will need to open the Python environemnt. cd trinity.nema.fasta.dammit python Then, using this script, will output a list of gene ID: import pandas as pd from dammit.fileio.gff3 import GFF3Parser gff_file = \"trinity.nema.fasta.dammit.gff3\" annotations = GFF3Parser(filename=gff_file).read() names = annotations.sort_values(by=['seqid', 'score'], ascending=True).query('score < 1e-05').drop_duplicates(subset='seqid')[['seqid', 'Name']] new_file = names.dropna(axis=0,how='all') new_file.head() new_file.to_csv(\"nema_gene_name_id.csv\") exit() This will output a table of genes with 'seqid' and 'Name' in a .csv file: nema_gene_name_id.csv . Let's take a look at that file: less nema_gene_name_id.csv Notice there are multiple transcripts per gene model prediction. This .csv file can be used in tximport in downstream DE analysis.","title":"Parse dammit output"},{"location":"old_docs/Annotation/#references","text":"Putnam NH, Srivastava M, Hellsten U, Dirks B, Chapman J, Salamov A, Terry A, Shapiro H, Lindquist E, Kapitonov VV, Jurka J, Genikhovich G, Grigoriev IV, Lucas SM, Steele RE, Finnerty JR, Technau U, Martindale MQ, Rokhsar DS. (2007) Sea anemone genome reveals ancestral eumetazoan gene repertoire and genomic organization. Science. 317, 86-94.","title":"References"},{"location":"old_docs/Assembly/","text":"Transcriptome Assembly \u00b6 Kmer Trimming \u00b6 Before running transcriptome assembly, we recommend doing some kmer spectral error trimming on your dataset, and if you have lots of reads, also performing digital normalization. We use khmer for both of these tasks. You can choose whether or not to use khmer diginal normalization with --no-diginorm . Note that you can also conduct diginorm with the Trinity assembler. The commands are as follows: With digital normalizition: \" (interleave-reads.py {input.r1} {input.r2} && zcat {input.r1_orphan} {input.r2_orphan}) | \" \" (trim-low-abund.py -V -k {params.k} -Z {params.Z} -C {params.C} - -o - -M {params.memory} \" \" --diginorm --diginorm-coverage={params.cov}) | (extract-paired-reads.py --gzip \" \" -p {output.paired} -s {output.single}) > {log}; split-paired-reads.py {output.paired} \" \" -1 {output.r1_out} -2 {output.r2_out} >> {log}\" Without digital normalization: \" (interleave-reads.py {input.r1} {input.r2} && zcat {input.r1_orphan} {input.r2_orphan}) | \" \" (trim-low-abund.py -V -k {params.k} -Z {params.Z} -C {params.C} - -o - -M {params.memory} \" \"| (extract-paired-reads.py --gzip -p {output.paired} -s {output.single}) > {log}; \" split-paired-reads.py {output.paired} -1 {output.r1_out} -2 {output.r2_out} >> {log}\" Assembling with Trinity \u00b6 We use the Trinity de novo transcriptome assembler to take short, trimmed/diginorm Illumina reads data and assemble (predict) full-length transcripts into a single fasta file output. Each contig in the fasta assembly file represents one unique transcript. The default k -mer size for the Trinity assembler is k = 25. The resulting output assembly fasta file can then be used to align the original, trimmed (not diginorm) short Illumina reads and quantify expression per transcript. The ID for each transcript is output (version 2.2.0 to current) as follows, where the TRINITY is constant, the DN2202 is an example of a variable contig/transcript ID, c stands for component, g gene and i isoform: TRINITY_DN2202_c0_g1_i1 This snakemake pipeline will run the following command: Trinity --left left.fq \\ --right right.fq --seqType fq --max_memory 10G \\ --CPU 4 Note, the current version of Trininty (after 2.3.2) is configured to diginorm the input reads before assembly begins. Since we have already applied diginorm to our reads, the result will be a negligible decrease in read counts prior to the assembly. Applying diginorm twice is fine. For data sets with large numbers of reads, applying diginorm as a separate step as we have here may decrease the memory requirements needed by the Trinity pipeline.","title":"Transcriptome Assembly"},{"location":"old_docs/Assembly/#transcriptome-assembly","text":"","title":"Transcriptome Assembly"},{"location":"old_docs/Assembly/#kmer-trimming","text":"Before running transcriptome assembly, we recommend doing some kmer spectral error trimming on your dataset, and if you have lots of reads, also performing digital normalization. We use khmer for both of these tasks. You can choose whether or not to use khmer diginal normalization with --no-diginorm . Note that you can also conduct diginorm with the Trinity assembler. The commands are as follows: With digital normalizition: \" (interleave-reads.py {input.r1} {input.r2} && zcat {input.r1_orphan} {input.r2_orphan}) | \" \" (trim-low-abund.py -V -k {params.k} -Z {params.Z} -C {params.C} - -o - -M {params.memory} \" \" --diginorm --diginorm-coverage={params.cov}) | (extract-paired-reads.py --gzip \" \" -p {output.paired} -s {output.single}) > {log}; split-paired-reads.py {output.paired} \" \" -1 {output.r1_out} -2 {output.r2_out} >> {log}\" Without digital normalization: \" (interleave-reads.py {input.r1} {input.r2} && zcat {input.r1_orphan} {input.r2_orphan}) | \" \" (trim-low-abund.py -V -k {params.k} -Z {params.Z} -C {params.C} - -o - -M {params.memory} \" \"| (extract-paired-reads.py --gzip -p {output.paired} -s {output.single}) > {log}; \" split-paired-reads.py {output.paired} -1 {output.r1_out} -2 {output.r2_out} >> {log}\"","title":"Kmer Trimming"},{"location":"old_docs/Assembly/#assembling-with-trinity","text":"We use the Trinity de novo transcriptome assembler to take short, trimmed/diginorm Illumina reads data and assemble (predict) full-length transcripts into a single fasta file output. Each contig in the fasta assembly file represents one unique transcript. The default k -mer size for the Trinity assembler is k = 25. The resulting output assembly fasta file can then be used to align the original, trimmed (not diginorm) short Illumina reads and quantify expression per transcript. The ID for each transcript is output (version 2.2.0 to current) as follows, where the TRINITY is constant, the DN2202 is an example of a variable contig/transcript ID, c stands for component, g gene and i isoform: TRINITY_DN2202_c0_g1_i1 This snakemake pipeline will run the following command: Trinity --left left.fq \\ --right right.fq --seqType fq --max_memory 10G \\ --CPU 4 Note, the current version of Trininty (after 2.3.2) is configured to diginorm the input reads before assembly begins. Since we have already applied diginorm to our reads, the result will be a negligible decrease in read counts prior to the assembly. Applying diginorm twice is fine. For data sets with large numbers of reads, applying diginorm as a separate step as we have here may decrease the memory requirements needed by the Trinity pipeline.","title":"Assembling with Trinity"},{"location":"old_docs/DE/","text":"Differential expression analysis with DESeq2 \u00b6 Comparing gene expression differences in samples between experimental conditions. We will be using DESeq2 . References: Documentation for DESeq2 with example analysis Love et al. 2014 * Love et al. 2016 Additional links: DE lecture by Jane Khudyakov, July 2017 Example DE analysis from two populations of killifish! (Fundulus heteroclitus MDPL vs. MDPL) * A Review of Differential Gene Expression Software for mRNA sequencing RStudio! \u00b6 The pipeline will be running these commands in an R script. You could run them in R Studio: Load libraries library(DESeq2) library(\"lattice\") library(tximport) library(readr) library(gplots) library(RColorBrewer) source('~/plotPCAWithSampleNames.R') Tell RStudio where your files are and ask whether they exist: setwd(\"/mnt/work/quant/salmon_out/\") dir<-\"/mnt/work/quant/\" files_list = list.files() files <- file.path(dir, \"salmon_out\",files_list, \"quant.sf\") names(files) <- c(\"0Hour_1\",\"0Hour_2\",\"0Hour_3\",\"0Hour_4\",\"0Hour_5\",\"6Hour_1\",\"6Hour_2\",\"6Hour_3\",\"6Hour_4\",\"6Hour_5\") files print(file.exists(files)) Grab the gene names and transcript ID file to summarize expression at the gene level . tx2gene <- read.table(\"~/nema_transcript_gene_id.txt\",sep=\"\\t\") cols<-c(\"transcript_id\",\"gene_id\") colnames(tx2gene)<-cols head(tx2gene) txi.salmon <- tximport(files, type = \"salmon\", tx2gene = tx2gene,importer=read.delim) head(txi.salmon$counts) dim(txi.salmon$counts) Assign experimental variables: condition = factor(c(\"0Hour\",\"0Hour\",\"0Hour\",\"0Hour\",\"0Hour\",\"6Hour\",\"6Hour\",\"6Hour\",\"6Hour\",\"6Hour\")) ExpDesign <- data.frame(row.names=colnames(txi.salmon$counts), condition = condition) ExpDesign Run DESeq2: dds <- DESeqDataSetFromTximport(txi.salmon, ExpDesign, ~condition) dds <- DESeq(dds, betaPrior=FALSE) Get counts: counts_table = counts( dds, normalized=TRUE ) Filtering out low expression transcripts: See plot from Lisa Komoroske generated with RNAseq123 filtered_norm_counts<-counts_table[!rowSums(counts_table==0)>=1, ] filtered_norm_counts<-as.data.frame(filtered_norm_counts) GeneID<-rownames(filtered_norm_counts) filtered_norm_counts<-cbind(filtered_norm_counts,GeneID) dim(filtered_norm_counts) head(filtered_norm_counts) Estimate dispersion: plotDispEsts(dds) PCA: log_dds<-rlog(dds) plotPCAWithSampleNames(log_dds, intgroup=\"condition\", ntop=40000) Get DE results: res<-results(dds,contrast=c(\"condition\",\"6Hour\",\"0Hour\")) head(res) res_ordered<-res[order(res$padj),] GeneID<-rownames(res_ordered) res_ordered<-as.data.frame(res_ordered) res_genes<-cbind(res_ordered,GeneID) dim(res_genes) head(res_genes) dim(res_genes) res_genes_merged <- merge(res_genes,filtered_norm_counts,by=unique(\"GeneID\")) dim(res_genes_merged) head(res_genes_merged) res_ordered<-res_genes_merged[order(res_genes_merged$padj),] write.csv(res_ordered, file=\"nema_DESeq_all.csv\" ) Set a threshold cutoff of padj<0.05 and \u00b1 log2FC 1: resSig = res_ordered[res_ordered$padj < 0.05, ] resSig = resSig[resSig$log2FoldChange > 1 | resSig$log2FoldChange < -1,] write.csv(resSig,file=\"nema_DESeq_padj0.05_log2FC1.csv\") MA plot with gene names: plot(log2(res_ordered$baseMean), res_ordered$log2FoldChange, col=ifelse(res_ordered$padj < 0.05, \"red\",\"gray67\"),main=\"nema (padj<0.05, log2FC = \u00b11)\",xlim=c(1,20),pch=20,cex=1,ylim=c(-12,12)) abline(h=c(-1,1), col=\"blue\") genes<-resSig$GeneID mygenes <- resSig[,] baseMean_mygenes <- mygenes[,\"baseMean\"] log2FoldChange_mygenes <- mygenes[,\"log2FoldChange\"] text(log2(baseMean_mygenes),log2FoldChange_mygenes,labels=genes,pos=2,cex=0.60) Heatmap d<-resSig dim(d) head(d) colnames(d) d<-d[,c(8:17)] d<-as.matrix(d) d<-as.data.frame(d) d<-as.matrix(d) rownames(d) <- resSig[,1] head(d) hr <- hclust(as.dist(1-cor(t(d), method=\"pearson\")), method=\"complete\") mycl <- cutree(hr, h=max(hr$height/1.5)) clusterCols <- rainbow(length(unique(mycl))) myClusterSideBar <- clusterCols[mycl] myheatcol <- greenred(75) heatmap.2(d, main=\"nema (padj<0.05, log2FC = \u00b11)\", Rowv=as.dendrogram(hr), cexRow=0.75,cexCol=0.8,srtCol= 90, adjCol = c(NA,0),offsetCol=2.5, Colv=NA, dendrogram=\"row\", scale=\"row\", col=myheatcol, density.info=\"none\", trace=\"none\", RowSideColors= myClusterSideBar)","title":"Differential expression analysis with DESeq2"},{"location":"old_docs/DE/#differential-expression-analysis-with-deseq2","text":"Comparing gene expression differences in samples between experimental conditions. We will be using DESeq2 . References: Documentation for DESeq2 with example analysis Love et al. 2014 * Love et al. 2016 Additional links: DE lecture by Jane Khudyakov, July 2017 Example DE analysis from two populations of killifish! (Fundulus heteroclitus MDPL vs. MDPL) * A Review of Differential Gene Expression Software for mRNA sequencing","title":"Differential expression analysis with DESeq2"},{"location":"old_docs/DE/#rstudio","text":"The pipeline will be running these commands in an R script. You could run them in R Studio: Load libraries library(DESeq2) library(\"lattice\") library(tximport) library(readr) library(gplots) library(RColorBrewer) source('~/plotPCAWithSampleNames.R') Tell RStudio where your files are and ask whether they exist: setwd(\"/mnt/work/quant/salmon_out/\") dir<-\"/mnt/work/quant/\" files_list = list.files() files <- file.path(dir, \"salmon_out\",files_list, \"quant.sf\") names(files) <- c(\"0Hour_1\",\"0Hour_2\",\"0Hour_3\",\"0Hour_4\",\"0Hour_5\",\"6Hour_1\",\"6Hour_2\",\"6Hour_3\",\"6Hour_4\",\"6Hour_5\") files print(file.exists(files)) Grab the gene names and transcript ID file to summarize expression at the gene level . tx2gene <- read.table(\"~/nema_transcript_gene_id.txt\",sep=\"\\t\") cols<-c(\"transcript_id\",\"gene_id\") colnames(tx2gene)<-cols head(tx2gene) txi.salmon <- tximport(files, type = \"salmon\", tx2gene = tx2gene,importer=read.delim) head(txi.salmon$counts) dim(txi.salmon$counts) Assign experimental variables: condition = factor(c(\"0Hour\",\"0Hour\",\"0Hour\",\"0Hour\",\"0Hour\",\"6Hour\",\"6Hour\",\"6Hour\",\"6Hour\",\"6Hour\")) ExpDesign <- data.frame(row.names=colnames(txi.salmon$counts), condition = condition) ExpDesign Run DESeq2: dds <- DESeqDataSetFromTximport(txi.salmon, ExpDesign, ~condition) dds <- DESeq(dds, betaPrior=FALSE) Get counts: counts_table = counts( dds, normalized=TRUE ) Filtering out low expression transcripts: See plot from Lisa Komoroske generated with RNAseq123 filtered_norm_counts<-counts_table[!rowSums(counts_table==0)>=1, ] filtered_norm_counts<-as.data.frame(filtered_norm_counts) GeneID<-rownames(filtered_norm_counts) filtered_norm_counts<-cbind(filtered_norm_counts,GeneID) dim(filtered_norm_counts) head(filtered_norm_counts) Estimate dispersion: plotDispEsts(dds) PCA: log_dds<-rlog(dds) plotPCAWithSampleNames(log_dds, intgroup=\"condition\", ntop=40000) Get DE results: res<-results(dds,contrast=c(\"condition\",\"6Hour\",\"0Hour\")) head(res) res_ordered<-res[order(res$padj),] GeneID<-rownames(res_ordered) res_ordered<-as.data.frame(res_ordered) res_genes<-cbind(res_ordered,GeneID) dim(res_genes) head(res_genes) dim(res_genes) res_genes_merged <- merge(res_genes,filtered_norm_counts,by=unique(\"GeneID\")) dim(res_genes_merged) head(res_genes_merged) res_ordered<-res_genes_merged[order(res_genes_merged$padj),] write.csv(res_ordered, file=\"nema_DESeq_all.csv\" ) Set a threshold cutoff of padj<0.05 and \u00b1 log2FC 1: resSig = res_ordered[res_ordered$padj < 0.05, ] resSig = resSig[resSig$log2FoldChange > 1 | resSig$log2FoldChange < -1,] write.csv(resSig,file=\"nema_DESeq_padj0.05_log2FC1.csv\") MA plot with gene names: plot(log2(res_ordered$baseMean), res_ordered$log2FoldChange, col=ifelse(res_ordered$padj < 0.05, \"red\",\"gray67\"),main=\"nema (padj<0.05, log2FC = \u00b11)\",xlim=c(1,20),pch=20,cex=1,ylim=c(-12,12)) abline(h=c(-1,1), col=\"blue\") genes<-resSig$GeneID mygenes <- resSig[,] baseMean_mygenes <- mygenes[,\"baseMean\"] log2FoldChange_mygenes <- mygenes[,\"log2FoldChange\"] text(log2(baseMean_mygenes),log2FoldChange_mygenes,labels=genes,pos=2,cex=0.60) Heatmap d<-resSig dim(d) head(d) colnames(d) d<-d[,c(8:17)] d<-as.matrix(d) d<-as.data.frame(d) d<-as.matrix(d) rownames(d) <- resSig[,1] head(d) hr <- hclust(as.dist(1-cor(t(d), method=\"pearson\")), method=\"complete\") mycl <- cutree(hr, h=max(hr$height/1.5)) clusterCols <- rainbow(length(unique(mycl))) myClusterSideBar <- clusterCols[mycl] myheatcol <- greenred(75) heatmap.2(d, main=\"nema (padj<0.05, log2FC = \u00b11)\", Rowv=as.dendrogram(hr), cexRow=0.75,cexCol=0.8,srtCol= 90, adjCol = c(NA,0),offsetCol=2.5, Colv=NA, dendrogram=\"row\", scale=\"row\", col=myheatcol, density.info=\"none\", trace=\"none\", RowSideColors= myClusterSideBar)","title":"RStudio!"},{"location":"old_docs/Diginorm/","text":"Digital Normalization \u00b6 In this section, we\u2019ll apply digital normalization and variable-coverage k-mer abundance trimming to the reads prior to assembly using the khmer software package (version 2.1). This has the effect of reducing the computational cost of assembly without negatively affecting the quality of the assembly. This is all run in one command, taking the trimmed reads as input, uses the orphaned reads that survived while their mated pair did not during adapter and quality trimming. Then, low-abundance reads are trimmed to a coverage of 18 and normalized to a k -mer ( k = 20) coverage of 20. (interleave-reads.py {}{}.trim_1P.fq {}{}.trim_2P.fq && zcat {}orphans.fq.gz)| \\\\ (trim-low-abund.py -V -k 20 -Z 18 -C 2 - -o - -M 4e9 --diginorm --diginorm-coverage=20) | \\\\ (extract-paired-reads.py --gzip -p {}{}.paired.gz -s {}{}.single.gz) > /dev/null The output files are the remaining reads, grouped as pairs and singles (orphans). Since the Trinity de novo assembly software expects paired reads, we will split them into left left.fq and right.fq read pair files, including the single orphans in the left.fq file. for file in *.paired.gz do split-paired-reads.py ${file} done cat *.1 > left.fq cat *.2 > right.fq gunzip -c ../diginorm/single.gz >> left.fq","title":"Digital Normalization"},{"location":"old_docs/Diginorm/#digital-normalization","text":"In this section, we\u2019ll apply digital normalization and variable-coverage k-mer abundance trimming to the reads prior to assembly using the khmer software package (version 2.1). This has the effect of reducing the computational cost of assembly without negatively affecting the quality of the assembly. This is all run in one command, taking the trimmed reads as input, uses the orphaned reads that survived while their mated pair did not during adapter and quality trimming. Then, low-abundance reads are trimmed to a coverage of 18 and normalized to a k -mer ( k = 20) coverage of 20. (interleave-reads.py {}{}.trim_1P.fq {}{}.trim_2P.fq && zcat {}orphans.fq.gz)| \\\\ (trim-low-abund.py -V -k 20 -Z 18 -C 2 - -o - -M 4e9 --diginorm --diginorm-coverage=20) | \\\\ (extract-paired-reads.py --gzip -p {}{}.paired.gz -s {}{}.single.gz) > /dev/null The output files are the remaining reads, grouped as pairs and singles (orphans). Since the Trinity de novo assembly software expects paired reads, we will split them into left left.fq and right.fq read pair files, including the single orphans in the left.fq file. for file in *.paired.gz do split-paired-reads.py ${file} done cat *.1 > left.fq cat *.2 > right.fq gunzip -c ../diginorm/single.gz >> left.fq","title":"Digital Normalization"},{"location":"old_docs/QC/","text":"Short read quality and trimming \u00b6 The first step of any sequencing pipeline is to assess read quality and perform quality control as necessary. We natively enable quality assessment with FastQC and multiqc , and do quality trimming with Trimmomatic . Quality Assessment \u00b6 We\u2019re going to use FastQC (version 0.11.5) and multiqc (version 1.2) to summarize the data before and after adapter trimming. There are several caveats about FastQC - the main one is that it only calculates certain statistics (like duplicated sequences) for subsets of the data (e.g. duplicate sequences are only analyzed for the first 100,000 sequences in each file. Multiqc will summarize individual fastqc output into one output so that you can see all quality information for all files simultaenously. Adapter trim each pair of files \u00b6 We use Trimmomatic (version 0.36) to trim off residual Illumina adapters that were left behind after demultiplexing. This pipeline assumes TruSeq3-PE.fa adapters. However, if running this on your own data, you\u2019ll need to know which Illumina sequencing adapters were used for your library prep in order to trim them off. If they are the right adapters, you should see that some of the reads are trimmed; if they\u2019re not, you won\u2019t see anything get trimmed. See excellent paper on trimming parameters by MacManes 2014 . Based on these recommendations by MacManes 2014, we use this command in this pipeline: TrimmomaticPE ${base}.fastq.gz ${baseR2}.fastq.gz \\ ${base}.qc.fq.gz s1_se \\ ${baseR2}.qc.fq.gz s2_se \\ ILLUMINACLIP:TruSeq3-PE.fa:2:40:15 \\ LEADING:2 TRAILING:2 \\ SLIDINGWINDOW:4:2 \\ MINLEN:25 Customizing the trimming pipeline \u00b6 The default trimming paramters can be overridden by providing the following in the .yaml configuration file: trimmomatic: trim_cmd: \"ILLUMINACLIP:{}:2:40:15 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35\" Be sure to modify the trimming commands as desired","title":"Short read quality and trimming"},{"location":"old_docs/QC/#short-read-quality-and-trimming","text":"The first step of any sequencing pipeline is to assess read quality and perform quality control as necessary. We natively enable quality assessment with FastQC and multiqc , and do quality trimming with Trimmomatic .","title":"Short read quality and trimming"},{"location":"old_docs/QC/#quality-assessment","text":"We\u2019re going to use FastQC (version 0.11.5) and multiqc (version 1.2) to summarize the data before and after adapter trimming. There are several caveats about FastQC - the main one is that it only calculates certain statistics (like duplicated sequences) for subsets of the data (e.g. duplicate sequences are only analyzed for the first 100,000 sequences in each file. Multiqc will summarize individual fastqc output into one output so that you can see all quality information for all files simultaenously.","title":"Quality Assessment"},{"location":"old_docs/QC/#adapter-trim-each-pair-of-files","text":"We use Trimmomatic (version 0.36) to trim off residual Illumina adapters that were left behind after demultiplexing. This pipeline assumes TruSeq3-PE.fa adapters. However, if running this on your own data, you\u2019ll need to know which Illumina sequencing adapters were used for your library prep in order to trim them off. If they are the right adapters, you should see that some of the reads are trimmed; if they\u2019re not, you won\u2019t see anything get trimmed. See excellent paper on trimming parameters by MacManes 2014 . Based on these recommendations by MacManes 2014, we use this command in this pipeline: TrimmomaticPE ${base}.fastq.gz ${baseR2}.fastq.gz \\ ${base}.qc.fq.gz s1_se \\ ${baseR2}.qc.fq.gz s2_se \\ ILLUMINACLIP:TruSeq3-PE.fa:2:40:15 \\ LEADING:2 TRAILING:2 \\ SLIDINGWINDOW:4:2 \\ MINLEN:25","title":"Adapter trim each pair of files"},{"location":"old_docs/QC/#customizing-the-trimming-pipeline","text":"The default trimming paramters can be overridden by providing the following in the .yaml configuration file: trimmomatic: trim_cmd: \"ILLUMINACLIP:{}:2:40:15 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35\" Be sure to modify the trimming commands as desired","title":"Customizing the trimming pipeline"},{"location":"old_docs/Quality/","text":"Evaluating your transcriptome assembly \u00b6 We will be using Transrate and BUSCO! BUSCO \u00b6 B enchmarking U niversal S ingle C opy O rthologs (BUSCO) Eukaryota database has 303 genes Metazoa database has 978 genes \"Complete\" lengths are within two standard deviations of the BUSCO group mean length Genes that make up the BUSCO sets for each major lineage are selected from orthologous groups with genes present as single-copy orthologs in at least 90% of the species. Useful links: Website with additional busco databases: http://busco.ezlab.org/ Paper: Simao et al. 2015 User Guide Command: run_BUSCO.py \\ -i Trinity.fixed.fasta \\ -o nema_busco_metazoa -l ~/busco/metazoa_odb9 \\ -m transcriptome --cpu 2 Transrate \u00b6 Transrate serves two main purposes. It can compare two assemblies to see how similar they are. Or, it can give you a score which represents proportion of input reads that provide positive support for the assembly. We will use transrate to get a score for the assembly. Use the trimmed reads. For a further explanation of metrics and how to run the reference-based transrate, see the documentation and the paper by Smith-Unna et al. 2016 . How do two transcriptomes compare with each other? transrate --reference=Trinity.fixed.fasta --assembly=trinity-nematostella-raw.fa --output=full_v_subset transrate --reference=trinity-nematostella-raw.fa --assembly=Trinity.fixed.fasta --output=subset_v_full","title":"Evaluating your transcriptome assembly"},{"location":"old_docs/Quality/#evaluating-your-transcriptome-assembly","text":"We will be using Transrate and BUSCO!","title":"Evaluating your transcriptome assembly"},{"location":"old_docs/Quality/#busco","text":"B enchmarking U niversal S ingle C opy O rthologs (BUSCO) Eukaryota database has 303 genes Metazoa database has 978 genes \"Complete\" lengths are within two standard deviations of the BUSCO group mean length Genes that make up the BUSCO sets for each major lineage are selected from orthologous groups with genes present as single-copy orthologs in at least 90% of the species. Useful links: Website with additional busco databases: http://busco.ezlab.org/ Paper: Simao et al. 2015 User Guide Command: run_BUSCO.py \\ -i Trinity.fixed.fasta \\ -o nema_busco_metazoa -l ~/busco/metazoa_odb9 \\ -m transcriptome --cpu 2","title":"BUSCO"},{"location":"old_docs/Quality/#transrate","text":"Transrate serves two main purposes. It can compare two assemblies to see how similar they are. Or, it can give you a score which represents proportion of input reads that provide positive support for the assembly. We will use transrate to get a score for the assembly. Use the trimmed reads. For a further explanation of metrics and how to run the reference-based transrate, see the documentation and the paper by Smith-Unna et al. 2016 . How do two transcriptomes compare with each other? transrate --reference=Trinity.fixed.fasta --assembly=trinity-nematostella-raw.fa --output=full_v_subset transrate --reference=trinity-nematostella-raw.fa --assembly=Trinity.fixed.fasta --output=subset_v_full","title":"Transrate"},{"location":"old_docs/Quant/","text":"Quantification with Salmon \u00b6 We will use Salmon to quantify expression. Salmon is a new breed of software for quantifying RNAseq reads that is both really fast and takes transcript length into consideration ( Patro et al. 2015 ). For further reading, see Intro blog post: http://robpatro.com/blog/?p=248 A 2016 blog post evaluating and comparing methods here Salmon github repo here https://github.com/ngs-docs/2015-nov-adv-rna/blob/master/salmon.rst http://angus.readthedocs.io/en/2016/rob_quant/tut.html https://2016-aug-nonmodel-rnaseq.readthedocs.io/en/latest/quantification.html The two most interesting files are salmon_quant.log and quant.sf . The latter contains the counts; the former contains the log information from running things. We recommend quantifying using the Trinity transcriptome assembly fasta file, which will give expression values for each contig, like this in quant.sf : Name Length EffectiveLength TPM NumReads TRINITY_DN2202_c0_g1_i1 210 39.818 2.683835 2.000000 TRINITY_DN2270_c0_g1_i1 213 41.064 0.000000 0.000000 TRINITY_DN2201_c0_g1_i1 266 69.681 0.766816 1.000000 TRINITY_DN2222_c0_g1_i1 243 55.794 2.873014 3.000000 TRINITY_DN2291_c0_g1_i1 245 56.916 0.000000 0.000000 TRINITY_DN2269_c0_g1_i1 294 89.251 0.000000 0.000000 TRINITY_DN2269_c1_g1_i1 246 57.479 0.000000 0.000000 TRINITY_DN2279_c0_g1_i1 426 207.443 0.000000 0.000000 TRINITY_DN2262_c0_g1_i1 500 280.803 0.190459 1.000912 TRINITY_DN2253_c0_g1_i1 1523 1303.116 0.164015 4.000000 TRINITY_DN2287_c0_g1_i1 467 247.962 0.000000 0.000000 TRINITY_DN2287_c1_g1_i1 325 113.826 0.469425 1.000000 TRINITY_DN2237_c0_g1_i1 306 98.441 0.542788 1.000000 TRINITY_DN2237_c0_g2_i1 307 99.229 0.000000 0.000000 TRINITY_DN2250_c0_g1_i1 368 151.832 0.000000 0.000000 TRINITY_DN2250_c1_g1_i1 271 72.988 0.000000 0.000000 TRINITY_DN2208_c0_g1_i1 379 162.080 1.978014 6.000000 TRINITY_DN2277_c0_g1_i1 269 71.657 0.745677 1.000000 TRINITY_DN2231_c0_g1_i1 209 39.409 0.000000 0.000000 TRINITY_DN2231_c1_g1_i1 334 121.411 0.000000 0.000000 TRINITY_DN2204_c0_g1_i1 287 84.121 0.000000 0.000000 There are two commands for salmon, salmon index and salmon quant . The first command, salmon index will index the transcriptome: salmon index --index nema --transcripts trinity.nema.full.fasta --type quasi And the second command, salmon quant will quantify the trimmed reads (not diginormed) using the transcriptome: for R1 in *R1*.fastq.gz do sample=$(basename $R1 extract.fastq.gz) echo sample is $sample, R1 is $R1 R2=${R1/R1/R2} echo R2 is $R2 salmon quant -i nema -p 2 -l IU -1 <(gunzip -c $R1) -2 <(gunzip -c $R2) -o ${sample}quant done","title":"Quantification with Salmon"},{"location":"old_docs/Quant/#quantification-with-salmon","text":"We will use Salmon to quantify expression. Salmon is a new breed of software for quantifying RNAseq reads that is both really fast and takes transcript length into consideration ( Patro et al. 2015 ). For further reading, see Intro blog post: http://robpatro.com/blog/?p=248 A 2016 blog post evaluating and comparing methods here Salmon github repo here https://github.com/ngs-docs/2015-nov-adv-rna/blob/master/salmon.rst http://angus.readthedocs.io/en/2016/rob_quant/tut.html https://2016-aug-nonmodel-rnaseq.readthedocs.io/en/latest/quantification.html The two most interesting files are salmon_quant.log and quant.sf . The latter contains the counts; the former contains the log information from running things. We recommend quantifying using the Trinity transcriptome assembly fasta file, which will give expression values for each contig, like this in quant.sf : Name Length EffectiveLength TPM NumReads TRINITY_DN2202_c0_g1_i1 210 39.818 2.683835 2.000000 TRINITY_DN2270_c0_g1_i1 213 41.064 0.000000 0.000000 TRINITY_DN2201_c0_g1_i1 266 69.681 0.766816 1.000000 TRINITY_DN2222_c0_g1_i1 243 55.794 2.873014 3.000000 TRINITY_DN2291_c0_g1_i1 245 56.916 0.000000 0.000000 TRINITY_DN2269_c0_g1_i1 294 89.251 0.000000 0.000000 TRINITY_DN2269_c1_g1_i1 246 57.479 0.000000 0.000000 TRINITY_DN2279_c0_g1_i1 426 207.443 0.000000 0.000000 TRINITY_DN2262_c0_g1_i1 500 280.803 0.190459 1.000912 TRINITY_DN2253_c0_g1_i1 1523 1303.116 0.164015 4.000000 TRINITY_DN2287_c0_g1_i1 467 247.962 0.000000 0.000000 TRINITY_DN2287_c1_g1_i1 325 113.826 0.469425 1.000000 TRINITY_DN2237_c0_g1_i1 306 98.441 0.542788 1.000000 TRINITY_DN2237_c0_g2_i1 307 99.229 0.000000 0.000000 TRINITY_DN2250_c0_g1_i1 368 151.832 0.000000 0.000000 TRINITY_DN2250_c1_g1_i1 271 72.988 0.000000 0.000000 TRINITY_DN2208_c0_g1_i1 379 162.080 1.978014 6.000000 TRINITY_DN2277_c0_g1_i1 269 71.657 0.745677 1.000000 TRINITY_DN2231_c0_g1_i1 209 39.409 0.000000 0.000000 TRINITY_DN2231_c1_g1_i1 334 121.411 0.000000 0.000000 TRINITY_DN2204_c0_g1_i1 287 84.121 0.000000 0.000000 There are two commands for salmon, salmon index and salmon quant . The first command, salmon index will index the transcriptome: salmon index --index nema --transcripts trinity.nema.full.fasta --type quasi And the second command, salmon quant will quantify the trimmed reads (not diginormed) using the transcriptome: for R1 in *R1*.fastq.gz do sample=$(basename $R1 extract.fastq.gz) echo sample is $sample, R1 is $R1 R2=${R1/R1/R2} echo R2 is $R2 salmon quant -i nema -p 2 -l IU -1 <(gunzip -c $R1) -2 <(gunzip -c $R2) -o ${sample}quant done","title":"Quantification with Salmon"}]}