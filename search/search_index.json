{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"EelPond \u00b6 Snakemake update of the Eel Pond Protocol for de novo RNAseq analysis \u00b6 ___ .-' `'. / \\ | ; | | ___.--, _.._ |O) ~ (O) | _.---'`__.-( (_. __.--'`_.. '.__.\\ '--. \\_.-' ,.--'` `\"\"` ( ,.--'` ',__ /./; ;, '.__.'` __ _`) ) .---.__.' / | |\\ \\__..--\"\" \"\"\"--.,_ `---' .'.''-._.-'`_./ /\\ '. \\_.-~~~````~~~-.__`-.__.' | | .' _.-' | | \\ \\ '. \\ \\/ .' \\ \\ '. '-._) \\/ / \\ \\ `=.__`-~-. / /\\ `) ) / / `\"\".`\\ , _.-'.'\\ \\ / / ( ( / / `--~` ) ) .-' .' '.'. | ( (/` ( (` ) ) `-; ` '--; (' This is a lightweight protocol for assembling up to a few hundred million mRNAseq reads, annotating the resulting assembly, and doing differential expression analysis. The input is short-insert paired-end Illumina reads. This protocol can be run in a single command because it uses the snakemake automated workflow management system. Previous versions of this protocol included line-by-line commands that the user could follow along with using a test dataset provided in the instructions. Since the recent development of snakemake workflow management tool and snakemake-wrappers to manage sofware installation of commonly-used bioinformatics tools, we have re-implemented the Eel Pond Protocol to make it easier for users to install software and run a de novo transcriptome assembly, annotation, and quick differential expression analysis on a set of short-read Illumina data using a single command. The software for this protocol can be found here . Getting Started \u00b6 At the moment, only Linux is supported. OSX issues: - fastqc fails about half the time - Trinity assembler does not work Install miniconda (for Ubuntu 16.04 Jetstream image ): wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh -b echo export PATH=\"$HOME/miniconda3/bin:$PATH\" >> ~/.bash_profile source ~/.bash_profile Now, get the eelpond code git clone https://github.com/dib-lab/eelpond.git cd eelpond Create a conda environment with all the dependencies for eelpond conda env create --file ep_utils/eelpond_environment.yaml -n eelpond Activate that environment. You'll need to do this anytime you want to run eelpond source activate eelpond Now you can start running eelpond! To test a \"full\" workflow, consisting of read pre-processing, kmer trimming, Trinity assembly, dammit annotation and salmon quantification: ./run_eelpond nema-test full These will run a small set of Nematostella vectensis test data (from Tulin et al., 2013 ) You can also run individual tools or subworkflows independently: ./run_eelpond nema-test preprocess ./run_eelpond nema-test trimmomatic See the help, here: ./run_eelpond -h Running your own data: To run your own data, you'll need to create two files, a tsv file containing your sample info, and a yaml file containing basic configuration info. To start, copy the test data files so you can modify them. cp nema_samples.tsv <my-tsv-name.tsv> Next, build a configfile to edit: ./run_eelpond config_name --build_config This configfile will contain all the default paramters for each step of the pipeline you target. If you don't specify any targets, it will default to the \"full\" pipeline, which executes read preprocessing, assembly, annotation, and quantification. Then, modify this configfile as necessary. The essential component is the samples.tsv file, which points eelpond to your sample files. References: original eel-pond protocol docs, last updated 2015 eel-pond protocol docs, last updated 2016 DIBSI, nonmodel RNAseq workshop, July 2017 SIO-BUG, nonmodel RNAseq workshop, October 2017 available workflows: preprocess: Read Quality Trimming and Filtering (fastqc, trimmomatic) kmer_trim: Kmer Trimming and/or Digital Normalization (khmer) assemble: Transcriptome Assembly (trinity) assemblyinput: Specify assembly for downstream steps annotate : Annotate the transcriptome (dammit, sourmash) quantify: Quantify transcripts (salmon) full: preprocess, kmer_trim, assemble, annotate, quantify","title":"About"},{"location":"#eelpond","text":"","title":"EelPond"},{"location":"#snakemake-update-of-the-eel-pond-protocol-for-de-novo-rnaseq-analysis","text":"___ .-' `'. / \\ | ; | | ___.--, _.._ |O) ~ (O) | _.---'`__.-( (_. __.--'`_.. '.__.\\ '--. \\_.-' ,.--'` `\"\"` ( ,.--'` ',__ /./; ;, '.__.'` __ _`) ) .---.__.' / | |\\ \\__..--\"\" \"\"\"--.,_ `---' .'.''-._.-'`_./ /\\ '. \\_.-~~~````~~~-.__`-.__.' | | .' _.-' | | \\ \\ '. \\ \\/ .' \\ \\ '. '-._) \\/ / \\ \\ `=.__`-~-. / /\\ `) ) / / `\"\".`\\ , _.-'.'\\ \\ / / ( ( / / `--~` ) ) .-' .' '.'. | ( (/` ( (` ) ) `-; ` '--; (' This is a lightweight protocol for assembling up to a few hundred million mRNAseq reads, annotating the resulting assembly, and doing differential expression analysis. The input is short-insert paired-end Illumina reads. This protocol can be run in a single command because it uses the snakemake automated workflow management system. Previous versions of this protocol included line-by-line commands that the user could follow along with using a test dataset provided in the instructions. Since the recent development of snakemake workflow management tool and snakemake-wrappers to manage sofware installation of commonly-used bioinformatics tools, we have re-implemented the Eel Pond Protocol to make it easier for users to install software and run a de novo transcriptome assembly, annotation, and quick differential expression analysis on a set of short-read Illumina data using a single command. The software for this protocol can be found here .","title":"Snakemake update of the Eel Pond Protocol for de novo RNAseq analysis"},{"location":"#getting-started","text":"At the moment, only Linux is supported. OSX issues: - fastqc fails about half the time - Trinity assembler does not work Install miniconda (for Ubuntu 16.04 Jetstream image ): wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh -b echo export PATH=\"$HOME/miniconda3/bin:$PATH\" >> ~/.bash_profile source ~/.bash_profile Now, get the eelpond code git clone https://github.com/dib-lab/eelpond.git cd eelpond Create a conda environment with all the dependencies for eelpond conda env create --file ep_utils/eelpond_environment.yaml -n eelpond Activate that environment. You'll need to do this anytime you want to run eelpond source activate eelpond Now you can start running eelpond! To test a \"full\" workflow, consisting of read pre-processing, kmer trimming, Trinity assembly, dammit annotation and salmon quantification: ./run_eelpond nema-test full These will run a small set of Nematostella vectensis test data (from Tulin et al., 2013 ) You can also run individual tools or subworkflows independently: ./run_eelpond nema-test preprocess ./run_eelpond nema-test trimmomatic See the help, here: ./run_eelpond -h Running your own data: To run your own data, you'll need to create two files, a tsv file containing your sample info, and a yaml file containing basic configuration info. To start, copy the test data files so you can modify them. cp nema_samples.tsv <my-tsv-name.tsv> Next, build a configfile to edit: ./run_eelpond config_name --build_config This configfile will contain all the default paramters for each step of the pipeline you target. If you don't specify any targets, it will default to the \"full\" pipeline, which executes read preprocessing, assembly, annotation, and quantification. Then, modify this configfile as necessary. The essential component is the samples.tsv file, which points eelpond to your sample files. References: original eel-pond protocol docs, last updated 2015 eel-pond protocol docs, last updated 2016 DIBSI, nonmodel RNAseq workshop, July 2017 SIO-BUG, nonmodel RNAseq workshop, October 2017 available workflows: preprocess: Read Quality Trimming and Filtering (fastqc, trimmomatic) kmer_trim: Kmer Trimming and/or Digital Normalization (khmer) assemble: Transcriptome Assembly (trinity) assemblyinput: Specify assembly for downstream steps annotate : Annotate the transcriptome (dammit, sourmash) quantify: Quantify transcripts (salmon) full: preprocess, kmer_trim, assemble, annotate, quantify","title":"Getting Started"},{"location":"Annotation/","text":"Annotating de novo transcriptomes with dammit \u00b6 dammit! dammit is an annotation pipeline written by Camille Scott . dammit runs a relatively standard annotation protocol for transcriptomes: it begins by building gene models with Transdecoder , and then uses the following protein databases as evidence for annotation: Pfam-A , Rfam , OrthoDB , uniref90 (uniref is optional with --full ). If a protein dataset is available, this can also be supplied to the dammit pipeline with --user-databases as optional evidence for annotation. In addition, BUSCO v3 is run, which will compare the gene content in your transcriptome with a lineage-specific data set. The output is a proportion of your transcriptome that matches with the data set, which can be used as an estimate of the completeness of your transcriptome based on evolutionary expectation ( Simho et al. 2015 ). There are several lineage-specific datasets available from the authors of BUSCO. We will use the metazoa dataset for this transcriptome. Database Preparation \u00b6 dammit has two major subcommands: dammit databases and dammit annotate . databases checks that the databases are installed and prepared, and if run with the --install flag, will perform that installation and preparation. If you just run dammit databases on its own, you should get a notification that some database tasks are not up-to-date -- we need to install them! Unless we're running short on time, we're going to do a full run. If you want to run a quick version of the pipeline, add a parameter, --quick , to omit OrthoDB, Uniref, Pfam, and Rfam. A \"full\" run will take longer to install and run, but you'll have access to the full annotation pipeline. dammit databases --install --busco-group metazoa # --quick We used the \"metazoa\" BUSCO group. We can use any of the BUSCO databases, so long as we install them with the dammit databases subcommand. You can see the whole list by running dammit databases -h . You should try to match your species as closely as possible for the best results. If we want to install another, for example: dammit databases --install --busco-group fungi # --quick Note: if you have limited space on your instance, you can also install these databases in a different location (e.g. on an external volume). You would want to run this command before running the database installs we just ran. #Run ONLY if you want to install databases in different location. #To run, remove the `#` from the front of the following command: # dammit databases --database-dir /path/to/databases Annotation \u00b6 Now we'll download a custom Nematostella vectensis protein database available from JGI. Here, somebody has already created a proper database for us [1] (it has a reference proteome available through uniprot). If your critter is a non-model organism, you will likely need to create your own with proteins from closely-related species. This will rely on your knowledge of your system! Run the command: dammit annotate trinity.nema.fasta --busco-group metazoa --user-databases nema.reference.prot.faa --n_threads 6 # --quick While dammit runs, it will print out which tasks its running to the terminal. dammit is written with a library called pydoit , which is a python workflow library similar to GNU Make. This not only helps organize the underlying workflow, but also means that if we interrupt it, it will properly resume! After a successful run, you'll have a new directory called trinity.nema.fasta.dammit . If you look inside, you'll see a lot of files: ls trinity.nema.fasta.dammit/ annotate.doit.db trinity.nema.fasta.dammit.namemap.csv trinity.nema.fasta.transdecoder.pep dammit.log trinity.nema.fasta.dammit.stats.json trinity.nema.fasta.x.nema.reference.prot.faa.crbl.csv run_trinity.nema.fasta.metazoa.busco.results trinity.nema.fasta.transdecoder.bed trinity.nema.fasta.x.nema.reference.prot.faa.crbl.gff3 tmp trinity.nema.fasta.transdecoder.cds trinity.nema.fasta.x.nema.reference.prot.faa.crbl.model.csv trinity.nema.fasta trinity.nema.fasta.transdecoder_dir trinity.nema.fasta.x.nema.reference.prot.faa.crbl.model.plot.pdf trinity.nema.fasta.dammit.fasta trinity.nema.fasta.transdecoder.gff3 trinity.nema.fasta.dammit.gff3 trinity.nema.fasta.transdecoder.mRNA The most important files for you are trinity.nema.fasta.dammit.fasta , trinity.nema.fasta.dammit.gff3 , and trinity.nema.fasta.dammit.stats.json . If the above dammit command is run again, there will be a message: **Pipeline is already completed!** Parse dammit output \u00b6 Cammille wrote dammit in Python, which includes a library to parse gff3 dammit output. To send this output to a useful table, we will need to open the Python environemnt. cd trinity.nema.fasta.dammit python Then, using this script, will output a list of gene ID: import pandas as pd from dammit.fileio.gff3 import GFF3Parser gff_file = \"trinity.nema.fasta.dammit.gff3\" annotations = GFF3Parser(filename=gff_file).read() names = annotations.sort_values(by=['seqid', 'score'], ascending=True).query('score < 1e-05').drop_duplicates(subset='seqid')[['seqid', 'Name']] new_file = names.dropna(axis=0,how='all') new_file.head() new_file.to_csv(\"nema_gene_name_id.csv\") exit() This will output a table of genes with 'seqid' and 'Name' in a .csv file: nema_gene_name_id.csv . Let's take a look at that file: less nema_gene_name_id.csv Notice there are multiple transcripts per gene model prediction. This .csv file can be used in tximport in downstream DE analysis. References \u00b6 Putnam NH, Srivastava M, Hellsten U, Dirks B, Chapman J, Salamov A, Terry A, Shapiro H, Lindquist E, Kapitonov VV, Jurka J, Genikhovich G, Grigoriev IV, Lucas SM, Steele RE, Finnerty JR, Technau U, Martindale MQ, Rokhsar DS. (2007) Sea anemone genome reveals ancestral eumetazoan gene repertoire and genomic organization. Science. 317, 86-94.","title":"Annotation"},{"location":"Annotation/#annotating-de-novo-transcriptomes-with-dammit","text":"dammit! dammit is an annotation pipeline written by Camille Scott . dammit runs a relatively standard annotation protocol for transcriptomes: it begins by building gene models with Transdecoder , and then uses the following protein databases as evidence for annotation: Pfam-A , Rfam , OrthoDB , uniref90 (uniref is optional with --full ). If a protein dataset is available, this can also be supplied to the dammit pipeline with --user-databases as optional evidence for annotation. In addition, BUSCO v3 is run, which will compare the gene content in your transcriptome with a lineage-specific data set. The output is a proportion of your transcriptome that matches with the data set, which can be used as an estimate of the completeness of your transcriptome based on evolutionary expectation ( Simho et al. 2015 ). There are several lineage-specific datasets available from the authors of BUSCO. We will use the metazoa dataset for this transcriptome.","title":"Annotating de novo transcriptomes with dammit"},{"location":"Annotation/#database-preparation","text":"dammit has two major subcommands: dammit databases and dammit annotate . databases checks that the databases are installed and prepared, and if run with the --install flag, will perform that installation and preparation. If you just run dammit databases on its own, you should get a notification that some database tasks are not up-to-date -- we need to install them! Unless we're running short on time, we're going to do a full run. If you want to run a quick version of the pipeline, add a parameter, --quick , to omit OrthoDB, Uniref, Pfam, and Rfam. A \"full\" run will take longer to install and run, but you'll have access to the full annotation pipeline. dammit databases --install --busco-group metazoa # --quick We used the \"metazoa\" BUSCO group. We can use any of the BUSCO databases, so long as we install them with the dammit databases subcommand. You can see the whole list by running dammit databases -h . You should try to match your species as closely as possible for the best results. If we want to install another, for example: dammit databases --install --busco-group fungi # --quick Note: if you have limited space on your instance, you can also install these databases in a different location (e.g. on an external volume). You would want to run this command before running the database installs we just ran. #Run ONLY if you want to install databases in different location. #To run, remove the `#` from the front of the following command: # dammit databases --database-dir /path/to/databases","title":"Database Preparation"},{"location":"Annotation/#annotation","text":"Now we'll download a custom Nematostella vectensis protein database available from JGI. Here, somebody has already created a proper database for us [1] (it has a reference proteome available through uniprot). If your critter is a non-model organism, you will likely need to create your own with proteins from closely-related species. This will rely on your knowledge of your system! Run the command: dammit annotate trinity.nema.fasta --busco-group metazoa --user-databases nema.reference.prot.faa --n_threads 6 # --quick While dammit runs, it will print out which tasks its running to the terminal. dammit is written with a library called pydoit , which is a python workflow library similar to GNU Make. This not only helps organize the underlying workflow, but also means that if we interrupt it, it will properly resume! After a successful run, you'll have a new directory called trinity.nema.fasta.dammit . If you look inside, you'll see a lot of files: ls trinity.nema.fasta.dammit/ annotate.doit.db trinity.nema.fasta.dammit.namemap.csv trinity.nema.fasta.transdecoder.pep dammit.log trinity.nema.fasta.dammit.stats.json trinity.nema.fasta.x.nema.reference.prot.faa.crbl.csv run_trinity.nema.fasta.metazoa.busco.results trinity.nema.fasta.transdecoder.bed trinity.nema.fasta.x.nema.reference.prot.faa.crbl.gff3 tmp trinity.nema.fasta.transdecoder.cds trinity.nema.fasta.x.nema.reference.prot.faa.crbl.model.csv trinity.nema.fasta trinity.nema.fasta.transdecoder_dir trinity.nema.fasta.x.nema.reference.prot.faa.crbl.model.plot.pdf trinity.nema.fasta.dammit.fasta trinity.nema.fasta.transdecoder.gff3 trinity.nema.fasta.dammit.gff3 trinity.nema.fasta.transdecoder.mRNA The most important files for you are trinity.nema.fasta.dammit.fasta , trinity.nema.fasta.dammit.gff3 , and trinity.nema.fasta.dammit.stats.json . If the above dammit command is run again, there will be a message: **Pipeline is already completed!**","title":"Annotation"},{"location":"Annotation/#parse-dammit-output","text":"Cammille wrote dammit in Python, which includes a library to parse gff3 dammit output. To send this output to a useful table, we will need to open the Python environemnt. cd trinity.nema.fasta.dammit python Then, using this script, will output a list of gene ID: import pandas as pd from dammit.fileio.gff3 import GFF3Parser gff_file = \"trinity.nema.fasta.dammit.gff3\" annotations = GFF3Parser(filename=gff_file).read() names = annotations.sort_values(by=['seqid', 'score'], ascending=True).query('score < 1e-05').drop_duplicates(subset='seqid')[['seqid', 'Name']] new_file = names.dropna(axis=0,how='all') new_file.head() new_file.to_csv(\"nema_gene_name_id.csv\") exit() This will output a table of genes with 'seqid' and 'Name' in a .csv file: nema_gene_name_id.csv . Let's take a look at that file: less nema_gene_name_id.csv Notice there are multiple transcripts per gene model prediction. This .csv file can be used in tximport in downstream DE analysis.","title":"Parse dammit output"},{"location":"Annotation/#references","text":"Putnam NH, Srivastava M, Hellsten U, Dirks B, Chapman J, Salamov A, Terry A, Shapiro H, Lindquist E, Kapitonov VV, Jurka J, Genikhovich G, Grigoriev IV, Lucas SM, Steele RE, Finnerty JR, Technau U, Martindale MQ, Rokhsar DS. (2007) Sea anemone genome reveals ancestral eumetazoan gene repertoire and genomic organization. Science. 317, 86-94.","title":"References"},{"location":"Assembly/","text":"Transcriptome Assembly \u00b6 Kmer Trimming \u00b6 Before running transcriptome assembly, we recommend doing some kmer spectral error trimming on your dataset, and if you have lots of reads, also performing digital normalization. We use khmer for both of these tasks. You can choose whether or not to use khmer diginal normalization with --no-diginorm . Note that you can also conduct diginorm with the Trinity assembler. The commands are as follows: With digital normalizition: \" (interleave-reads.py {input.r1} {input.r2} && zcat {input.r1_orphan} {input.r2_orphan}) | \" \" (trim-low-abund.py -V -k {params.k} -Z {params.Z} -C {params.C} - -o - -M {params.memory} \" \" --diginorm --diginorm-coverage={params.cov}) | (extract-paired-reads.py --gzip \" \" -p {output.paired} -s {output.single}) > {log}; split-paired-reads.py {output.paired} \" \" -1 {output.r1_out} -2 {output.r2_out} >> {log}\" Without digital normalization: \" (interleave-reads.py {input.r1} {input.r2} && zcat {input.r1_orphan} {input.r2_orphan}) | \" \" (trim-low-abund.py -V -k {params.k} -Z {params.Z} -C {params.C} - -o - -M {params.memory} \" \"| (extract-paired-reads.py --gzip -p {output.paired} -s {output.single}) > {log}; \" split-paired-reads.py {output.paired} -1 {output.r1_out} -2 {output.r2_out} >> {log}\" Assembling with Trinity \u00b6 We use the Trinity de novo transcriptome assembler to take short, trimmed/diginorm Illumina reads data and assemble (predict) full-length transcripts into a single fasta file output. Each contig in the fasta assembly file represents one unique transcript. The default k -mer size for the Trinity assembler is k = 25. The resulting output assembly fasta file can then be used to align the original, trimmed (not diginorm) short Illumina reads and quantify expression per transcript. The ID for each transcript is output (version 2.2.0 to current) as follows, where the TRINITY is constant, the DN2202 is an example of a variable contig/transcript ID, c stands for component, g gene and i isoform: TRINITY_DN2202_c0_g1_i1 This snakemake pipeline will run the following command: Trinity --left left.fq \\ --right right.fq --seqType fq --max_memory 10G \\ --CPU 4 Note, the current version of Trininty (after 2.3.2) is configured to diginorm the input reads before assembly begins. Since we have already applied diginorm to our reads, the result will be a negligible decrease in read counts prior to the assembly. Applying diginorm twice is fine. For data sets with large numbers of reads, applying diginorm as a separate step as we have here may decrease the memory requirements needed by the Trinity pipeline.","title":"Assembly"},{"location":"Assembly/#transcriptome-assembly","text":"","title":"Transcriptome Assembly"},{"location":"Assembly/#kmer-trimming","text":"Before running transcriptome assembly, we recommend doing some kmer spectral error trimming on your dataset, and if you have lots of reads, also performing digital normalization. We use khmer for both of these tasks. You can choose whether or not to use khmer diginal normalization with --no-diginorm . Note that you can also conduct diginorm with the Trinity assembler. The commands are as follows: With digital normalizition: \" (interleave-reads.py {input.r1} {input.r2} && zcat {input.r1_orphan} {input.r2_orphan}) | \" \" (trim-low-abund.py -V -k {params.k} -Z {params.Z} -C {params.C} - -o - -M {params.memory} \" \" --diginorm --diginorm-coverage={params.cov}) | (extract-paired-reads.py --gzip \" \" -p {output.paired} -s {output.single}) > {log}; split-paired-reads.py {output.paired} \" \" -1 {output.r1_out} -2 {output.r2_out} >> {log}\" Without digital normalization: \" (interleave-reads.py {input.r1} {input.r2} && zcat {input.r1_orphan} {input.r2_orphan}) | \" \" (trim-low-abund.py -V -k {params.k} -Z {params.Z} -C {params.C} - -o - -M {params.memory} \" \"| (extract-paired-reads.py --gzip -p {output.paired} -s {output.single}) > {log}; \" split-paired-reads.py {output.paired} -1 {output.r1_out} -2 {output.r2_out} >> {log}\"","title":"Kmer Trimming"},{"location":"Assembly/#assembling-with-trinity","text":"We use the Trinity de novo transcriptome assembler to take short, trimmed/diginorm Illumina reads data and assemble (predict) full-length transcripts into a single fasta file output. Each contig in the fasta assembly file represents one unique transcript. The default k -mer size for the Trinity assembler is k = 25. The resulting output assembly fasta file can then be used to align the original, trimmed (not diginorm) short Illumina reads and quantify expression per transcript. The ID for each transcript is output (version 2.2.0 to current) as follows, where the TRINITY is constant, the DN2202 is an example of a variable contig/transcript ID, c stands for component, g gene and i isoform: TRINITY_DN2202_c0_g1_i1 This snakemake pipeline will run the following command: Trinity --left left.fq \\ --right right.fq --seqType fq --max_memory 10G \\ --CPU 4 Note, the current version of Trininty (after 2.3.2) is configured to diginorm the input reads before assembly begins. Since we have already applied diginorm to our reads, the result will be a negligible decrease in read counts prior to the assembly. Applying diginorm twice is fine. For data sets with large numbers of reads, applying diginorm as a separate step as we have here may decrease the memory requirements needed by the Trinity pipeline.","title":"Assembling with Trinity"},{"location":"Configuration/","text":"Configuring the pipeline \u00b6 Each step of this pipeline is highly configurable via the yaml configuration file. More info coming soon.","title":"Configuring the pipeline"},{"location":"Configuration/#configuring-the-pipeline","text":"Each step of this pipeline is highly configurable via the yaml configuration file. More info coming soon.","title":"Configuring the pipeline"},{"location":"DE/","text":"Differential expression analysis with DESeq2 \u00b6 Comparing gene expression differences in samples between experimental conditions. We will be using DESeq2 . References: Documentation for DESeq2 with example analysis Love et al. 2014 * Love et al. 2016 Additional links: DE lecture by Jane Khudyakov, July 2017 Example DE analysis from two populations of killifish! (Fundulus heteroclitus MDPL vs. MDPL) * A Review of Differential Gene Expression Software for mRNA sequencing RStudio! \u00b6 The pipeline will be running these commands in an R script. You could run them in R Studio: Load libraries library(DESeq2) library(\"lattice\") library(tximport) library(readr) library(gplots) library(RColorBrewer) source('~/plotPCAWithSampleNames.R') Tell RStudio where your files are and ask whether they exist: setwd(\"/mnt/work/quant/salmon_out/\") dir<-\"/mnt/work/quant/\" files_list = list.files() files <- file.path(dir, \"salmon_out\",files_list, \"quant.sf\") names(files) <- c(\"0Hour_1\",\"0Hour_2\",\"0Hour_3\",\"0Hour_4\",\"0Hour_5\",\"6Hour_1\",\"6Hour_2\",\"6Hour_3\",\"6Hour_4\",\"6Hour_5\") files print(file.exists(files)) Grab the gene names and transcript ID file to summarize expression at the gene level . tx2gene <- read.table(\"~/nema_transcript_gene_id.txt\",sep=\"\\t\") cols<-c(\"transcript_id\",\"gene_id\") colnames(tx2gene)<-cols head(tx2gene) txi.salmon <- tximport(files, type = \"salmon\", tx2gene = tx2gene,importer=read.delim) head(txi.salmon$counts) dim(txi.salmon$counts) Assign experimental variables: condition = factor(c(\"0Hour\",\"0Hour\",\"0Hour\",\"0Hour\",\"0Hour\",\"6Hour\",\"6Hour\",\"6Hour\",\"6Hour\",\"6Hour\")) ExpDesign <- data.frame(row.names=colnames(txi.salmon$counts), condition = condition) ExpDesign Run DESeq2: dds <- DESeqDataSetFromTximport(txi.salmon, ExpDesign, ~condition) dds <- DESeq(dds, betaPrior=FALSE) Get counts: counts_table = counts( dds, normalized=TRUE ) Filtering out low expression transcripts: See plot from Lisa Komoroske generated with RNAseq123 filtered_norm_counts<-counts_table[!rowSums(counts_table==0)>=1, ] filtered_norm_counts<-as.data.frame(filtered_norm_counts) GeneID<-rownames(filtered_norm_counts) filtered_norm_counts<-cbind(filtered_norm_counts,GeneID) dim(filtered_norm_counts) head(filtered_norm_counts) Estimate dispersion: plotDispEsts(dds) PCA: log_dds<-rlog(dds) plotPCAWithSampleNames(log_dds, intgroup=\"condition\", ntop=40000) Get DE results: res<-results(dds,contrast=c(\"condition\",\"6Hour\",\"0Hour\")) head(res) res_ordered<-res[order(res$padj),] GeneID<-rownames(res_ordered) res_ordered<-as.data.frame(res_ordered) res_genes<-cbind(res_ordered,GeneID) dim(res_genes) head(res_genes) dim(res_genes) res_genes_merged <- merge(res_genes,filtered_norm_counts,by=unique(\"GeneID\")) dim(res_genes_merged) head(res_genes_merged) res_ordered<-res_genes_merged[order(res_genes_merged$padj),] write.csv(res_ordered, file=\"nema_DESeq_all.csv\" ) Set a threshold cutoff of padj<0.05 and \u00b1 log2FC 1: resSig = res_ordered[res_ordered$padj < 0.05, ] resSig = resSig[resSig$log2FoldChange > 1 | resSig$log2FoldChange < -1,] write.csv(resSig,file=\"nema_DESeq_padj0.05_log2FC1.csv\") MA plot with gene names: plot(log2(res_ordered$baseMean), res_ordered$log2FoldChange, col=ifelse(res_ordered$padj < 0.05, \"red\",\"gray67\"),main=\"nema (padj<0.05, log2FC = \u00b11)\",xlim=c(1,20),pch=20,cex=1,ylim=c(-12,12)) abline(h=c(-1,1), col=\"blue\") genes<-resSig$GeneID mygenes <- resSig[,] baseMean_mygenes <- mygenes[,\"baseMean\"] log2FoldChange_mygenes <- mygenes[,\"log2FoldChange\"] text(log2(baseMean_mygenes),log2FoldChange_mygenes,labels=genes,pos=2,cex=0.60) Heatmap d<-resSig dim(d) head(d) colnames(d) d<-d[,c(8:17)] d<-as.matrix(d) d<-as.data.frame(d) d<-as.matrix(d) rownames(d) <- resSig[,1] head(d) hr <- hclust(as.dist(1-cor(t(d), method=\"pearson\")), method=\"complete\") mycl <- cutree(hr, h=max(hr$height/1.5)) clusterCols <- rainbow(length(unique(mycl))) myClusterSideBar <- clusterCols[mycl] myheatcol <- greenred(75) heatmap.2(d, main=\"nema (padj<0.05, log2FC = \u00b11)\", Rowv=as.dendrogram(hr), cexRow=0.75,cexCol=0.8,srtCol= 90, adjCol = c(NA,0),offsetCol=2.5, Colv=NA, dendrogram=\"row\", scale=\"row\", col=myheatcol, density.info=\"none\", trace=\"none\", RowSideColors= myClusterSideBar)","title":"Differential Expression"},{"location":"DE/#differential-expression-analysis-with-deseq2","text":"Comparing gene expression differences in samples between experimental conditions. We will be using DESeq2 . References: Documentation for DESeq2 with example analysis Love et al. 2014 * Love et al. 2016 Additional links: DE lecture by Jane Khudyakov, July 2017 Example DE analysis from two populations of killifish! (Fundulus heteroclitus MDPL vs. MDPL) * A Review of Differential Gene Expression Software for mRNA sequencing","title":"Differential expression analysis with DESeq2"},{"location":"DE/#rstudio","text":"The pipeline will be running these commands in an R script. You could run them in R Studio: Load libraries library(DESeq2) library(\"lattice\") library(tximport) library(readr) library(gplots) library(RColorBrewer) source('~/plotPCAWithSampleNames.R') Tell RStudio where your files are and ask whether they exist: setwd(\"/mnt/work/quant/salmon_out/\") dir<-\"/mnt/work/quant/\" files_list = list.files() files <- file.path(dir, \"salmon_out\",files_list, \"quant.sf\") names(files) <- c(\"0Hour_1\",\"0Hour_2\",\"0Hour_3\",\"0Hour_4\",\"0Hour_5\",\"6Hour_1\",\"6Hour_2\",\"6Hour_3\",\"6Hour_4\",\"6Hour_5\") files print(file.exists(files)) Grab the gene names and transcript ID file to summarize expression at the gene level . tx2gene <- read.table(\"~/nema_transcript_gene_id.txt\",sep=\"\\t\") cols<-c(\"transcript_id\",\"gene_id\") colnames(tx2gene)<-cols head(tx2gene) txi.salmon <- tximport(files, type = \"salmon\", tx2gene = tx2gene,importer=read.delim) head(txi.salmon$counts) dim(txi.salmon$counts) Assign experimental variables: condition = factor(c(\"0Hour\",\"0Hour\",\"0Hour\",\"0Hour\",\"0Hour\",\"6Hour\",\"6Hour\",\"6Hour\",\"6Hour\",\"6Hour\")) ExpDesign <- data.frame(row.names=colnames(txi.salmon$counts), condition = condition) ExpDesign Run DESeq2: dds <- DESeqDataSetFromTximport(txi.salmon, ExpDesign, ~condition) dds <- DESeq(dds, betaPrior=FALSE) Get counts: counts_table = counts( dds, normalized=TRUE ) Filtering out low expression transcripts: See plot from Lisa Komoroske generated with RNAseq123 filtered_norm_counts<-counts_table[!rowSums(counts_table==0)>=1, ] filtered_norm_counts<-as.data.frame(filtered_norm_counts) GeneID<-rownames(filtered_norm_counts) filtered_norm_counts<-cbind(filtered_norm_counts,GeneID) dim(filtered_norm_counts) head(filtered_norm_counts) Estimate dispersion: plotDispEsts(dds) PCA: log_dds<-rlog(dds) plotPCAWithSampleNames(log_dds, intgroup=\"condition\", ntop=40000) Get DE results: res<-results(dds,contrast=c(\"condition\",\"6Hour\",\"0Hour\")) head(res) res_ordered<-res[order(res$padj),] GeneID<-rownames(res_ordered) res_ordered<-as.data.frame(res_ordered) res_genes<-cbind(res_ordered,GeneID) dim(res_genes) head(res_genes) dim(res_genes) res_genes_merged <- merge(res_genes,filtered_norm_counts,by=unique(\"GeneID\")) dim(res_genes_merged) head(res_genes_merged) res_ordered<-res_genes_merged[order(res_genes_merged$padj),] write.csv(res_ordered, file=\"nema_DESeq_all.csv\" ) Set a threshold cutoff of padj<0.05 and \u00b1 log2FC 1: resSig = res_ordered[res_ordered$padj < 0.05, ] resSig = resSig[resSig$log2FoldChange > 1 | resSig$log2FoldChange < -1,] write.csv(resSig,file=\"nema_DESeq_padj0.05_log2FC1.csv\") MA plot with gene names: plot(log2(res_ordered$baseMean), res_ordered$log2FoldChange, col=ifelse(res_ordered$padj < 0.05, \"red\",\"gray67\"),main=\"nema (padj<0.05, log2FC = \u00b11)\",xlim=c(1,20),pch=20,cex=1,ylim=c(-12,12)) abline(h=c(-1,1), col=\"blue\") genes<-resSig$GeneID mygenes <- resSig[,] baseMean_mygenes <- mygenes[,\"baseMean\"] log2FoldChange_mygenes <- mygenes[,\"log2FoldChange\"] text(log2(baseMean_mygenes),log2FoldChange_mygenes,labels=genes,pos=2,cex=0.60) Heatmap d<-resSig dim(d) head(d) colnames(d) d<-d[,c(8:17)] d<-as.matrix(d) d<-as.data.frame(d) d<-as.matrix(d) rownames(d) <- resSig[,1] head(d) hr <- hclust(as.dist(1-cor(t(d), method=\"pearson\")), method=\"complete\") mycl <- cutree(hr, h=max(hr$height/1.5)) clusterCols <- rainbow(length(unique(mycl))) myClusterSideBar <- clusterCols[mycl] myheatcol <- greenred(75) heatmap.2(d, main=\"nema (padj<0.05, log2FC = \u00b11)\", Rowv=as.dendrogram(hr), cexRow=0.75,cexCol=0.8,srtCol= 90, adjCol = c(NA,0),offsetCol=2.5, Colv=NA, dendrogram=\"row\", scale=\"row\", col=myheatcol, density.info=\"none\", trace=\"none\", RowSideColors= myClusterSideBar)","title":"RStudio!"},{"location":"Diginorm/","text":"Digital Normalization \u00b6 In this section, we\u2019ll apply digital normalization and variable-coverage k-mer abundance trimming to the reads prior to assembly using the khmer software package (version 2.1). This has the effect of reducing the computational cost of assembly without negatively affecting the quality of the assembly. This is all run in one command, taking the trimmed reads as input, uses the orphaned reads that survived while their mated pair did not during adapter and quality trimming. Then, low-abundance reads are trimmed to a coverage of 18 and normalized to a k -mer ( k = 20) coverage of 20. (interleave-reads.py {}{}.trim_1P.fq {}{}.trim_2P.fq && zcat {}orphans.fq.gz)| \\\\ (trim-low-abund.py -V -k 20 -Z 18 -C 2 - -o - -M 4e9 --diginorm --diginorm-coverage=20) | \\\\ (extract-paired-reads.py --gzip -p {}{}.paired.gz -s {}{}.single.gz) > /dev/null The output files are the remaining reads, grouped as pairs and singles (orphans). Since the Trinity de novo assembly software expects paired reads, we will split them into left left.fq and right.fq read pair files, including the single orphans in the left.fq file. for file in *.paired.gz do split-paired-reads.py ${file} done cat *.1 > left.fq cat *.2 > right.fq gunzip -c ../diginorm/single.gz >> left.fq","title":"Digital Normalization"},{"location":"Diginorm/#digital-normalization","text":"In this section, we\u2019ll apply digital normalization and variable-coverage k-mer abundance trimming to the reads prior to assembly using the khmer software package (version 2.1). This has the effect of reducing the computational cost of assembly without negatively affecting the quality of the assembly. This is all run in one command, taking the trimmed reads as input, uses the orphaned reads that survived while their mated pair did not during adapter and quality trimming. Then, low-abundance reads are trimmed to a coverage of 18 and normalized to a k -mer ( k = 20) coverage of 20. (interleave-reads.py {}{}.trim_1P.fq {}{}.trim_2P.fq && zcat {}orphans.fq.gz)| \\\\ (trim-low-abund.py -V -k 20 -Z 18 -C 2 - -o - -M 4e9 --diginorm --diginorm-coverage=20) | \\\\ (extract-paired-reads.py --gzip -p {}{}.paired.gz -s {}{}.single.gz) > /dev/null The output files are the remaining reads, grouped as pairs and singles (orphans). Since the Trinity de novo assembly software expects paired reads, we will split them into left left.fq and right.fq read pair files, including the single orphans in the left.fq file. for file in *.paired.gz do split-paired-reads.py ${file} done cat *.1 > left.fq cat *.2 > right.fq gunzip -c ../diginorm/single.gz >> left.fq","title":"Digital Normalization"},{"location":"QC/","text":"Short read quality and trimming \u00b6 The first step of any sequencing pipeline is to assess read quality and perform quality control as necessary. We natively enable quality assessment with FastQC and multiqc , and do quality trimming with Trimmomatic . Quality Assessment \u00b6 We\u2019re going to use FastQC (version 0.11.5) and multiqc (version 1.2) to summarize the data before and after adapter trimming. There are several caveats about FastQC - the main one is that it only calculates certain statistics (like duplicated sequences) for subsets of the data (e.g. duplicate sequences are only analyzed for the first 100,000 sequences in each file. Multiqc will summarize individual fastqc output into one output so that you can see all quality information for all files simultaenously. Adapter trim each pair of files \u00b6 We use Trimmomatic (version 0.36) to trim off residual Illumina adapters that were left behind after demultiplexing. This pipeline assumes TruSeq3-PE.fa adapters. However, if running this on your own data, you\u2019ll need to know which Illumina sequencing adapters were used for your library prep in order to trim them off. If they are the right adapters, you should see that some of the reads are trimmed; if they\u2019re not, you won\u2019t see anything get trimmed. See excellent paper on trimming parameters by MacManes 2014 . Based on these recommendations by MacManes 2014, we use this command in this pipeline: TrimmomaticPE ${base}.fastq.gz ${baseR2}.fastq.gz \\ ${base}.qc.fq.gz s1_se \\ ${baseR2}.qc.fq.gz s2_se \\ ILLUMINACLIP:TruSeq3-PE.fa:2:40:15 \\ LEADING:2 TRAILING:2 \\ SLIDINGWINDOW:4:2 \\ MINLEN:25 Customizing the trimming pipeline \u00b6 The default trimming paramters can be overridden by providing the following in the .yaml configuration file: trimmomatic: trim_cmd: \"ILLUMINACLIP:{}:2:40:15 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35\" Be sure to modify the trimming commands as desired","title":"Read Quality Trimming and Filtering"},{"location":"QC/#short-read-quality-and-trimming","text":"The first step of any sequencing pipeline is to assess read quality and perform quality control as necessary. We natively enable quality assessment with FastQC and multiqc , and do quality trimming with Trimmomatic .","title":"Short read quality and trimming"},{"location":"QC/#quality-assessment","text":"We\u2019re going to use FastQC (version 0.11.5) and multiqc (version 1.2) to summarize the data before and after adapter trimming. There are several caveats about FastQC - the main one is that it only calculates certain statistics (like duplicated sequences) for subsets of the data (e.g. duplicate sequences are only analyzed for the first 100,000 sequences in each file. Multiqc will summarize individual fastqc output into one output so that you can see all quality information for all files simultaenously.","title":"Quality Assessment"},{"location":"QC/#adapter-trim-each-pair-of-files","text":"We use Trimmomatic (version 0.36) to trim off residual Illumina adapters that were left behind after demultiplexing. This pipeline assumes TruSeq3-PE.fa adapters. However, if running this on your own data, you\u2019ll need to know which Illumina sequencing adapters were used for your library prep in order to trim them off. If they are the right adapters, you should see that some of the reads are trimmed; if they\u2019re not, you won\u2019t see anything get trimmed. See excellent paper on trimming parameters by MacManes 2014 . Based on these recommendations by MacManes 2014, we use this command in this pipeline: TrimmomaticPE ${base}.fastq.gz ${baseR2}.fastq.gz \\ ${base}.qc.fq.gz s1_se \\ ${baseR2}.qc.fq.gz s2_se \\ ILLUMINACLIP:TruSeq3-PE.fa:2:40:15 \\ LEADING:2 TRAILING:2 \\ SLIDINGWINDOW:4:2 \\ MINLEN:25","title":"Adapter trim each pair of files"},{"location":"QC/#customizing-the-trimming-pipeline","text":"The default trimming paramters can be overridden by providing the following in the .yaml configuration file: trimmomatic: trim_cmd: \"ILLUMINACLIP:{}:2:40:15 LEADING:20 TRAILING:20 SLIDINGWINDOW:4:15 MINLEN:35\" Be sure to modify the trimming commands as desired","title":"Customizing the trimming pipeline"},{"location":"Quality/","text":"Evaluating your transcriptome assembly \u00b6 We will be using Transrate and BUSCO! BUSCO \u00b6 B enchmarking U niversal S ingle C opy O rthologs (BUSCO) Eukaryota database has 303 genes Metazoa database has 978 genes \"Complete\" lengths are within two standard deviations of the BUSCO group mean length Genes that make up the BUSCO sets for each major lineage are selected from orthologous groups with genes present as single-copy orthologs in at least 90% of the species. Useful links: Website with additional busco databases: http://busco.ezlab.org/ Paper: Simao et al. 2015 User Guide Command: run_BUSCO.py \\ -i Trinity.fixed.fasta \\ -o nema_busco_metazoa -l ~/busco/metazoa_odb9 \\ -m transcriptome --cpu 2 Transrate \u00b6 Transrate serves two main purposes. It can compare two assemblies to see how similar they are. Or, it can give you a score which represents proportion of input reads that provide positive support for the assembly. We will use transrate to get a score for the assembly. Use the trimmed reads. For a further explanation of metrics and how to run the reference-based transrate, see the documentation and the paper by Smith-Unna et al. 2016 . How do two transcriptomes compare with each other? transrate --reference=Trinity.fixed.fasta --assembly=trinity-nematostella-raw.fa --output=full_v_subset transrate --reference=trinity-nematostella-raw.fa --assembly=Trinity.fixed.fasta --output=subset_v_full","title":"Quality Assessment"},{"location":"Quality/#evaluating-your-transcriptome-assembly","text":"We will be using Transrate and BUSCO!","title":"Evaluating your transcriptome assembly"},{"location":"Quality/#busco","text":"B enchmarking U niversal S ingle C opy O rthologs (BUSCO) Eukaryota database has 303 genes Metazoa database has 978 genes \"Complete\" lengths are within two standard deviations of the BUSCO group mean length Genes that make up the BUSCO sets for each major lineage are selected from orthologous groups with genes present as single-copy orthologs in at least 90% of the species. Useful links: Website with additional busco databases: http://busco.ezlab.org/ Paper: Simao et al. 2015 User Guide Command: run_BUSCO.py \\ -i Trinity.fixed.fasta \\ -o nema_busco_metazoa -l ~/busco/metazoa_odb9 \\ -m transcriptome --cpu 2","title":"BUSCO"},{"location":"Quality/#transrate","text":"Transrate serves two main purposes. It can compare two assemblies to see how similar they are. Or, it can give you a score which represents proportion of input reads that provide positive support for the assembly. We will use transrate to get a score for the assembly. Use the trimmed reads. For a further explanation of metrics and how to run the reference-based transrate, see the documentation and the paper by Smith-Unna et al. 2016 . How do two transcriptomes compare with each other? transrate --reference=Trinity.fixed.fasta --assembly=trinity-nematostella-raw.fa --output=full_v_subset transrate --reference=trinity-nematostella-raw.fa --assembly=Trinity.fixed.fasta --output=subset_v_full","title":"Transrate"},{"location":"Quant/","text":"Quantification with Salmon \u00b6 We will use Salmon to quantify expression. Salmon is a new breed of software for quantifying RNAseq reads that is both really fast and takes transcript length into consideration ( Patro et al. 2015 ). For further reading, see Intro blog post: http://robpatro.com/blog/?p=248 A 2016 blog post evaluating and comparing methods here Salmon github repo here https://github.com/ngs-docs/2015-nov-adv-rna/blob/master/salmon.rst http://angus.readthedocs.io/en/2016/rob_quant/tut.html https://2016-aug-nonmodel-rnaseq.readthedocs.io/en/latest/quantification.html The two most interesting files are salmon_quant.log and quant.sf . The latter contains the counts; the former contains the log information from running things. We recommend quantifying using the Trinity transcriptome assembly fasta file, which will give expression values for each contig, like this in quant.sf : Name Length EffectiveLength TPM NumReads TRINITY_DN2202_c0_g1_i1 210 39.818 2.683835 2.000000 TRINITY_DN2270_c0_g1_i1 213 41.064 0.000000 0.000000 TRINITY_DN2201_c0_g1_i1 266 69.681 0.766816 1.000000 TRINITY_DN2222_c0_g1_i1 243 55.794 2.873014 3.000000 TRINITY_DN2291_c0_g1_i1 245 56.916 0.000000 0.000000 TRINITY_DN2269_c0_g1_i1 294 89.251 0.000000 0.000000 TRINITY_DN2269_c1_g1_i1 246 57.479 0.000000 0.000000 TRINITY_DN2279_c0_g1_i1 426 207.443 0.000000 0.000000 TRINITY_DN2262_c0_g1_i1 500 280.803 0.190459 1.000912 TRINITY_DN2253_c0_g1_i1 1523 1303.116 0.164015 4.000000 TRINITY_DN2287_c0_g1_i1 467 247.962 0.000000 0.000000 TRINITY_DN2287_c1_g1_i1 325 113.826 0.469425 1.000000 TRINITY_DN2237_c0_g1_i1 306 98.441 0.542788 1.000000 TRINITY_DN2237_c0_g2_i1 307 99.229 0.000000 0.000000 TRINITY_DN2250_c0_g1_i1 368 151.832 0.000000 0.000000 TRINITY_DN2250_c1_g1_i1 271 72.988 0.000000 0.000000 TRINITY_DN2208_c0_g1_i1 379 162.080 1.978014 6.000000 TRINITY_DN2277_c0_g1_i1 269 71.657 0.745677 1.000000 TRINITY_DN2231_c0_g1_i1 209 39.409 0.000000 0.000000 TRINITY_DN2231_c1_g1_i1 334 121.411 0.000000 0.000000 TRINITY_DN2204_c0_g1_i1 287 84.121 0.000000 0.000000 There are two commands for salmon, salmon index and salmon quant . The first command, salmon index will index the transcriptome: salmon index --index nema --transcripts trinity.nema.full.fasta --type quasi And the second command, salmon quant will quantify the trimmed reads (not diginormed) using the transcriptome: for R1 in *R1*.fastq.gz do sample=$(basename $R1 extract.fastq.gz) echo sample is $sample, R1 is $R1 R2=${R1/R1/R2} echo R2 is $R2 salmon quant -i nema -p 2 -l IU -1 <(gunzip -c $R1) -2 <(gunzip -c $R2) -o ${sample}quant done","title":"Transcript Quantification"},{"location":"Quant/#quantification-with-salmon","text":"We will use Salmon to quantify expression. Salmon is a new breed of software for quantifying RNAseq reads that is both really fast and takes transcript length into consideration ( Patro et al. 2015 ). For further reading, see Intro blog post: http://robpatro.com/blog/?p=248 A 2016 blog post evaluating and comparing methods here Salmon github repo here https://github.com/ngs-docs/2015-nov-adv-rna/blob/master/salmon.rst http://angus.readthedocs.io/en/2016/rob_quant/tut.html https://2016-aug-nonmodel-rnaseq.readthedocs.io/en/latest/quantification.html The two most interesting files are salmon_quant.log and quant.sf . The latter contains the counts; the former contains the log information from running things. We recommend quantifying using the Trinity transcriptome assembly fasta file, which will give expression values for each contig, like this in quant.sf : Name Length EffectiveLength TPM NumReads TRINITY_DN2202_c0_g1_i1 210 39.818 2.683835 2.000000 TRINITY_DN2270_c0_g1_i1 213 41.064 0.000000 0.000000 TRINITY_DN2201_c0_g1_i1 266 69.681 0.766816 1.000000 TRINITY_DN2222_c0_g1_i1 243 55.794 2.873014 3.000000 TRINITY_DN2291_c0_g1_i1 245 56.916 0.000000 0.000000 TRINITY_DN2269_c0_g1_i1 294 89.251 0.000000 0.000000 TRINITY_DN2269_c1_g1_i1 246 57.479 0.000000 0.000000 TRINITY_DN2279_c0_g1_i1 426 207.443 0.000000 0.000000 TRINITY_DN2262_c0_g1_i1 500 280.803 0.190459 1.000912 TRINITY_DN2253_c0_g1_i1 1523 1303.116 0.164015 4.000000 TRINITY_DN2287_c0_g1_i1 467 247.962 0.000000 0.000000 TRINITY_DN2287_c1_g1_i1 325 113.826 0.469425 1.000000 TRINITY_DN2237_c0_g1_i1 306 98.441 0.542788 1.000000 TRINITY_DN2237_c0_g2_i1 307 99.229 0.000000 0.000000 TRINITY_DN2250_c0_g1_i1 368 151.832 0.000000 0.000000 TRINITY_DN2250_c1_g1_i1 271 72.988 0.000000 0.000000 TRINITY_DN2208_c0_g1_i1 379 162.080 1.978014 6.000000 TRINITY_DN2277_c0_g1_i1 269 71.657 0.745677 1.000000 TRINITY_DN2231_c0_g1_i1 209 39.409 0.000000 0.000000 TRINITY_DN2231_c1_g1_i1 334 121.411 0.000000 0.000000 TRINITY_DN2204_c0_g1_i1 287 84.121 0.000000 0.000000 There are two commands for salmon, salmon index and salmon quant . The first command, salmon index will index the transcriptome: salmon index --index nema --transcripts trinity.nema.full.fasta --type quasi And the second command, salmon quant will quantify the trimmed reads (not diginormed) using the transcriptome: for R1 in *R1*.fastq.gz do sample=$(basename $R1 extract.fastq.gz) echo sample is $sample, R1 is $R1 R2=${R1/R1/R2} echo R2 is $R2 salmon quant -i nema -p 2 -l IU -1 <(gunzip -c $R1) -2 <(gunzip -c $R2) -o ${sample}quant done","title":"Quantification with Salmon"},{"location":"dev_tips/","text":"Notes for Developers \u00b6 Admin: updating mkdocs documentation update the docs and commit your changes mkdocs build to update the docs note: if you don't already have mkdocs, install with: conda install -c conda-forge mkdocs if you haven't already, install ghp-import: conda install -c conda-forge ghp-import use ghp-import to push the updated to docs to the gh-pages branch ghp-import site -p Some useful conda, snakemake, workflow hints: optional: to make conda installs simpler, set up conda configuration \u00b6 conda config --set always_yes yes --set changeps1 no conda config --add channels conda-forge conda config --add channels defaults conda config --add channels bioconda If you need to modify a conda package: \u00b6 you'll need to work with a local install of that package. Here's how to use conda to install the dependenciesfrom the conda recipe. see also https://conda.io/docs/user-guide/tutorials/build-pkgs.html#building-and-installing install conda-build ``` conda install conda-build ``` clone the repo of interest and cd into it ``` git clone dammit-repo cd dammit-repo ``` There should be a folder called recipe. Use conda-build to build it. ``` conda build recipe ``` Install the local code ``` ## not working conda install dammit --use-local # or, you can use pip: # pip install -e . \u2014no-deps # NOW, use: conda develop . pip install -e . ```","title":"Notes for Developers"},{"location":"dev_tips/#notes-for-developers","text":"Admin: updating mkdocs documentation update the docs and commit your changes mkdocs build to update the docs note: if you don't already have mkdocs, install with: conda install -c conda-forge mkdocs if you haven't already, install ghp-import: conda install -c conda-forge ghp-import use ghp-import to push the updated to docs to the gh-pages branch ghp-import site -p Some useful conda, snakemake, workflow hints:","title":"Notes for Developers"},{"location":"dev_tips/#optional-to-make-conda-installs-simpler-set-up-conda-configuration","text":"conda config --set always_yes yes --set changeps1 no conda config --add channels conda-forge conda config --add channels defaults conda config --add channels bioconda","title":"optional: to make conda installs simpler, set up conda configuration"},{"location":"dev_tips/#if-you-need-to-modify-a-conda-package","text":"you'll need to work with a local install of that package. Here's how to use conda to install the dependenciesfrom the conda recipe. see also https://conda.io/docs/user-guide/tutorials/build-pkgs.html#building-and-installing install conda-build ``` conda install conda-build ``` clone the repo of interest and cd into it ``` git clone dammit-repo cd dammit-repo ``` There should be a folder called recipe. Use conda-build to build it. ``` conda build recipe ``` Install the local code ``` ## not working conda install dammit --use-local # or, you can use pip: # pip install -e . \u2014no-deps # NOW, use: conda develop . pip install -e . ```","title":"If you need to modify a conda package:"}]}